{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple MLP using all the data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re as re\n",
    "import math\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load all files & put into dataframes\n",
    "\n",
    "Negative training data:\n",
    "- `pfam_training_data_augment.h5`\n",
    "- `non_cazy_kegg.h5`\n",
    "\n",
    "Positive training data:\n",
    "- `vicreg_train_val_embeddings_noCAZOME_noLargeSeqs_combined.h5` (+ use `cazy_family_by_taxa_60.json` to pick a representative sample across all enzyme classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUL_embeddings = []\n",
    "PUL_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\PUL.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        PUL_embeddings.append(np.array(f[key][()]))\n",
    "        PUL_keys.append(key)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PFAM_embeddings = []\n",
    "PFAM_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\pfam_training_data_augment.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        PFAM_embeddings.append(f[key][()])\n",
    "        PFAM_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_embeddings = []\n",
    "kegg_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\non_cazy_kegg.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        kegg_embeddings.append(f[key][()])\n",
    "        kegg_keys.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the PULs are stored in `PUL_df`, which also indicates if each protein is a CAZyme (1) or not (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUL file:\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "f_path = 'C:\\\\Users\\\\alpha\\\\OneDrive - University of Cambridge\\\\BACKUP 14-04-22\\\\docs\\\\Maths\\\\SRIM\\\\code\\\\PUL.faa'\n",
    "PUL_array, PUL_keys2 = [], []\n",
    "\n",
    "with open(f_path, mode='r') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        identifier, description = record.id, record.description\n",
    "        PUL_keys2.append(identifier)\n",
    "        if 'CAZyme' in description:\n",
    "            PUL_array.append(1)\n",
    "        else:\n",
    "            PUL_array.append(0)\n",
    "\n",
    "PUL_array = np.array(PUL_array)\n",
    "PUL_array = PUL_array.reshape(np.shape(PUL_array)[0],-1)\n",
    "PUL_array_df = pd.DataFrame(PUL_array, index=PUL_keys2,columns=['cazy'])\n",
    "\n",
    "col_label=['emb'+str(i) for i in range(len(list(PUL_embeddings)[0]))]\n",
    "# col_label.append('cazy')\n",
    "\n",
    "PUL_embeddings_list=list(PUL_embeddings)\n",
    "temp_df = pd.DataFrame(PUL_embeddings_list, index=PUL_keys, columns=['emb'+str(i) for i in range(len(list(PUL_embeddings)[0]))])\n",
    "\n",
    "# PUL_df = pd.DataFrame(data=np.concatenate([PUL_embeddings,PUL_array], axis=1), index=PUL_keys2, columns=col_label)\n",
    "PUL_df = temp_df.join(PUL_array_df)\n",
    "\n",
    "#indexing issue fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>emb9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1015</th>\n",
       "      <th>emb1016</th>\n",
       "      <th>emb1017</th>\n",
       "      <th>emb1018</th>\n",
       "      <th>emb1019</th>\n",
       "      <th>emb1020</th>\n",
       "      <th>emb1021</th>\n",
       "      <th>emb1022</th>\n",
       "      <th>emb1023</th>\n",
       "      <th>cazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PUL0001_1</th>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>-0.032013</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>-0.044128</td>\n",
       "      <td>-0.086609</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049255</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>-0.111633</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>0.030334</td>\n",
       "      <td>-0.028244</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>0.025970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_10</th>\n",
       "      <td>-0.014236</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.034546</td>\n",
       "      <td>-0.026245</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>-0.001535</td>\n",
       "      <td>-0.026840</td>\n",
       "      <td>-0.044830</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>-0.120483</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>-0.005569</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_11</th>\n",
       "      <td>-0.019104</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>0.051361</td>\n",
       "      <td>0.030670</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.076233</td>\n",
       "      <td>0.047943</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042297</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.089661</td>\n",
       "      <td>0.064331</td>\n",
       "      <td>-0.042755</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_12</th>\n",
       "      <td>0.047211</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>-0.008369</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>-0.002466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.036804</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>-0.035187</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>0.069641</td>\n",
       "      <td>-0.001657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_13</th>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.021515</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>0.024506</td>\n",
       "      <td>-0.014908</td>\n",
       "      <td>-0.058167</td>\n",
       "      <td>-0.014183</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>-0.064758</td>\n",
       "      <td>0.049774</td>\n",
       "      <td>-0.009315</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>0.060211</td>\n",
       "      <td>0.017761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_5</th>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.035370</td>\n",
       "      <td>-0.035553</td>\n",
       "      <td>-0.049744</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006264</td>\n",
       "      <td>-0.016479</td>\n",
       "      <td>-0.024261</td>\n",
       "      <td>0.067261</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>-0.026321</td>\n",
       "      <td>0.032532</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>0.066528</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_6</th>\n",
       "      <td>0.009483</td>\n",
       "      <td>0.044464</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>-0.036957</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>-0.014572</td>\n",
       "      <td>-0.093689</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.013908</td>\n",
       "      <td>-0.104553</td>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>-0.053558</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_7</th>\n",
       "      <td>0.051819</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>-0.028381</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>-0.026932</td>\n",
       "      <td>-0.030212</td>\n",
       "      <td>0.031433</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>-0.069275</td>\n",
       "      <td>0.019424</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>-0.048309</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>-0.010933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_8</th>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.047760</td>\n",
       "      <td>0.041046</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.082153</td>\n",
       "      <td>-0.036713</td>\n",
       "      <td>-0.099487</td>\n",
       "      <td>-0.023865</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015564</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>-0.069214</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.011467</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>0.034729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_9</th>\n",
       "      <td>0.024521</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.020950</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>-0.080750</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>-0.006287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019974</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>0.042999</td>\n",
       "      <td>0.024536</td>\n",
       "      <td>-0.007603</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>0.062805</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                emb0      emb1      emb2      emb3      emb4      emb5  \\\n",
       "PUL0001_1   0.017563  0.059692  0.030075  0.007584 -0.032013  0.063904   \n",
       "PUL0001_10 -0.014236  0.032715  0.034546 -0.026245  0.007820 -0.001535   \n",
       "PUL0001_11 -0.019104  0.034027  0.051361  0.030670 -0.013756  0.012695   \n",
       "PUL0001_12  0.047211  0.002373  0.000755  0.020142 -0.008369  0.028976   \n",
       "PUL0001_13  0.024719  0.031494  0.027359  0.021515 -0.007629  0.024506   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "PUL0602_5   0.017685  0.051422  0.006428 -0.001532  0.006233  0.035370   \n",
       "PUL0602_6   0.009483  0.044464  0.050842 -0.036957  0.004826 -0.014664   \n",
       "PUL0602_7   0.051819  0.009346  0.015388 -0.028381  0.013084  0.042877   \n",
       "PUL0602_8   0.032349  0.047760  0.041046 -0.033142  0.004124  0.082153   \n",
       "PUL0602_9   0.024521  0.037872  0.005112 -0.020950  0.020767  0.042908   \n",
       "\n",
       "                emb6      emb7      emb8      emb9  ...   emb1015   emb1016  \\\n",
       "PUL0001_1  -0.044128 -0.086609  0.033752 -0.031281  ... -0.049255 -0.006824   \n",
       "PUL0001_10 -0.026840 -0.044830 -0.022736  0.003244  ... -0.020203  0.010963   \n",
       "PUL0001_11 -0.019608 -0.076233  0.047943  0.000046  ... -0.042297 -0.000536   \n",
       "PUL0001_12 -0.000050 -0.083252  0.017487 -0.002466  ... -0.031281 -0.036804   \n",
       "PUL0001_13 -0.014908 -0.058167 -0.014183 -0.020538  ...  0.004028  0.015884   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "PUL0602_5  -0.035553 -0.049744  0.028076  0.002031  ... -0.006264 -0.016479   \n",
       "PUL0602_6  -0.014572 -0.093689 -0.026871  0.017822  ... -0.001588  0.013908   \n",
       "PUL0602_7  -0.026932 -0.030212  0.031433  0.006268  ... -0.001817  0.013542   \n",
       "PUL0602_8  -0.036713 -0.099487 -0.023865  0.007652  ... -0.015564  0.004181   \n",
       "PUL0602_9  -0.052765 -0.080750 -0.012650 -0.006287  ... -0.019974  0.022400   \n",
       "\n",
       "             emb1017   emb1018   emb1019   emb1020   emb1021   emb1022  \\\n",
       "PUL0001_1  -0.111633  0.083191  0.030334 -0.028244 -0.008194  0.031616   \n",
       "PUL0001_10 -0.120483  0.041931  0.013710  0.003368 -0.005569  0.021667   \n",
       "PUL0001_11 -0.089661  0.064331 -0.042755 -0.018158 -0.025391  0.001602   \n",
       "PUL0001_12 -0.075195  0.022964  0.014938 -0.035187 -0.028992  0.069641   \n",
       "PUL0001_13 -0.064758  0.049774 -0.009315  0.007256  0.006168  0.060211   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "PUL0602_5  -0.024261  0.067261  0.056061 -0.026321  0.032532 -0.009926   \n",
       "PUL0602_6  -0.104553  0.027496  0.050049 -0.053558  0.024475  0.020248   \n",
       "PUL0602_7  -0.069275  0.019424 -0.009377 -0.048309  0.009567  0.007690   \n",
       "PUL0602_8  -0.069214  0.032410  0.000823  0.000344 -0.011467  0.043060   \n",
       "PUL0602_9  -0.098633  0.042999  0.024536 -0.007603 -0.011642  0.062805   \n",
       "\n",
       "             emb1023  cazy  \n",
       "PUL0001_1   0.025970     1  \n",
       "PUL0001_10 -0.024216     0  \n",
       "PUL0001_11  0.011215     0  \n",
       "PUL0001_12 -0.001657     0  \n",
       "PUL0001_13  0.017761     1  \n",
       "...              ...   ...  \n",
       "PUL0602_5   0.066528     1  \n",
       "PUL0602_6   0.038025     0  \n",
       "PUL0602_7  -0.010933     0  \n",
       "PUL0602_8   0.034729     0  \n",
       "PUL0602_9   0.114685     0  \n",
       "\n",
       "[7699 rows x 1025 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUL_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-PUL negative (non-CAZyme) data is stored in `non_cazy_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-cazymes:\n",
    "non_cazy_embeddings = np.concatenate([PFAM_embeddings,kegg_embeddings], axis=0)\n",
    "non_cazy_keys = np.concatenate([PFAM_keys, kegg_keys], axis=0)\n",
    "non_cazy_df = pd.DataFrame(data=non_cazy_embeddings, index=non_cazy_keys, columns=['emb'+str(i) for i in range(len(list(non_cazy_embeddings)[0]))])\n",
    "non_cazy_df = non_cazy_df.join(pd.DataFrame(data=np.zeros(np.array(non_cazy_keys).shape[0]), index=non_cazy_keys, columns=['cazy']), how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_embeddings = []\n",
    "cazy_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\vicreg_train_val_embeddings_noCAZOME_noLargeSeqs_combined.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        cazy_embeddings.append(f[key][()])\n",
    "        cazy_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7699, 1024)\n",
      "(59673, 1024)\n",
      "(244592, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(PUL_embeddings))\n",
    "print(np.shape(non_cazy_embeddings))\n",
    "print(np.shape(cazy_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create lookup using .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_ids = []\n",
    "family_keys = []\n",
    "file = open('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\cazy_family_by_taxa_60.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_ids = []\n",
    "cazy_families = []\n",
    "file = open('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\cazy_family_by_taxa_60.json')\n",
    "data = json.loads(file.read())\n",
    "\n",
    "for key in data.keys(): #keys are IDs, values are classes\n",
    "        cazy_ids.append(key)\n",
    "        cazy_families.append(data.get(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cazy_df = pd.DataFrame(data=concat , columns=['id','class'])\n",
    "cazy_df_2 = pd.DataFrame(data=cazy_families, index=cazy_ids , columns=['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe of cazymes with extra columnns containing embeddings, then inner join with cazy_df_2 (intersect dataframes at indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_df_1 = pd.DataFrame(data=cazy_embeddings, index=cazy_keys, columns=['emb'+str(i) for i in range(len(list(cazy_embeddings)[0]))])\n",
    "\n",
    "# long (~12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             class      emb0      emb1      emb2      emb3      emb4  \\\n",
      "AZS17016.1  GH13_2  0.049500  0.037415 -0.038025  0.031235  0.017197   \n",
      "AVO05213.1    GH32 -0.014870  0.036865  0.009872 -0.011703  0.015808   \n",
      "QPG94344.1    GT61  0.016739 -0.028412  0.026016  0.017227  0.013565   \n",
      "APD47233.1     GT4  0.015976 -0.009315 -0.032532  0.033173  0.011223   \n",
      "ANU32067.1     GT4  0.010872 -0.021881 -0.008972 -0.013237  0.000924   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "QMI07227.1    GT83 -0.008034 -0.009453  0.029785  0.033478  0.024063   \n",
      "UMA63307.1     GT2  0.019272 -0.044830  0.024490  0.026031  0.026672   \n",
      "UPW01296.1     GT4  0.055878 -0.059723 -0.039551 -0.001044  0.057983   \n",
      "ULL17023.1    GH43  0.013275  0.033997  0.013687  0.021133  0.018799   \n",
      "USF23742.1     GT9  0.022705 -0.043182 -0.023972  0.007046 -0.025818   \n",
      "\n",
      "                emb5      emb6      emb7      emb8  ...   emb1014   emb1015  \\\n",
      "AZS17016.1  0.031616 -0.019043 -0.046234  0.007179  ... -0.040833 -0.034882   \n",
      "AVO05213.1 -0.001546  0.005375 -0.102600 -0.002607  ... -0.015167 -0.013428   \n",
      "QPG94344.1  0.033905 -0.067627 -0.064880 -0.005409  ... -0.072754 -0.009453   \n",
      "APD47233.1  0.055298 -0.019485 -0.062164  0.036804  ... -0.027649 -0.028488   \n",
      "ANU32067.1  0.037537 -0.050995 -0.051697  0.014412  ... -0.032379 -0.040771   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "QMI07227.1  0.021744 -0.033081 -0.051086  0.059387  ... -0.012657 -0.030960   \n",
      "UMA63307.1  0.003311 -0.064575 -0.080627 -0.030411  ...  0.004433 -0.040588   \n",
      "UPW01296.1  0.034698 -0.031525 -0.088623  0.003918  ... -0.030212  0.000360   \n",
      "ULL17023.1  0.044891 -0.020218 -0.037231  0.026855  ... -0.012169 -0.036713   \n",
      "USF23742.1  0.057068 -0.012955 -0.104492  0.002665  ...  0.012947 -0.064270   \n",
      "\n",
      "             emb1016   emb1017   emb1018   emb1019   emb1020   emb1021  \\\n",
      "AZS17016.1 -0.048950 -0.054535  0.053986  0.058350 -0.017746  0.000415   \n",
      "AVO05213.1  0.028091 -0.086609  0.072632  0.039520 -0.009178  0.034973   \n",
      "QPG94344.1 -0.022400  0.017334  0.031250  0.055725  0.022354 -0.053772   \n",
      "APD47233.1 -0.006145  0.000610  0.007973  0.017776 -0.014000 -0.018326   \n",
      "ANU32067.1  0.010300 -0.068787  0.038727  0.052124 -0.036377  0.008217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "QMI07227.1 -0.010986 -0.069885  0.026779  0.051392  0.016006 -0.047211   \n",
      "UMA63307.1 -0.030167 -0.038300 -0.033752  0.017395 -0.015884 -0.026962   \n",
      "UPW01296.1 -0.020020 -0.026672  0.013168  0.062378 -0.035492 -0.030624   \n",
      "ULL17023.1  0.012344 -0.091064  0.013885  0.050232  0.000664  0.004559   \n",
      "USF23742.1 -0.004471 -0.082336  0.065735  0.049774 -0.016922 -0.023758   \n",
      "\n",
      "             emb1022   emb1023  \n",
      "AZS17016.1  0.031189  0.030411  \n",
      "AVO05213.1  0.057617  0.035248  \n",
      "QPG94344.1  0.024490  0.016556  \n",
      "APD47233.1  0.027618  0.011330  \n",
      "ANU32067.1  0.031891 -0.020660  \n",
      "...              ...       ...  \n",
      "QMI07227.1  0.022720  0.053162  \n",
      "UMA63307.1  0.026901 -0.016068  \n",
      "UPW01296.1  0.041962  0.034149  \n",
      "ULL17023.1  0.057648  0.028473  \n",
      "USF23742.1  0.039551  0.022232  \n",
      "\n",
      "[232736 rows x 1025 columns]\n"
     ]
    }
   ],
   "source": [
    "cazy_df = cazy_df_2.join(cazy_df_1, how='inner')\n",
    "print(cazy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_GH_df = cazy_df[cazy_df['class'].str.contains('GH')]\n",
    "cazy_GT_df = cazy_df[cazy_df['class'].str.contains('GT')]\n",
    "cazy_PL_df = cazy_df[cazy_df['class'].str.contains('PL')]\n",
    "cazy_CE_df = cazy_df[cazy_df['class'].str.contains('CE')]\n",
    "\n",
    "allclasses=['GH', 'GT', 'PL', 'CE']\n",
    "cazy_other_df = cazy_df[cazy_df['class'].str.contains('|'.join(allclasses))]\n",
    "\n",
    "#then use pd.DataFrame.sample to take random sample of items across axis 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size of first four dataframes add up to 232736; so in fact no other classes to account for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample proportionally to size of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=5000/232736\n",
    "new_cazy_GH_df = cazy_GH_df.sample(frac=x, axis=0)\n",
    "new_cazy_GT_df = cazy_GT_df.sample(frac=x, axis=0)\n",
    "new_cazy_PL_df = cazy_PL_df.sample(frac=x, axis=0)\n",
    "new_cazy_CE_df = cazy_CE_df.sample(frac=x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate into one DataFrame:\n",
    "\n",
    "Positive training data is stored in `cazy_train_df` (5001 embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_train_df = pd.concat([new_cazy_GH_df, new_cazy_GT_df, new_cazy_PL_df, new_cazy_CE_df], axis=0)\n",
    "cazy_train_df = cazy_train_df.join(pd.DataFrame(data=np.ones(np.array(cazy_ids).shape[0]), index=cazy_ids, columns=['cazy']), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1015</th>\n",
       "      <th>emb1016</th>\n",
       "      <th>emb1017</th>\n",
       "      <th>emb1018</th>\n",
       "      <th>emb1019</th>\n",
       "      <th>emb1020</th>\n",
       "      <th>emb1021</th>\n",
       "      <th>emb1022</th>\n",
       "      <th>emb1023</th>\n",
       "      <th>cazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QOL01086.1</th>\n",
       "      <td>GH31</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>0.044281</td>\n",
       "      <td>-0.039459</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>-0.074646</td>\n",
       "      <td>-0.043121</td>\n",
       "      <td>-0.054077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049164</td>\n",
       "      <td>-0.023560</td>\n",
       "      <td>-0.031158</td>\n",
       "      <td>0.071106</td>\n",
       "      <td>0.025864</td>\n",
       "      <td>-0.001612</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>0.048798</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBG98846.1</th>\n",
       "      <td>GH32</td>\n",
       "      <td>0.030823</td>\n",
       "      <td>0.082581</td>\n",
       "      <td>0.065735</td>\n",
       "      <td>-0.014557</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>-0.008774</td>\n",
       "      <td>-0.032104</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>-0.050018</td>\n",
       "      <td>0.052002</td>\n",
       "      <td>0.009552</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>0.032928</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UOB18734.1</th>\n",
       "      <td>GH29</td>\n",
       "      <td>0.044586</td>\n",
       "      <td>0.096252</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.014107</td>\n",
       "      <td>-0.008736</td>\n",
       "      <td>0.069031</td>\n",
       "      <td>-0.054413</td>\n",
       "      <td>-0.088379</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008293</td>\n",
       "      <td>-0.030853</td>\n",
       "      <td>-0.055084</td>\n",
       "      <td>0.054749</td>\n",
       "      <td>0.027939</td>\n",
       "      <td>-0.001834</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.035553</td>\n",
       "      <td>0.029160</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAU42234.1</th>\n",
       "      <td>GH5_39</td>\n",
       "      <td>0.052338</td>\n",
       "      <td>0.069763</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>-0.003042</td>\n",
       "      <td>-0.013245</td>\n",
       "      <td>0.026962</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>-0.076721</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020416</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.055206</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.022125</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>-0.006020</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UKZ59741.1</th>\n",
       "      <td>GH16_18</td>\n",
       "      <td>-0.017899</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.092224</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>-0.032684</td>\n",
       "      <td>-0.027451</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.059509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>-0.006351</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>-0.025681</td>\n",
       "      <td>0.022720</td>\n",
       "      <td>0.039886</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGQ53951.1</th>\n",
       "      <td>CE9</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.070435</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>-0.021805</td>\n",
       "      <td>0.029556</td>\n",
       "      <td>-0.036926</td>\n",
       "      <td>-0.059082</td>\n",
       "      <td>0.024155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012581</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.052856</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>-0.021179</td>\n",
       "      <td>-0.017365</td>\n",
       "      <td>-0.042297</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKK12048.1</th>\n",
       "      <td>CE1</td>\n",
       "      <td>0.056427</td>\n",
       "      <td>-0.032715</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>-0.026855</td>\n",
       "      <td>-0.015839</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>-0.085632</td>\n",
       "      <td>-0.047546</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.033875</td>\n",
       "      <td>-0.043243</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>0.048615</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>-0.013275</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>0.047058</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALR14359.1</th>\n",
       "      <td>CE14</td>\n",
       "      <td>-0.002836</td>\n",
       "      <td>-0.002098</td>\n",
       "      <td>-0.003376</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.026108</td>\n",
       "      <td>-0.002766</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>0.030563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>-0.032532</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>-0.020401</td>\n",
       "      <td>-0.058777</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UAY05060.1</th>\n",
       "      <td>CE4</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>0.007248</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>-0.013657</td>\n",
       "      <td>0.064392</td>\n",
       "      <td>0.069153</td>\n",
       "      <td>-0.035309</td>\n",
       "      <td>-0.058472</td>\n",
       "      <td>-0.021759</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>0.043884</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>-0.016647</td>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.037750</td>\n",
       "      <td>0.057495</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QRG65564.1</th>\n",
       "      <td>CE4</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>-0.008430</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.007927</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>-0.053986</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>0.027481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011307</td>\n",
       "      <td>0.016083</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>0.058777</td>\n",
       "      <td>0.103638</td>\n",
       "      <td>-0.015541</td>\n",
       "      <td>-0.004276</td>\n",
       "      <td>0.023926</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              class      emb0      emb1      emb2      emb3      emb4  \\\n",
       "QOL01086.1     GH31  0.068115  0.009529  0.044281 -0.039459  0.003263   \n",
       "BBG98846.1     GH32  0.030823  0.082581  0.065735 -0.014557  0.005985   \n",
       "UOB18734.1     GH29  0.044586  0.096252  0.019272  0.014107 -0.008736   \n",
       "BAU42234.1   GH5_39  0.052338  0.069763  0.018265 -0.003042 -0.013245   \n",
       "UKZ59741.1  GH16_18 -0.017899  0.019196  0.092224  0.018677  0.010742   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "AGQ53951.1      CE9  0.064209  0.070435  0.011192  0.029099 -0.021805   \n",
       "AKK12048.1      CE1  0.056427 -0.032715 -0.001029 -0.026855 -0.015839   \n",
       "ALR14359.1     CE14 -0.002836 -0.002098 -0.003376  0.019119  0.025253   \n",
       "UAY05060.1      CE4  0.021118  0.007248  0.033295 -0.013657  0.064392   \n",
       "QRG65564.1      CE4  0.014633  0.009842 -0.008430  0.030075  0.007927   \n",
       "\n",
       "                emb5      emb6      emb7      emb8  ...   emb1015   emb1016  \\\n",
       "QOL01086.1  0.009926 -0.074646 -0.043121 -0.054077  ... -0.049164 -0.023560   \n",
       "BBG98846.1  0.007996 -0.008774 -0.032104  0.061371  ...  0.006680  0.029663   \n",
       "UOB18734.1  0.069031 -0.054413 -0.088379  0.014938  ... -0.008293 -0.030853   \n",
       "BAU42234.1  0.026962  0.006710 -0.076721  0.045898  ... -0.020416 -0.019775   \n",
       "UKZ59741.1 -0.032684 -0.027451 -0.080322 -0.059509  ... -0.014549  0.002020   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "AGQ53951.1  0.029556 -0.036926 -0.059082  0.024155  ... -0.012581 -0.001436   \n",
       "AKK12048.1  0.021698 -0.085632 -0.047546  0.009430  ... -0.056641 -0.033875   \n",
       "ALR14359.1  0.026108 -0.002766 -0.076538  0.030563  ... -0.004440 -0.024658   \n",
       "UAY05060.1  0.069153 -0.035309 -0.058472 -0.021759  ... -0.005718  0.023819   \n",
       "QRG65564.1  0.053711 -0.053986 -0.039124  0.027481  ... -0.011307  0.016083   \n",
       "\n",
       "             emb1017   emb1018   emb1019   emb1020   emb1021   emb1022  \\\n",
       "QOL01086.1 -0.031158  0.071106  0.025864 -0.001612  0.008064  0.061768   \n",
       "BBG98846.1 -0.050018  0.052002  0.009552  0.007038  0.036499  0.015991   \n",
       "UOB18734.1 -0.055084  0.054749  0.027939 -0.001834  0.000238  0.035553   \n",
       "BAU42234.1 -0.055206  0.014702  0.022125 -0.018082 -0.006020  0.015549   \n",
       "UKZ59741.1 -0.006351  0.072266 -0.025681  0.022720  0.039886  0.019455   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "AGQ53951.1 -0.052856  0.017151 -0.021179 -0.017365 -0.042297  0.018311   \n",
       "AKK12048.1 -0.043243 -0.002583  0.048615  0.001557 -0.013275  0.016235   \n",
       "ALR14359.1 -0.077637 -0.032532  0.003077 -0.020401 -0.058777  0.007858   \n",
       "UAY05060.1 -0.091919  0.043884  0.018112 -0.016647 -0.064270  0.037750   \n",
       "QRG65564.1 -0.057251  0.058777  0.103638 -0.015541 -0.004276  0.023926   \n",
       "\n",
       "             emb1023  cazy  \n",
       "QOL01086.1  0.048798   1.0  \n",
       "BBG98846.1  0.032928   1.0  \n",
       "UOB18734.1  0.029160   1.0  \n",
       "BAU42234.1  0.081116   1.0  \n",
       "UKZ59741.1  0.022537   1.0  \n",
       "...              ...   ...  \n",
       "AGQ53951.1  0.012939   1.0  \n",
       "AKK12048.1  0.047058   1.0  \n",
       "ALR14359.1 -0.004440   1.0  \n",
       "UAY05060.1  0.057495   1.0  \n",
       "QRG65564.1  0.032776   1.0  \n",
       "\n",
       "[5001 rows x 1026 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cazy_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create training dataframe + train a simple MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative training data is stored in `non_cazy_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cazy_train_df = non_cazy_df.sample(n=10000, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Using PUL file entirely for inference; `train_df` for training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Concatenate non-cazy and cazy training dataframes (size = 10000 + 5001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([cazy_train_df, non_cazy_train_df], axis=0).iloc[:,1:] #removes 'class' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(train_df.iloc[:,:-1]), np.array(train_df.iloc[:,-1:])\n",
    "X_test, y_test = np.array(PUL_df.iloc[:,:-1]), np.array(PUL_df.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP: 2 layers, each with 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.23759776\n",
      "Iteration 2, loss = 0.04788831\n",
      "Iteration 3, loss = 0.03467691\n",
      "Iteration 4, loss = 0.02716519\n",
      "Iteration 5, loss = 0.02287360\n",
      "Iteration 6, loss = 0.01954641\n",
      "Iteration 7, loss = 0.01579857\n",
      "Iteration 8, loss = 0.01361286\n",
      "Iteration 9, loss = 0.01144769\n",
      "Iteration 10, loss = 0.00864978\n",
      "Iteration 11, loss = 0.00764976\n",
      "Iteration 12, loss = 0.00568887\n",
      "Iteration 13, loss = 0.00458982\n",
      "Iteration 14, loss = 0.00392541\n",
      "Iteration 15, loss = 0.00299054\n",
      "Iteration 16, loss = 0.00239206\n",
      "Iteration 17, loss = 0.00158386\n",
      "Iteration 18, loss = 0.00115602\n",
      "Iteration 19, loss = 0.00276790\n",
      "Iteration 20, loss = 0.00110579\n",
      "Iteration 21, loss = 0.00060842\n",
      "Iteration 22, loss = 0.00046610\n",
      "Iteration 23, loss = 0.00040026\n",
      "Iteration 24, loss = 0.00036738\n",
      "Iteration 25, loss = 0.00034347\n",
      "Iteration 26, loss = 0.00031856\n",
      "Iteration 27, loss = 0.00030618\n",
      "Iteration 28, loss = 0.00029441\n",
      "Iteration 29, loss = 0.00028042\n",
      "Iteration 30, loss = 0.00027351\n",
      "Iteration 31, loss = 0.00026209\n",
      "Iteration 32, loss = 0.00025560\n",
      "Iteration 33, loss = 0.00024944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100),\n",
    "                     activation = 'relu',\n",
    "                     solver = 'adam',\n",
    "                     verbose = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7468502402909468\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             4101         1933\n",
      "Cazyme                   16         1649\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "#confusion matrix\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 layer**:\n",
    "- Using PUL file entirely for inference produces terrible results (59.2%); worse than trivial classifier (78.4%) - why??\n",
    "- positive prediction is poor\n",
    "- Increasing number of layers gives better results (10 layers: 75.1%) for (a) but worse for (c) (10 layers: 80.8%) (why)\n",
    "\n",
    "**2 layers:**\n",
    "- relu activation: 61.7%; \n",
    "\n",
    "**3 layers:**\n",
    "- relu activaiton: 61.9%; (negative prediction 60.0%; positive 68.7%)\n",
    "\n",
    "**4 layers:**\n",
    " - relu: 72.6% (10,10,10,10)\n",
    "\n",
    " **5 layers:**\n",
    "- relu: 59.4%, but positive prediction much better (71.2%) than negative (56.1%)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivial Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trivial classifier =  [0.7788961]\n"
     ]
    }
   ],
   "source": [
    "s=sum(y_test)\n",
    "p=len(y_test)\n",
    "#predict all zeros:\n",
    "trivial_score = (p-s)/p\n",
    "print('Accuracy of trivial classifier = ',trivial_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = mlp.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('false positive rate')\n",
    "# plt.ylabel('true positive rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_neg, tpr_neg, thresholds_neg = roc_curve(y_test, y_prob[:,0], pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr_neg, tpr_neg)\n",
    "plt.xlabel('false negative rate')\n",
    "plt.ylabel('true negative rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.681482012340481\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(y_test, y_prob[:,1])\n",
    "print('auc = ', auc_score)\n",
    "\n",
    "# auc_score_neg = roc_auc_score((np.ones(np.shape(y_test))-y_test), y_prob[:,0])\n",
    "# print('negative auc score = ', auc_score_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score of 0.68; quite poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [(b) Use PUL file (60/40) + `train_df` for training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = np.zeros(PUL_df.shape[0])\n",
    "train_ind[:int(np.floor(PUL_df.shape[0]*0.6))] = 1 #prop. train\n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "X_PUL, y_PUL = PUL_df.to_numpy()[:,:-1], PUL_df.to_numpy()[:,-1:]\n",
    "\n",
    "X_PUL_train, y_PUL_train = X_PUL[train_ind==1,:], y_PUL[train_ind==1]\n",
    "X_PUL_test, y_PUL_test = X_PUL[train_ind==0,:], y_PUL[train_ind==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, y1_train = np.concatenate([np.array(train_df.iloc[:,:-1]), X_PUL_train]), np.concatenate([np.array(train_df.iloc[:,-1:]), y_PUL_train])\n",
    "X1_test, y1_test = X_PUL_test, y_PUL_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55924477\n",
      "Iteration 2, loss = 0.29139208\n",
      "Iteration 3, loss = 0.22117616\n",
      "Iteration 4, loss = 0.20915026\n",
      "Iteration 5, loss = 0.20070854\n",
      "Iteration 6, loss = 0.19331638\n",
      "Iteration 7, loss = 0.18711276\n",
      "Iteration 8, loss = 0.18265605\n",
      "Iteration 9, loss = 0.17920480\n",
      "Iteration 10, loss = 0.17504394\n",
      "Iteration 11, loss = 0.17313134\n",
      "Iteration 12, loss = 0.16985304\n",
      "Iteration 13, loss = 0.16765605\n",
      "Iteration 14, loss = 0.16487077\n",
      "Iteration 15, loss = 0.16344621\n",
      "Iteration 16, loss = 0.16196618\n",
      "Iteration 17, loss = 0.16021931\n",
      "Iteration 18, loss = 0.15801320\n",
      "Iteration 19, loss = 0.15618093\n",
      "Iteration 20, loss = 0.15528583\n",
      "Iteration 21, loss = 0.15392939\n",
      "Iteration 22, loss = 0.15088660\n",
      "Iteration 23, loss = 0.14992218\n",
      "Iteration 24, loss = 0.14986731\n",
      "Iteration 25, loss = 0.14754133\n",
      "Iteration 26, loss = 0.14511670\n",
      "Iteration 27, loss = 0.14479812\n",
      "Iteration 28, loss = 0.14488288\n",
      "Iteration 29, loss = 0.14122364\n",
      "Iteration 30, loss = 0.14026534\n",
      "Iteration 31, loss = 0.13930781\n",
      "Iteration 32, loss = 0.13640778\n",
      "Iteration 33, loss = 0.13719185\n",
      "Iteration 34, loss = 0.13446066\n",
      "Iteration 35, loss = 0.13298580\n",
      "Iteration 36, loss = 0.13135012\n",
      "Iteration 37, loss = 0.13188630\n",
      "Iteration 38, loss = 0.12911649\n",
      "Iteration 39, loss = 0.12813363\n",
      "Iteration 40, loss = 0.12666047\n",
      "Iteration 41, loss = 0.12516994\n",
      "Iteration 42, loss = 0.12406269\n",
      "Iteration 43, loss = 0.12308235\n",
      "Iteration 44, loss = 0.12151294\n",
      "Iteration 45, loss = 0.11922356\n",
      "Iteration 46, loss = 0.11866244\n",
      "Iteration 47, loss = 0.11808564\n",
      "Iteration 48, loss = 0.11761269\n",
      "Iteration 49, loss = 0.11580052\n",
      "Iteration 50, loss = 0.11326819\n",
      "Iteration 51, loss = 0.11407523\n",
      "Iteration 52, loss = 0.11182439\n",
      "Iteration 53, loss = 0.11029862\n",
      "Iteration 54, loss = 0.10888584\n",
      "Iteration 55, loss = 0.10844928\n",
      "Iteration 56, loss = 0.10876150\n",
      "Iteration 57, loss = 0.10750824\n",
      "Iteration 58, loss = 0.10741558\n",
      "Iteration 59, loss = 0.10495868\n",
      "Iteration 60, loss = 0.10448230\n",
      "Iteration 61, loss = 0.10420631\n",
      "Iteration 62, loss = 0.10199502\n",
      "Iteration 63, loss = 0.10085547\n",
      "Iteration 64, loss = 0.10107114\n",
      "Iteration 65, loss = 0.10242165\n",
      "Iteration 66, loss = 0.09814789\n",
      "Iteration 67, loss = 0.09652970\n",
      "Iteration 68, loss = 0.09663775\n",
      "Iteration 69, loss = 0.09559004\n",
      "Iteration 70, loss = 0.09605898\n",
      "Iteration 71, loss = 0.09414653\n",
      "Iteration 72, loss = 0.09518098\n",
      "Iteration 73, loss = 0.09614531\n",
      "Iteration 74, loss = 0.09080549\n",
      "Iteration 75, loss = 0.09107247\n",
      "Iteration 76, loss = 0.08948247\n",
      "Iteration 77, loss = 0.09144564\n",
      "Iteration 78, loss = 0.09295039\n",
      "Iteration 79, loss = 0.09076427\n",
      "Iteration 80, loss = 0.08857596\n",
      "Iteration 81, loss = 0.09042773\n",
      "Iteration 82, loss = 0.08933098\n",
      "Iteration 83, loss = 0.08707305\n",
      "Iteration 84, loss = 0.08993337\n",
      "Iteration 85, loss = 0.08601673\n",
      "Iteration 86, loss = 0.08589219\n",
      "Iteration 87, loss = 0.08653173\n",
      "Iteration 88, loss = 0.08531895\n",
      "Iteration 89, loss = 0.08442138\n",
      "Iteration 90, loss = 0.08507588\n",
      "Iteration 91, loss = 0.08422397\n",
      "Iteration 92, loss = 0.08059345\n",
      "Iteration 93, loss = 0.08069802\n",
      "Iteration 94, loss = 0.08384620\n",
      "Iteration 95, loss = 0.08058050\n",
      "Iteration 96, loss = 0.08068472\n",
      "Iteration 97, loss = 0.08064427\n",
      "Iteration 98, loss = 0.08195625\n",
      "Iteration 99, loss = 0.07937596\n",
      "Iteration 100, loss = 0.08199007\n",
      "Iteration 101, loss = 0.07828719\n",
      "Iteration 102, loss = 0.07953981\n",
      "Iteration 103, loss = 0.07562774\n",
      "Iteration 104, loss = 0.08387362\n",
      "Iteration 105, loss = 0.08284704\n",
      "Iteration 106, loss = 0.07914355\n",
      "Iteration 107, loss = 0.07519486\n",
      "Iteration 108, loss = 0.07373464\n",
      "Iteration 109, loss = 0.07512293\n",
      "Iteration 110, loss = 0.07511697\n",
      "Iteration 111, loss = 0.08343873\n",
      "Iteration 112, loss = 0.07410545\n",
      "Iteration 113, loss = 0.07289695\n",
      "Iteration 114, loss = 0.07082442\n",
      "Iteration 115, loss = 0.07052561\n",
      "Iteration 116, loss = 0.07342104\n",
      "Iteration 117, loss = 0.07294571\n",
      "Iteration 118, loss = 0.07007540\n",
      "Iteration 119, loss = 0.07338114\n",
      "Iteration 120, loss = 0.07186557\n",
      "Iteration 121, loss = 0.06959671\n",
      "Iteration 122, loss = 0.07343326\n",
      "Iteration 123, loss = 0.07020582\n",
      "Iteration 124, loss = 0.06939228\n",
      "Iteration 125, loss = 0.06829622\n",
      "Iteration 126, loss = 0.06640320\n",
      "Iteration 127, loss = 0.06787265\n",
      "Iteration 128, loss = 0.06615104\n",
      "Iteration 129, loss = 0.06796794\n",
      "Iteration 130, loss = 0.06908494\n",
      "Iteration 131, loss = 0.06725186\n",
      "Iteration 132, loss = 0.06795441\n",
      "Iteration 133, loss = 0.06832567\n",
      "Iteration 134, loss = 0.06915430\n",
      "Iteration 135, loss = 0.07113305\n",
      "Iteration 136, loss = 0.06854727\n",
      "Iteration 137, loss = 0.06254273\n",
      "Iteration 138, loss = 0.06412740\n",
      "Iteration 139, loss = 0.06333927\n",
      "Iteration 140, loss = 0.06317426\n",
      "Iteration 141, loss = 0.06334952\n",
      "Iteration 142, loss = 0.06280923\n",
      "Iteration 143, loss = 0.06084453\n",
      "Iteration 144, loss = 0.06480564\n",
      "Iteration 145, loss = 0.06071911\n",
      "Iteration 146, loss = 0.05883010\n",
      "Iteration 147, loss = 0.06432337\n",
      "Iteration 148, loss = 0.07045332\n",
      "Iteration 149, loss = 0.06437956\n",
      "Iteration 150, loss = 0.05918374\n",
      "Iteration 151, loss = 0.06119153\n",
      "Iteration 152, loss = 0.05850180\n",
      "Iteration 153, loss = 0.05844482\n",
      "Iteration 154, loss = 0.06038813\n",
      "Iteration 155, loss = 0.06270840\n",
      "Iteration 156, loss = 0.05983144\n",
      "Iteration 157, loss = 0.06015369\n",
      "Iteration 158, loss = 0.06082673\n",
      "Iteration 159, loss = 0.05887568\n",
      "Iteration 160, loss = 0.05928668\n",
      "Iteration 161, loss = 0.05462650\n",
      "Iteration 162, loss = 0.05350442\n",
      "Iteration 163, loss = 0.06321139\n",
      "Iteration 164, loss = 0.05735028\n",
      "Iteration 165, loss = 0.05702334\n",
      "Iteration 166, loss = 0.05804177\n",
      "Iteration 167, loss = 0.05738718\n",
      "Iteration 168, loss = 0.05663722\n",
      "Iteration 169, loss = 0.06075030\n",
      "Iteration 170, loss = 0.05641776\n",
      "Iteration 171, loss = 0.05954658\n",
      "Iteration 172, loss = 0.08143264\n",
      "Iteration 173, loss = 0.05777054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,10,10,10,10),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    verbose = True).fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = mlp1.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412337662337662\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             1938          461\n",
      "Cazyme                  336          345\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y1_test,y1_pred))\n",
    "#confusion matrix\n",
    "mat = confusion_matrix(y1_test, y1_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 layer:**\n",
    "\n",
    "- 66.4% accuracy; still much worse than trivial classifier\n",
    "\n",
    "- Positive prediction really poor\n",
    "\n",
    "**2 layers:** (100,100)\n",
    "\n",
    "- Relu: 73.6%\n",
    "\n",
    "**3 layers:** (100,100,100)\n",
    "\n",
    "- Relu: 76.7% , poor positive prediction (<50%)\n",
    "\n",
    "**4:**\n",
    "- Relu: 75.2%, poor positive prediction (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Only use PUL file for training and testing: (60/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, y2_train = X_PUL_train, y_PUL_train\n",
    "X2_test, y2_test = X_PUL_test, y_PUL_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55567637\n",
      "Iteration 2, loss = 0.48351977\n",
      "Iteration 3, loss = 0.46832773\n",
      "Iteration 4, loss = 0.46029756\n",
      "Iteration 5, loss = 0.45292093\n",
      "Iteration 6, loss = 0.44632938\n",
      "Iteration 7, loss = 0.43771742\n",
      "Iteration 8, loss = 0.43126449\n",
      "Iteration 9, loss = 0.42326714\n",
      "Iteration 10, loss = 0.41405291\n",
      "Iteration 11, loss = 0.40557543\n",
      "Iteration 12, loss = 0.39433457\n",
      "Iteration 13, loss = 0.38140771\n",
      "Iteration 14, loss = 0.37806976\n",
      "Iteration 15, loss = 0.36222290\n",
      "Iteration 16, loss = 0.34947108\n",
      "Iteration 17, loss = 0.33497619\n",
      "Iteration 18, loss = 0.33345877\n",
      "Iteration 19, loss = 0.31490704\n",
      "Iteration 20, loss = 0.30467610\n",
      "Iteration 21, loss = 0.29570629\n",
      "Iteration 22, loss = 0.29469141\n",
      "Iteration 23, loss = 0.28360093\n",
      "Iteration 24, loss = 0.26760903\n",
      "Iteration 25, loss = 0.25455942\n",
      "Iteration 26, loss = 0.25855992\n",
      "Iteration 27, loss = 0.24419306\n",
      "Iteration 28, loss = 0.23548279\n",
      "Iteration 29, loss = 0.23478562\n",
      "Iteration 30, loss = 0.22276626\n",
      "Iteration 31, loss = 0.21173582\n",
      "Iteration 32, loss = 0.21055562\n",
      "Iteration 33, loss = 0.21046419\n",
      "Iteration 34, loss = 0.20013969\n",
      "Iteration 35, loss = 0.21819698\n",
      "Iteration 36, loss = 0.19353431\n",
      "Iteration 37, loss = 0.18025912\n",
      "Iteration 38, loss = 0.17164254\n",
      "Iteration 39, loss = 0.17105498\n",
      "Iteration 40, loss = 0.17500035\n",
      "Iteration 41, loss = 0.17241933\n",
      "Iteration 42, loss = 0.15471744\n",
      "Iteration 43, loss = 0.15360922\n",
      "Iteration 44, loss = 0.15065597\n",
      "Iteration 45, loss = 0.14810023\n",
      "Iteration 46, loss = 0.15736140\n",
      "Iteration 47, loss = 0.14480004\n",
      "Iteration 48, loss = 0.13034247\n",
      "Iteration 49, loss = 0.12963461\n",
      "Iteration 50, loss = 0.14007803\n",
      "Iteration 51, loss = 0.14243194\n",
      "Iteration 52, loss = 0.13294739\n",
      "Iteration 53, loss = 0.12943857\n",
      "Iteration 54, loss = 0.11551450\n",
      "Iteration 55, loss = 0.12083270\n",
      "Iteration 56, loss = 0.12136853\n",
      "Iteration 57, loss = 0.12196392\n",
      "Iteration 58, loss = 0.12024021\n",
      "Iteration 59, loss = 0.11156580\n",
      "Iteration 60, loss = 0.12101500\n",
      "Iteration 61, loss = 0.11448066\n",
      "Iteration 62, loss = 0.12431444\n",
      "Iteration 63, loss = 0.11711987\n",
      "Iteration 64, loss = 0.11493320\n",
      "Iteration 65, loss = 0.12315288\n",
      "Iteration 66, loss = 0.12576551\n",
      "Iteration 67, loss = 0.10335898\n",
      "Iteration 68, loss = 0.09599847\n",
      "Iteration 69, loss = 0.09642549\n",
      "Iteration 70, loss = 0.09486623\n",
      "Iteration 71, loss = 0.09304361\n",
      "Iteration 72, loss = 0.09445703\n",
      "Iteration 73, loss = 0.10010370\n",
      "Iteration 74, loss = 0.08909470\n",
      "Iteration 75, loss = 0.08864664\n",
      "Iteration 76, loss = 0.10703056\n",
      "Iteration 77, loss = 0.09306277\n",
      "Iteration 78, loss = 0.08679808\n",
      "Iteration 79, loss = 0.08557771\n",
      "Iteration 80, loss = 0.09101618\n",
      "Iteration 81, loss = 0.08200378\n",
      "Iteration 82, loss = 0.10554619\n",
      "Iteration 83, loss = 0.09939182\n",
      "Iteration 84, loss = 0.08138967\n",
      "Iteration 85, loss = 0.07835039\n",
      "Iteration 86, loss = 0.08435137\n",
      "Iteration 87, loss = 0.08920643\n",
      "Iteration 88, loss = 0.08570269\n",
      "Iteration 89, loss = 0.09739528\n",
      "Iteration 90, loss = 0.09820611\n",
      "Iteration 91, loss = 0.08353258\n",
      "Iteration 92, loss = 0.09490031\n",
      "Iteration 93, loss = 0.08004158\n",
      "Iteration 94, loss = 0.07103520\n",
      "Iteration 95, loss = 0.08222889\n",
      "Iteration 96, loss = 0.08732374\n",
      "Iteration 97, loss = 0.08306501\n",
      "Iteration 98, loss = 0.07541736\n",
      "Iteration 99, loss = 0.07820552\n",
      "Iteration 100, loss = 0.09644322\n",
      "Iteration 101, loss = 0.07618397\n",
      "Iteration 102, loss = 0.08756649\n",
      "Iteration 103, loss = 0.07975766\n",
      "Iteration 104, loss = 0.09161657\n",
      "Iteration 105, loss = 0.08153388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    verbose = True).fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = mlp2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707792207792208\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             2048          351\n",
      "Cazyme                  355          326\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y2_test,y2_pred))\n",
    "mat = confusion_matrix(y2_test, y2_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 layer (10 neurons): 80.7% accuracy, \n",
    "\n",
    "- 2 layers: 78.3% accuracy (100,100); 78.7% (10,10)\n",
    "- poor positive prediction\n",
    "\n",
    "- 3 layers: 79.7% (100,100,100); 78.2% (10,10,10) \n",
    "\n",
    "- 4 layers: 77.0% (10,10,10,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fewer neurons lead to poorer positive prediction (why?)\n",
    "- (10,10): 35.4%\n",
    "- (3,3): 30.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y2_prob = mlp.predict_proba(X2_test)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y2_test, y2_prob[:,0], pos_label=0)\n",
    "plt.plot(fpr2, tpr2)\n",
    "plt.xlabel('false negative rate')\n",
    "plt.ylabel('true negative rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.694573852663769\n"
     ]
    }
   ],
   "source": [
    "auc2_score = roc_auc_score(y2_test, y2_prob[:,1])\n",
    "print('auc = ', auc2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score = 0.69; not much better than in (a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
