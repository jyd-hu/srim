{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP using all of the data provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to explore the classification of proteins into Carbohydrate Active enzymes (CAZymes) and non-CAZymes. A simple MLP is trained on a dataset of classified non-PULs (Polysaccharide Utilisation Loci) using sklearn, then applied to the dataset of PULs. The methods and results are presented below, analysing the model's ability to accurately classify proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re as re\n",
    "import math\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load all files & put into dataframes\n",
    "\n",
    "Negative training data:\n",
    "- `pfam_training_data_augment.h5`\n",
    "- `non_cazy_kegg.h5`\n",
    "\n",
    "Positive training data:\n",
    "- `vicreg_train_val_embeddings_noCAZOME_noLargeSeqs_combined.h5` (+ use `cazy_family_by_taxa_60.json` to pick a representative sample across all enzyme classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUL_embeddings = []\n",
    "PUL_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\PUL.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        PUL_embeddings.append(np.array(f[key][()]))\n",
    "        PUL_keys.append(key)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PFAM_embeddings = []\n",
    "PFAM_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\pfam_training_data_augment.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        PFAM_embeddings.append(f[key][()])\n",
    "        PFAM_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_embeddings = []\n",
    "kegg_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\non_cazy_kegg.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        kegg_embeddings.append(f[key][()])\n",
    "        kegg_keys.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the PULs are stored in `PUL_df`, where the last column indicates if each protein is a CAZyme (1) or not (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUL file:\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "f_path = 'C:\\\\Users\\\\alpha\\\\OneDrive - University of Cambridge\\\\BACKUP 14-04-22\\\\docs\\\\Maths\\\\SRIM\\\\code\\\\PUL.faa'\n",
    "PUL_array, PUL_keys2 = [], []\n",
    "\n",
    "with open(f_path, mode='r') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        identifier, description = record.id, record.description\n",
    "        PUL_keys2.append(identifier)\n",
    "        if 'CAZyme' in description:\n",
    "            PUL_array.append(1)\n",
    "        else:\n",
    "            PUL_array.append(0)\n",
    "\n",
    "PUL_array = np.array(PUL_array)\n",
    "PUL_array = PUL_array.reshape(np.shape(PUL_array)[0],-1)\n",
    "PUL_array_df = pd.DataFrame(PUL_array, index=PUL_keys2,columns=['cazy'])\n",
    "\n",
    "col_label=['emb'+str(i) for i in range(len(list(PUL_embeddings)[0]))]\n",
    "# col_label.append('cazy')\n",
    "\n",
    "PUL_embeddings_list=list(PUL_embeddings)\n",
    "temp_df = pd.DataFrame(PUL_embeddings_list, index=PUL_keys, columns=['emb'+str(i) for i in range(len(list(PUL_embeddings)[0]))])\n",
    "\n",
    "# PUL_df = pd.DataFrame(data=np.concatenate([PUL_embeddings,PUL_array], axis=1), index=PUL_keys2, columns=col_label)\n",
    "PUL_df = temp_df.join(PUL_array_df)\n",
    "\n",
    "#indexing issue fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>emb9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1015</th>\n",
       "      <th>emb1016</th>\n",
       "      <th>emb1017</th>\n",
       "      <th>emb1018</th>\n",
       "      <th>emb1019</th>\n",
       "      <th>emb1020</th>\n",
       "      <th>emb1021</th>\n",
       "      <th>emb1022</th>\n",
       "      <th>emb1023</th>\n",
       "      <th>cazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PUL0001_1</th>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>-0.032013</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>-0.044128</td>\n",
       "      <td>-0.086609</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049255</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>-0.111633</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>0.030334</td>\n",
       "      <td>-0.028244</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>0.025970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_10</th>\n",
       "      <td>-0.014236</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.034546</td>\n",
       "      <td>-0.026245</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>-0.001535</td>\n",
       "      <td>-0.026840</td>\n",
       "      <td>-0.044830</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>-0.120483</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>-0.005569</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_11</th>\n",
       "      <td>-0.019104</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>0.051361</td>\n",
       "      <td>0.030670</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.076233</td>\n",
       "      <td>0.047943</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042297</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.089661</td>\n",
       "      <td>0.064331</td>\n",
       "      <td>-0.042755</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_12</th>\n",
       "      <td>0.047211</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>-0.008369</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>-0.002466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.036804</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>-0.035187</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>0.069641</td>\n",
       "      <td>-0.001657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_13</th>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.021515</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>0.024506</td>\n",
       "      <td>-0.014908</td>\n",
       "      <td>-0.058167</td>\n",
       "      <td>-0.014183</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>-0.064758</td>\n",
       "      <td>0.049774</td>\n",
       "      <td>-0.009315</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>0.060211</td>\n",
       "      <td>0.017761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_5</th>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.035370</td>\n",
       "      <td>-0.035553</td>\n",
       "      <td>-0.049744</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006264</td>\n",
       "      <td>-0.016479</td>\n",
       "      <td>-0.024261</td>\n",
       "      <td>0.067261</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>-0.026321</td>\n",
       "      <td>0.032532</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>0.066528</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_6</th>\n",
       "      <td>0.009483</td>\n",
       "      <td>0.044464</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>-0.036957</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>-0.014572</td>\n",
       "      <td>-0.093689</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.013908</td>\n",
       "      <td>-0.104553</td>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>-0.053558</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_7</th>\n",
       "      <td>0.051819</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>-0.028381</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>-0.026932</td>\n",
       "      <td>-0.030212</td>\n",
       "      <td>0.031433</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>-0.069275</td>\n",
       "      <td>0.019424</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>-0.048309</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>-0.010933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_8</th>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.047760</td>\n",
       "      <td>0.041046</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.082153</td>\n",
       "      <td>-0.036713</td>\n",
       "      <td>-0.099487</td>\n",
       "      <td>-0.023865</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015564</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>-0.069214</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.011467</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>0.034729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_9</th>\n",
       "      <td>0.024521</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.020950</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>-0.080750</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>-0.006287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019974</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>0.042999</td>\n",
       "      <td>0.024536</td>\n",
       "      <td>-0.007603</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>0.062805</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                emb0      emb1      emb2      emb3      emb4      emb5  \\\n",
       "PUL0001_1   0.017563  0.059692  0.030075  0.007584 -0.032013  0.063904   \n",
       "PUL0001_10 -0.014236  0.032715  0.034546 -0.026245  0.007820 -0.001535   \n",
       "PUL0001_11 -0.019104  0.034027  0.051361  0.030670 -0.013756  0.012695   \n",
       "PUL0001_12  0.047211  0.002373  0.000755  0.020142 -0.008369  0.028976   \n",
       "PUL0001_13  0.024719  0.031494  0.027359  0.021515 -0.007629  0.024506   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "PUL0602_5   0.017685  0.051422  0.006428 -0.001532  0.006233  0.035370   \n",
       "PUL0602_6   0.009483  0.044464  0.050842 -0.036957  0.004826 -0.014664   \n",
       "PUL0602_7   0.051819  0.009346  0.015388 -0.028381  0.013084  0.042877   \n",
       "PUL0602_8   0.032349  0.047760  0.041046 -0.033142  0.004124  0.082153   \n",
       "PUL0602_9   0.024521  0.037872  0.005112 -0.020950  0.020767  0.042908   \n",
       "\n",
       "                emb6      emb7      emb8      emb9  ...   emb1015   emb1016  \\\n",
       "PUL0001_1  -0.044128 -0.086609  0.033752 -0.031281  ... -0.049255 -0.006824   \n",
       "PUL0001_10 -0.026840 -0.044830 -0.022736  0.003244  ... -0.020203  0.010963   \n",
       "PUL0001_11 -0.019608 -0.076233  0.047943  0.000046  ... -0.042297 -0.000536   \n",
       "PUL0001_12 -0.000050 -0.083252  0.017487 -0.002466  ... -0.031281 -0.036804   \n",
       "PUL0001_13 -0.014908 -0.058167 -0.014183 -0.020538  ...  0.004028  0.015884   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "PUL0602_5  -0.035553 -0.049744  0.028076  0.002031  ... -0.006264 -0.016479   \n",
       "PUL0602_6  -0.014572 -0.093689 -0.026871  0.017822  ... -0.001588  0.013908   \n",
       "PUL0602_7  -0.026932 -0.030212  0.031433  0.006268  ... -0.001817  0.013542   \n",
       "PUL0602_8  -0.036713 -0.099487 -0.023865  0.007652  ... -0.015564  0.004181   \n",
       "PUL0602_9  -0.052765 -0.080750 -0.012650 -0.006287  ... -0.019974  0.022400   \n",
       "\n",
       "             emb1017   emb1018   emb1019   emb1020   emb1021   emb1022  \\\n",
       "PUL0001_1  -0.111633  0.083191  0.030334 -0.028244 -0.008194  0.031616   \n",
       "PUL0001_10 -0.120483  0.041931  0.013710  0.003368 -0.005569  0.021667   \n",
       "PUL0001_11 -0.089661  0.064331 -0.042755 -0.018158 -0.025391  0.001602   \n",
       "PUL0001_12 -0.075195  0.022964  0.014938 -0.035187 -0.028992  0.069641   \n",
       "PUL0001_13 -0.064758  0.049774 -0.009315  0.007256  0.006168  0.060211   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "PUL0602_5  -0.024261  0.067261  0.056061 -0.026321  0.032532 -0.009926   \n",
       "PUL0602_6  -0.104553  0.027496  0.050049 -0.053558  0.024475  0.020248   \n",
       "PUL0602_7  -0.069275  0.019424 -0.009377 -0.048309  0.009567  0.007690   \n",
       "PUL0602_8  -0.069214  0.032410  0.000823  0.000344 -0.011467  0.043060   \n",
       "PUL0602_9  -0.098633  0.042999  0.024536 -0.007603 -0.011642  0.062805   \n",
       "\n",
       "             emb1023  cazy  \n",
       "PUL0001_1   0.025970     1  \n",
       "PUL0001_10 -0.024216     0  \n",
       "PUL0001_11  0.011215     0  \n",
       "PUL0001_12 -0.001657     0  \n",
       "PUL0001_13  0.017761     1  \n",
       "...              ...   ...  \n",
       "PUL0602_5   0.066528     1  \n",
       "PUL0602_6   0.038025     0  \n",
       "PUL0602_7  -0.010933     0  \n",
       "PUL0602_8   0.034729     0  \n",
       "PUL0602_9   0.114685     0  \n",
       "\n",
       "[7699 rows x 1025 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUL_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-PUL negative (non-CAZyme) data is stored in `non_cazy_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-cazymes:\n",
    "non_cazy_embeddings = np.concatenate([PFAM_embeddings,kegg_embeddings], axis=0)\n",
    "non_cazy_keys = np.concatenate([PFAM_keys, kegg_keys], axis=0)\n",
    "non_cazy_df = pd.DataFrame(data=non_cazy_embeddings, index=non_cazy_keys, columns=['emb'+str(i) for i in range(len(list(non_cazy_embeddings)[0]))])\n",
    "non_cazy_df = non_cazy_df.join(pd.DataFrame(data=np.zeros(np.array(non_cazy_keys).shape[0]), index=non_cazy_keys, columns=['cazy']), how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_embeddings = []\n",
    "cazy_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\vicreg_train_val_embeddings_noCAZOME_noLargeSeqs_combined.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        cazy_embeddings.append(f[key][()])\n",
    "        cazy_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(PUL_embeddings))\n",
    "# print(np.shape(non_cazy_embeddings))\n",
    "# print(np.shape(cazy_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create lookup using .json file\n",
    "\n",
    "To obtain a representative sample of CAZymes to use as positive training data, we create a lookup for each class using `cazy_family_by_taxa_60.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_ids = []\n",
    "family_keys = []\n",
    "file = open('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\cazy_family_by_taxa_60.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_ids = []\n",
    "cazy_families = []\n",
    "file = open('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\cazy_family_by_taxa_60.json')\n",
    "data = json.loads(file.read())\n",
    "\n",
    "for key in data.keys(): #keys are IDs, values are classes\n",
    "        cazy_ids.append(key)\n",
    "        cazy_families.append(data.get(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cazy_df = pd.DataFrame(data=concat , columns=['id','class'])\n",
    "cazy_df_2 = pd.DataFrame(data=cazy_families, index=cazy_ids , columns=['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe of cazymes with extra columnns containing embeddings, then inner join with cazy_df_2 (intersect dataframes at indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_df_1 = pd.DataFrame(data=cazy_embeddings, index=cazy_keys, columns=['emb'+str(i) for i in range(len(list(cazy_embeddings)[0]))])\n",
    "\n",
    "# long (~12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             class      emb0      emb1      emb2      emb3      emb4  \\\n",
      "AZS17016.1  GH13_2  0.049500  0.037415 -0.038025  0.031235  0.017197   \n",
      "AVO05213.1    GH32 -0.014870  0.036865  0.009872 -0.011703  0.015808   \n",
      "QPG94344.1    GT61  0.016739 -0.028412  0.026016  0.017227  0.013565   \n",
      "APD47233.1     GT4  0.015976 -0.009315 -0.032532  0.033173  0.011223   \n",
      "ANU32067.1     GT4  0.010872 -0.021881 -0.008972 -0.013237  0.000924   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "QMI07227.1    GT83 -0.008034 -0.009453  0.029785  0.033478  0.024063   \n",
      "UMA63307.1     GT2  0.019272 -0.044830  0.024490  0.026031  0.026672   \n",
      "UPW01296.1     GT4  0.055878 -0.059723 -0.039551 -0.001044  0.057983   \n",
      "ULL17023.1    GH43  0.013275  0.033997  0.013687  0.021133  0.018799   \n",
      "USF23742.1     GT9  0.022705 -0.043182 -0.023972  0.007046 -0.025818   \n",
      "\n",
      "                emb5      emb6      emb7      emb8  ...   emb1014   emb1015  \\\n",
      "AZS17016.1  0.031616 -0.019043 -0.046234  0.007179  ... -0.040833 -0.034882   \n",
      "AVO05213.1 -0.001546  0.005375 -0.102600 -0.002607  ... -0.015167 -0.013428   \n",
      "QPG94344.1  0.033905 -0.067627 -0.064880 -0.005409  ... -0.072754 -0.009453   \n",
      "APD47233.1  0.055298 -0.019485 -0.062164  0.036804  ... -0.027649 -0.028488   \n",
      "ANU32067.1  0.037537 -0.050995 -0.051697  0.014412  ... -0.032379 -0.040771   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "QMI07227.1  0.021744 -0.033081 -0.051086  0.059387  ... -0.012657 -0.030960   \n",
      "UMA63307.1  0.003311 -0.064575 -0.080627 -0.030411  ...  0.004433 -0.040588   \n",
      "UPW01296.1  0.034698 -0.031525 -0.088623  0.003918  ... -0.030212  0.000360   \n",
      "ULL17023.1  0.044891 -0.020218 -0.037231  0.026855  ... -0.012169 -0.036713   \n",
      "USF23742.1  0.057068 -0.012955 -0.104492  0.002665  ...  0.012947 -0.064270   \n",
      "\n",
      "             emb1016   emb1017   emb1018   emb1019   emb1020   emb1021  \\\n",
      "AZS17016.1 -0.048950 -0.054535  0.053986  0.058350 -0.017746  0.000415   \n",
      "AVO05213.1  0.028091 -0.086609  0.072632  0.039520 -0.009178  0.034973   \n",
      "QPG94344.1 -0.022400  0.017334  0.031250  0.055725  0.022354 -0.053772   \n",
      "APD47233.1 -0.006145  0.000610  0.007973  0.017776 -0.014000 -0.018326   \n",
      "ANU32067.1  0.010300 -0.068787  0.038727  0.052124 -0.036377  0.008217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "QMI07227.1 -0.010986 -0.069885  0.026779  0.051392  0.016006 -0.047211   \n",
      "UMA63307.1 -0.030167 -0.038300 -0.033752  0.017395 -0.015884 -0.026962   \n",
      "UPW01296.1 -0.020020 -0.026672  0.013168  0.062378 -0.035492 -0.030624   \n",
      "ULL17023.1  0.012344 -0.091064  0.013885  0.050232  0.000664  0.004559   \n",
      "USF23742.1 -0.004471 -0.082336  0.065735  0.049774 -0.016922 -0.023758   \n",
      "\n",
      "             emb1022   emb1023  \n",
      "AZS17016.1  0.031189  0.030411  \n",
      "AVO05213.1  0.057617  0.035248  \n",
      "QPG94344.1  0.024490  0.016556  \n",
      "APD47233.1  0.027618  0.011330  \n",
      "ANU32067.1  0.031891 -0.020660  \n",
      "...              ...       ...  \n",
      "QMI07227.1  0.022720  0.053162  \n",
      "UMA63307.1  0.026901 -0.016068  \n",
      "UPW01296.1  0.041962  0.034149  \n",
      "ULL17023.1  0.057648  0.028473  \n",
      "USF23742.1  0.039551  0.022232  \n",
      "\n",
      "[232736 rows x 1025 columns]\n"
     ]
    }
   ],
   "source": [
    "cazy_df = cazy_df_2.join(cazy_df_1, how='inner')\n",
    "print(cazy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_GH_df = cazy_df[cazy_df['class'].str.contains('GH')]\n",
    "cazy_GT_df = cazy_df[cazy_df['class'].str.contains('GT')]\n",
    "cazy_PL_df = cazy_df[cazy_df['class'].str.contains('PL')]\n",
    "cazy_CE_df = cazy_df[cazy_df['class'].str.contains('CE')]\n",
    "\n",
    "allclasses=['GH', 'GT', 'PL', 'CE']\n",
    "cazy_other_df = cazy_df[cazy_df['class'].str.contains('|'.join(allclasses))]\n",
    "\n",
    "#then use pd.DataFrame.sample to take random sample of items across axis 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[size of first four dataframes add up to 232736 so in fact no other classes to account for]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample proportionally to size of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=5000/232736\n",
    "new_cazy_GH_df = cazy_GH_df.sample(frac=x, axis=0)\n",
    "new_cazy_GT_df = cazy_GT_df.sample(frac=x, axis=0)\n",
    "new_cazy_PL_df = cazy_PL_df.sample(frac=x, axis=0)\n",
    "new_cazy_CE_df = cazy_CE_df.sample(frac=x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate into one DataFrame:\n",
    "\n",
    "Positive training data is stored in `cazy_train_df` (5001 embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_train_df = pd.concat([new_cazy_GH_df, new_cazy_GT_df, new_cazy_PL_df, new_cazy_CE_df], axis=0)\n",
    "cazy_train_df = cazy_train_df.join(pd.DataFrame(data=np.ones(np.array(cazy_ids).shape[0]), index=cazy_ids, columns=['cazy']), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1015</th>\n",
       "      <th>emb1016</th>\n",
       "      <th>emb1017</th>\n",
       "      <th>emb1018</th>\n",
       "      <th>emb1019</th>\n",
       "      <th>emb1020</th>\n",
       "      <th>emb1021</th>\n",
       "      <th>emb1022</th>\n",
       "      <th>emb1023</th>\n",
       "      <th>cazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>URZ12899.1</th>\n",
       "      <td>GH5_4</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.131958</td>\n",
       "      <td>0.015144</td>\n",
       "      <td>0.025970</td>\n",
       "      <td>0.034729</td>\n",
       "      <td>0.052582</td>\n",
       "      <td>0.035095</td>\n",
       "      <td>-0.100830</td>\n",
       "      <td>-0.015930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105225</td>\n",
       "      <td>0.049194</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>-0.070068</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANW95842.1</th>\n",
       "      <td>GH78</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>0.103821</td>\n",
       "      <td>0.016434</td>\n",
       "      <td>-0.005215</td>\n",
       "      <td>-0.013748</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>-0.082642</td>\n",
       "      <td>-0.099548</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>-0.020584</td>\n",
       "      <td>-0.054413</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>-0.038116</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBG02072.1</th>\n",
       "      <td>GH16</td>\n",
       "      <td>0.036682</td>\n",
       "      <td>-0.007381</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>-0.040100</td>\n",
       "      <td>-0.048981</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>-0.015442</td>\n",
       "      <td>-0.067688</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052582</td>\n",
       "      <td>-0.020340</td>\n",
       "      <td>-0.062561</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.058685</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>0.004879</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>-0.001222</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDT56626.1</th>\n",
       "      <td>GH33</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.051361</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.000928</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>-0.035492</td>\n",
       "      <td>-0.077515</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011826</td>\n",
       "      <td>-0.002647</td>\n",
       "      <td>-0.057281</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>-0.005970</td>\n",
       "      <td>-0.038452</td>\n",
       "      <td>-0.036194</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AQV98255.1</th>\n",
       "      <td>GH3</td>\n",
       "      <td>0.035065</td>\n",
       "      <td>0.041473</td>\n",
       "      <td>0.015457</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>-0.023636</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>-0.026901</td>\n",
       "      <td>-0.047974</td>\n",
       "      <td>-0.013748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022873</td>\n",
       "      <td>-0.016678</td>\n",
       "      <td>-0.041534</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QCK17107.1</th>\n",
       "      <td>CE4</td>\n",
       "      <td>0.053833</td>\n",
       "      <td>-0.023239</td>\n",
       "      <td>-0.029633</td>\n",
       "      <td>0.046173</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.054047</td>\n",
       "      <td>-0.021194</td>\n",
       "      <td>-0.079041</td>\n",
       "      <td>-0.008835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>-0.091736</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>-0.038300</td>\n",
       "      <td>0.073364</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABO67469.1</th>\n",
       "      <td>CE14</td>\n",
       "      <td>0.029648</td>\n",
       "      <td>0.018005</td>\n",
       "      <td>-0.004051</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>-0.029495</td>\n",
       "      <td>-0.084106</td>\n",
       "      <td>-0.040375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>-0.089478</td>\n",
       "      <td>-0.012840</td>\n",
       "      <td>-0.018600</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>-0.005470</td>\n",
       "      <td>0.072327</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UJO20230.1</th>\n",
       "      <td>CE8</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>-0.034363</td>\n",
       "      <td>0.048218</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.036407</td>\n",
       "      <td>0.008141</td>\n",
       "      <td>-0.092529</td>\n",
       "      <td>-0.092041</td>\n",
       "      <td>0.022995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018967</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.044708</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>0.053284</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>-0.011627</td>\n",
       "      <td>0.043884</td>\n",
       "      <td>0.016708</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UGU14409.1</th>\n",
       "      <td>CE8</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.051941</td>\n",
       "      <td>0.018463</td>\n",
       "      <td>-0.014381</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>0.017288</td>\n",
       "      <td>-0.040192</td>\n",
       "      <td>-0.111023</td>\n",
       "      <td>-0.009979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024078</td>\n",
       "      <td>0.025070</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>-0.005127</td>\n",
       "      <td>0.033173</td>\n",
       "      <td>-0.024582</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QOL55770.1</th>\n",
       "      <td>CE9</td>\n",
       "      <td>-0.069641</td>\n",
       "      <td>-0.062286</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>-0.008255</td>\n",
       "      <td>0.042786</td>\n",
       "      <td>0.068054</td>\n",
       "      <td>-0.042572</td>\n",
       "      <td>-0.073730</td>\n",
       "      <td>0.070984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099304</td>\n",
       "      <td>-0.058258</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>-0.115051</td>\n",
       "      <td>0.019394</td>\n",
       "      <td>0.091736</td>\n",
       "      <td>-0.040375</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2499 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            class      emb0      emb1      emb2      emb3      emb4      emb5  \\\n",
       "URZ12899.1  GH5_4  0.002209  0.131958  0.015144  0.025970  0.034729  0.052582   \n",
       "ANW95842.1   GH78  0.053955  0.103821  0.016434 -0.005215 -0.013748  0.010643   \n",
       "BBG02072.1   GH16  0.036682 -0.007381  0.001229 -0.040100 -0.048981  0.002577   \n",
       "QDT56626.1   GH33  0.046448  0.088867  0.051361  0.002258 -0.000928  0.038269   \n",
       "AQV98255.1    GH3  0.035065  0.041473  0.015457  0.061768 -0.023636  0.033966   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "QCK17107.1    CE4  0.053833 -0.023239 -0.029633  0.046173  0.012451  0.054047   \n",
       "ABO67469.1   CE14  0.029648  0.018005 -0.004051 -0.001152  0.023483  0.048340   \n",
       "UJO20230.1    CE8  0.017715 -0.034363  0.048218 -0.002123 -0.036407  0.008141   \n",
       "UGU14409.1    CE8  0.008400  0.051941  0.018463 -0.014381 -0.022781  0.017288   \n",
       "QOL55770.1    CE9 -0.069641 -0.062286  0.042908 -0.008255  0.042786  0.068054   \n",
       "\n",
       "                emb6      emb7      emb8  ...   emb1015   emb1016   emb1017  \\\n",
       "URZ12899.1  0.035095 -0.100830 -0.015930  ... -0.105225  0.049194 -0.070251   \n",
       "ANW95842.1 -0.082642 -0.099548  0.003157  ... -0.040314 -0.020584 -0.054413   \n",
       "BBG02072.1 -0.015442 -0.067688  0.015915  ... -0.052582 -0.020340 -0.062561   \n",
       "QDT56626.1 -0.035492 -0.077515  0.012627  ...  0.011826 -0.002647 -0.057281   \n",
       "AQV98255.1 -0.026901 -0.047974 -0.013748  ... -0.022873 -0.016678 -0.041534   \n",
       "...              ...       ...       ...  ...       ...       ...       ...   \n",
       "QCK17107.1 -0.021194 -0.079041 -0.008835  ... -0.000267  0.021286 -0.091736   \n",
       "ABO67469.1 -0.029495 -0.084106 -0.040375  ...  0.000961  0.034790 -0.089478   \n",
       "UJO20230.1 -0.092529 -0.092041  0.022995  ... -0.018967 -0.000084 -0.044708   \n",
       "UGU14409.1 -0.040192 -0.111023 -0.009979  ... -0.024078  0.025070 -0.070251   \n",
       "QOL55770.1 -0.042572 -0.073730  0.070984  ... -0.099304 -0.058258 -0.125000   \n",
       "\n",
       "             emb1018   emb1019   emb1020   emb1021   emb1022   emb1023  cazy  \n",
       "URZ12899.1  0.112000  0.040100 -0.070068  0.000009 -0.034180  0.014732   1.0  \n",
       "ANW95842.1  0.070190  0.003551 -0.038116  0.012283  0.042297  0.007759   1.0  \n",
       "BBG02072.1  0.020233  0.058685 -0.002800  0.004879  0.002249 -0.001222   1.0  \n",
       "QDT56626.1  0.007668 -0.005970 -0.038452 -0.036194  0.018967  0.004322   1.0  \n",
       "AQV98255.1  0.040009  0.008453  0.016174  0.003933  0.011948  0.015274   1.0  \n",
       "...              ...       ...       ...       ...       ...       ...   ...  \n",
       "QCK17107.1  0.059570  0.003613  0.002373 -0.038300  0.073364  0.056396   1.0  \n",
       "ABO67469.1 -0.012840 -0.018600  0.006897 -0.005470  0.072327  0.003300   1.0  \n",
       "UJO20230.1  0.068726  0.053284  0.013611 -0.011627  0.043884  0.016708   1.0  \n",
       "UGU14409.1  0.008911  0.047272  0.008064 -0.005127  0.033173 -0.024582   1.0  \n",
       "QOL55770.1 -0.048584 -0.115051  0.019394  0.091736 -0.040375 -0.035980   1.0  \n",
       "\n",
       "[2499 rows x 1026 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cazy_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create training dataframe + train a simple MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative training data is stored in `non_cazy_df`: (10,000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cazy_train_df = non_cazy_df.sample(n=10000, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Using PUL file entirely for inference; `train_df` for training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Concatenate non-cazy and cazy training dataframes into `train_df` (size = 10000 + 5001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([cazy_train_df, non_cazy_train_df], axis=0).iloc[:,1:] #removes 'class' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(train_df.iloc[:,:-1]), np.array(train_df.iloc[:,-1:])\n",
    "X_test, y_test = np.array(PUL_df.iloc[:,:-1]), np.array(PUL_df.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.ones(20,)*100\n",
    "vec = [int(x) for x in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.30476094\n",
      "Iteration 2, loss = 0.05380883\n",
      "Iteration 3, loss = 0.03216013\n",
      "Iteration 4, loss = 0.02354807\n",
      "Iteration 5, loss = 0.01953598\n",
      "Iteration 6, loss = 0.01714948\n",
      "Iteration 7, loss = 0.01030105\n",
      "Iteration 8, loss = 0.00811181\n",
      "Iteration 9, loss = 0.01133788\n",
      "Iteration 10, loss = 0.00801611\n",
      "Iteration 11, loss = 0.00408774\n",
      "Iteration 12, loss = 0.00456517\n",
      "Iteration 13, loss = 0.00497140\n",
      "Iteration 14, loss = 0.00345354\n",
      "Iteration 15, loss = 0.00346233\n",
      "Iteration 16, loss = 0.00315148\n",
      "Iteration 17, loss = 0.00779096\n",
      "Iteration 18, loss = 0.00051648\n",
      "Iteration 19, loss = 0.00095924\n",
      "Iteration 20, loss = 0.01043091\n",
      "Iteration 21, loss = 0.00324289\n",
      "Iteration 22, loss = 0.00197177\n",
      "Iteration 23, loss = 0.00154394\n",
      "Iteration 24, loss = 0.00051632\n",
      "Iteration 25, loss = 0.00033828\n",
      "Iteration 26, loss = 0.00033599\n",
      "Iteration 27, loss = 0.00033454\n",
      "Iteration 28, loss = 0.00033323\n",
      "Iteration 29, loss = 0.00033199\n",
      "Iteration 30, loss = 0.00033077\n",
      "Iteration 31, loss = 0.00032957\n",
      "Iteration 32, loss = 0.00032837\n",
      "Iteration 33, loss = 0.00032717\n",
      "Iteration 34, loss = 0.00032598\n",
      "Iteration 35, loss = 0.00032478\n",
      "Iteration 36, loss = 0.00032359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=vec,\n",
    "                     activation = 'relu',\n",
    "                     solver = 'adam',\n",
    "                     verbose = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8141317054162879\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             4635         1399\n",
      "Cazyme                   32         1633\n",
      "negative accuracy=  0.768147166058999\n",
      "positive accuracy=  0.9807807807807808\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "#confusion matrix\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)\n",
    "\n",
    "print('negative accuracy= ', cfmat_df.iloc[0,0]/sum(cfmat_df.iloc[0,:]))\n",
    "print('positive accuracy= ', cfmat_df.iloc[1,1]/sum(cfmat_df.iloc[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Using PUL file entirely for inference produces poor results, worse than trivial classifier (78.4%) (!)\n",
    "- positive prediction is much better (~99%) than negative (~64%) (why?)\n",
    "- Increasing number of layers generally gives worse accuracy; at a large (>50) number of layers, the whole test set is classfied as non-cazyme ie a trivial classifier\n",
    "\n",
    "<!-- **2 layers:** (relu)\n",
    "- 76.2% (10,10); 74.7% (100,100)\n",
    "\n",
    "**3 layers:**\n",
    "- 70.3% (10,10,10); 69.2% (100,100,100)\n",
    "\n",
    "**4 layers:**\n",
    " - 72.6% (10,10,10,10) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and AUC scores are displayed below, for MLPs of varying depth and fixed layer sizes (10 at each layer).\n",
    "\n",
    "- As number of layers increases past 3, the accuracy and AUC score decreases. AUC score is calculated below\n",
    "\n",
    "    (more neurons neeeded at each layer?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| no. layers | accuracy | AUC   |\n",
    "|------------|----------|-------|\n",
    "| 2          | 0.803    | 0.934 |\n",
    "| 3          | 0.807    | 0.924 |\n",
    "| 4          | 0.800    | 0.940 |\n",
    "| 5          | 0.801    | 0.929 |\n",
    "| 10         | 0.775    | 0.933 |\n",
    "| 20         | 0.766    | 0.859 |\n",
    "| 100        | 0.784    | 0.500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When size of each layer is increased to 100, the increase in accuracy and AUC score is greater for models with greater depth (as expected):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| no. layers | accuracy | AUC   |\n",
    "|------------|----------|-------|\n",
    "| 2          | 0.792    | 0.935 |\n",
    "| 3          | 0.817    | 0.939 |\n",
    "| 4          | 0.805    | 0.936 |\n",
    "| 5          | 0.831    | 0.937 |\n",
    "| 10         | 0.831    | 0.938 |\n",
    "| 20         | 0.814    | 0.939 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomizedSearchCV for hyperparameter tuning of a 3 layer MLP:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = []\n",
    "for i in range(2,10):\n",
    "    for j in range(2,10):\n",
    "        for k in range(2,10):\n",
    "            templist.append((i,j,k))\n",
    "\n",
    "distributions = {'hidden_layer_sizes':templist,\n",
    "              'activation':('logistic','relu')\n",
    "              }\n",
    "\n",
    "#layer norm\n",
    "# regularise weights\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(estimator=mlp, param_distributions=distributions, cv=5, verbose=1) #, refit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Best parameters: 'hidden_layer_sizes': (7, 2, 4), (3, 8, 7), 'activation': 'logistic' -->\n",
    "\n",
    "[Q: Large variance in best parameters (x,y,z) after running the whole notebook several times - but the randomness in selecting training data should not affect this?]\n",
    "\n",
    "No large increase in accuracy (~0.1%); still quite poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting this \"optimal\" 3-layer MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.02179484\n",
      "Iteration 2, loss = 0.89383048\n",
      "Iteration 3, loss = 0.78807026\n",
      "Iteration 4, loss = 0.71436478\n",
      "Iteration 5, loss = 0.66082358\n",
      "Iteration 6, loss = 0.62852736\n",
      "Iteration 7, loss = 0.60196286\n",
      "Iteration 8, loss = 0.57762651\n",
      "Iteration 9, loss = 0.55485516\n",
      "Iteration 10, loss = 0.53315048\n",
      "Iteration 11, loss = 0.51265888\n",
      "Iteration 12, loss = 0.49314245\n",
      "Iteration 13, loss = 0.47443681\n",
      "Iteration 14, loss = 0.45646560\n",
      "Iteration 15, loss = 0.43941713\n",
      "Iteration 16, loss = 0.42290946\n",
      "Iteration 17, loss = 0.40719877\n",
      "Iteration 18, loss = 0.39213501\n",
      "Iteration 19, loss = 0.37759052\n",
      "Iteration 20, loss = 0.36387261\n",
      "Iteration 21, loss = 0.35081424\n",
      "Iteration 22, loss = 0.33786861\n",
      "Iteration 23, loss = 0.32602138\n",
      "Iteration 24, loss = 0.31424889\n",
      "Iteration 25, loss = 0.30302390\n",
      "Iteration 26, loss = 0.29245290\n",
      "Iteration 27, loss = 0.28211310\n",
      "Iteration 28, loss = 0.27233989\n",
      "Iteration 29, loss = 0.26301739\n",
      "Iteration 30, loss = 0.25407921\n",
      "Iteration 31, loss = 0.24523764\n",
      "Iteration 32, loss = 0.23706568\n",
      "Iteration 33, loss = 0.22902496\n",
      "Iteration 34, loss = 0.22135302\n",
      "Iteration 35, loss = 0.21426021\n",
      "Iteration 36, loss = 0.20705733\n",
      "Iteration 37, loss = 0.20038339\n",
      "Iteration 38, loss = 0.19391174\n",
      "Iteration 39, loss = 0.18780168\n",
      "Iteration 40, loss = 0.18180941\n",
      "Iteration 41, loss = 0.17609426\n",
      "Iteration 42, loss = 0.17056425\n",
      "Iteration 43, loss = 0.16519489\n",
      "Iteration 44, loss = 0.16013327\n",
      "Iteration 45, loss = 0.15523556\n",
      "Iteration 46, loss = 0.15048448\n",
      "Iteration 47, loss = 0.14609661\n",
      "Iteration 48, loss = 0.14167640\n",
      "Iteration 49, loss = 0.13759176\n",
      "Iteration 50, loss = 0.13347466\n",
      "Iteration 51, loss = 0.12964660\n",
      "Iteration 52, loss = 0.12596644\n",
      "Iteration 53, loss = 0.12240535\n",
      "Iteration 54, loss = 0.11893029\n",
      "Iteration 55, loss = 0.11569592\n",
      "Iteration 56, loss = 0.11248001\n",
      "Iteration 57, loss = 0.10944959\n",
      "Iteration 58, loss = 0.10649751\n",
      "Iteration 59, loss = 0.10369977\n",
      "Iteration 60, loss = 0.10096891\n",
      "Iteration 61, loss = 0.09832083\n",
      "Iteration 62, loss = 0.09581502\n",
      "Iteration 63, loss = 0.09336768\n",
      "Iteration 64, loss = 0.09116777\n",
      "Iteration 65, loss = 0.08876683\n",
      "Iteration 66, loss = 0.08647480\n",
      "Iteration 67, loss = 0.08431454\n",
      "Iteration 68, loss = 0.08204233\n",
      "Iteration 69, loss = 0.08003737\n",
      "Iteration 70, loss = 0.07804186\n",
      "Iteration 71, loss = 0.07624136\n",
      "Iteration 72, loss = 0.07433952\n",
      "Iteration 73, loss = 0.07253104\n",
      "Iteration 74, loss = 0.07088059\n",
      "Iteration 75, loss = 0.06922013\n",
      "Iteration 76, loss = 0.06767452\n",
      "Iteration 77, loss = 0.06619337\n",
      "Iteration 78, loss = 0.06472367\n",
      "Iteration 79, loss = 0.06334472\n",
      "Iteration 80, loss = 0.06198032\n",
      "Iteration 81, loss = 0.06066694\n",
      "Iteration 82, loss = 0.05941234\n",
      "Iteration 83, loss = 0.05819605\n",
      "Iteration 84, loss = 0.05699839\n",
      "Iteration 85, loss = 0.05585601\n",
      "Iteration 86, loss = 0.05475335\n",
      "Iteration 87, loss = 0.05368310\n",
      "Iteration 88, loss = 0.05264864\n",
      "Iteration 89, loss = 0.05164924\n",
      "Iteration 90, loss = 0.05068294\n",
      "Iteration 91, loss = 0.04974994\n",
      "Iteration 92, loss = 0.04884706\n",
      "Iteration 93, loss = 0.04796908\n",
      "Iteration 94, loss = 0.04712432\n",
      "Iteration 95, loss = 0.04630462\n",
      "Iteration 96, loss = 0.04550838\n",
      "Iteration 97, loss = 0.04474254\n",
      "Iteration 98, loss = 0.04399435\n",
      "Iteration 99, loss = 0.04327554\n",
      "Iteration 100, loss = 0.04258122\n",
      "Iteration 101, loss = 0.04191180\n",
      "Iteration 102, loss = 0.04125396\n",
      "Iteration 103, loss = 0.04062663\n",
      "Iteration 104, loss = 0.04001504\n",
      "Iteration 105, loss = 0.03942607\n",
      "Iteration 106, loss = 0.03885326\n",
      "Iteration 107, loss = 0.03830343\n",
      "Iteration 108, loss = 0.03776644\n",
      "Iteration 109, loss = 0.03724741\n",
      "Iteration 110, loss = 0.03674463\n",
      "Iteration 111, loss = 0.03625999\n",
      "Iteration 112, loss = 0.03579586\n",
      "Iteration 113, loss = 0.03534153\n",
      "Iteration 114, loss = 0.03489610\n",
      "Iteration 115, loss = 0.03447021\n",
      "Iteration 116, loss = 0.03406671\n",
      "Iteration 117, loss = 0.03366657\n",
      "Iteration 118, loss = 0.03327977\n",
      "Iteration 119, loss = 0.03290576\n",
      "Iteration 120, loss = 0.03254979\n",
      "Iteration 121, loss = 0.03220350\n",
      "Iteration 122, loss = 0.03187134\n",
      "Iteration 123, loss = 0.03154110\n",
      "Iteration 124, loss = 0.03122885\n",
      "Iteration 125, loss = 0.03092705\n",
      "Iteration 126, loss = 0.03063203\n",
      "Iteration 127, loss = 0.03034686\n",
      "Iteration 128, loss = 0.03007528\n",
      "Iteration 129, loss = 0.02980944\n",
      "Iteration 130, loss = 0.02955697\n",
      "Iteration 131, loss = 0.02930866\n",
      "Iteration 132, loss = 0.02907019\n",
      "Iteration 133, loss = 0.02884423\n",
      "Iteration 134, loss = 0.02862205\n",
      "Iteration 135, loss = 0.02840634\n",
      "Iteration 136, loss = 0.02820195\n",
      "Iteration 137, loss = 0.02800504\n",
      "Iteration 138, loss = 0.02781474\n",
      "Iteration 139, loss = 0.02762925\n",
      "Iteration 140, loss = 0.02745242\n",
      "Iteration 141, loss = 0.02728165\n",
      "Iteration 142, loss = 0.02711656\n",
      "Iteration 143, loss = 0.02695930\n",
      "Iteration 144, loss = 0.02680563\n",
      "Iteration 145, loss = 0.02666049\n",
      "Iteration 146, loss = 0.02651702\n",
      "Iteration 147, loss = 0.02638252\n",
      "Iteration 148, loss = 0.02625393\n",
      "Iteration 149, loss = 0.02612970\n",
      "Iteration 150, loss = 0.02600843\n",
      "Iteration 151, loss = 0.02589654\n",
      "Iteration 152, loss = 0.02578065\n",
      "Iteration 153, loss = 0.02567511\n",
      "Iteration 154, loss = 0.02557021\n",
      "Iteration 155, loss = 0.02547227\n",
      "Iteration 156, loss = 0.02537978\n",
      "Iteration 157, loss = 0.02529173\n",
      "Iteration 158, loss = 0.02520485\n",
      "Iteration 159, loss = 0.02512434\n",
      "Iteration 160, loss = 0.02504482\n",
      "Iteration 161, loss = 0.02496834\n",
      "Iteration 162, loss = 0.02489594\n",
      "Iteration 163, loss = 0.02482695\n",
      "Iteration 164, loss = 0.02476284\n",
      "Iteration 165, loss = 0.02469892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71434244\n",
      "Iteration 2, loss = 0.38703201\n",
      "Iteration 3, loss = 0.14851434\n",
      "Iteration 4, loss = 0.09362885\n",
      "Iteration 5, loss = 0.07114289\n",
      "Iteration 6, loss = 0.05876805\n",
      "Iteration 7, loss = 0.05118705\n",
      "Iteration 8, loss = 0.04579027\n",
      "Iteration 9, loss = 0.04195397\n",
      "Iteration 10, loss = 0.03866569\n",
      "Iteration 11, loss = 0.03632632\n",
      "Iteration 12, loss = 0.03398956\n",
      "Iteration 13, loss = 0.03196898\n",
      "Iteration 14, loss = 0.03032810\n",
      "Iteration 15, loss = 0.02851133\n",
      "Iteration 16, loss = 0.02694771\n",
      "Iteration 17, loss = 0.03186748\n",
      "Iteration 18, loss = 0.02627731\n",
      "Iteration 19, loss = 0.02506518\n",
      "Iteration 20, loss = 0.02555956\n",
      "Iteration 21, loss = 0.02367689\n",
      "Iteration 22, loss = 0.02279113\n",
      "Iteration 23, loss = 0.02201356\n",
      "Iteration 24, loss = 0.02125659\n",
      "Iteration 25, loss = 0.02073130\n",
      "Iteration 26, loss = 0.02005747\n",
      "Iteration 27, loss = 0.01927687\n",
      "Iteration 28, loss = 0.01871769\n",
      "Iteration 29, loss = 0.01816328\n",
      "Iteration 30, loss = 0.02445155\n",
      "Iteration 31, loss = 0.01846954\n",
      "Iteration 32, loss = 0.01750006\n",
      "Iteration 33, loss = 0.01676680\n",
      "Iteration 34, loss = 0.01614508\n",
      "Iteration 35, loss = 0.01559946\n",
      "Iteration 36, loss = 0.01522850\n",
      "Iteration 37, loss = 0.01473021\n",
      "Iteration 38, loss = 0.01441399\n",
      "Iteration 39, loss = 0.01381417\n",
      "Iteration 40, loss = 0.01385639\n",
      "Iteration 41, loss = 0.01304998\n",
      "Iteration 42, loss = 0.01579071\n",
      "Iteration 43, loss = 0.01312749\n",
      "Iteration 44, loss = 0.01226684\n",
      "Iteration 45, loss = 0.01193284\n",
      "Iteration 46, loss = 0.01134747\n",
      "Iteration 47, loss = 0.01110153\n",
      "Iteration 48, loss = 0.01068672\n",
      "Iteration 49, loss = 0.01043187\n",
      "Iteration 50, loss = 0.01090432\n",
      "Iteration 51, loss = 0.01014539\n",
      "Iteration 52, loss = 0.00955195\n",
      "Iteration 53, loss = 0.00926790\n",
      "Iteration 54, loss = 0.00882129\n",
      "Iteration 55, loss = 0.00852238\n",
      "Iteration 56, loss = 0.00851879\n",
      "Iteration 57, loss = 0.00810924\n",
      "Iteration 58, loss = 0.00776665\n",
      "Iteration 59, loss = 0.00757446\n",
      "Iteration 60, loss = 0.00822159\n",
      "Iteration 61, loss = 0.00716403\n",
      "Iteration 62, loss = 0.00705059\n",
      "Iteration 63, loss = 0.00647209\n",
      "Iteration 64, loss = 0.00629214\n",
      "Iteration 65, loss = 0.00607825\n",
      "Iteration 66, loss = 0.00580395\n",
      "Iteration 67, loss = 0.00558806\n",
      "Iteration 68, loss = 0.00540998\n",
      "Iteration 69, loss = 0.00513632\n",
      "Iteration 70, loss = 0.00481276\n",
      "Iteration 71, loss = 0.00472597\n",
      "Iteration 72, loss = 0.00445362\n",
      "Iteration 73, loss = 0.00435031\n",
      "Iteration 74, loss = 0.00411196\n",
      "Iteration 75, loss = 0.00397091\n",
      "Iteration 76, loss = 0.00383430\n",
      "Iteration 77, loss = 0.00369435\n",
      "Iteration 78, loss = 0.00365853\n",
      "Iteration 79, loss = 0.00329752\n",
      "Iteration 80, loss = 0.00318517\n",
      "Iteration 81, loss = 0.00308769\n",
      "Iteration 82, loss = 0.00286717\n",
      "Iteration 83, loss = 0.00271063\n",
      "Iteration 84, loss = 0.00265809\n",
      "Iteration 85, loss = 0.00251570\n",
      "Iteration 86, loss = 0.00235815\n",
      "Iteration 87, loss = 0.00221048\n",
      "Iteration 88, loss = 0.00213990\n",
      "Iteration 89, loss = 0.00196759\n",
      "Iteration 90, loss = 0.00184021\n",
      "Iteration 91, loss = 0.00164769\n",
      "Iteration 92, loss = 0.00152744\n",
      "Iteration 93, loss = 0.00146009\n",
      "Iteration 94, loss = 0.00138330\n",
      "Iteration 95, loss = 0.00126941\n",
      "Iteration 96, loss = 0.00120675\n",
      "Iteration 97, loss = 0.00116596\n",
      "Iteration 98, loss = 0.00505857\n",
      "Iteration 99, loss = 0.00178025\n",
      "Iteration 100, loss = 0.00129130\n",
      "Iteration 101, loss = 0.00109514\n",
      "Iteration 102, loss = 0.00096019\n",
      "Iteration 103, loss = 0.00088016\n",
      "Iteration 104, loss = 0.00077611\n",
      "Iteration 105, loss = 0.00075343\n",
      "Iteration 106, loss = 0.00071789\n",
      "Iteration 107, loss = 0.00063144\n",
      "Iteration 108, loss = 0.00061215\n",
      "Iteration 109, loss = 0.00055760\n",
      "Iteration 110, loss = 0.00051582\n",
      "Iteration 111, loss = 0.00046506\n",
      "Iteration 112, loss = 0.00042416\n",
      "Iteration 113, loss = 0.00037979\n",
      "Iteration 114, loss = 0.00035759\n",
      "Iteration 115, loss = 0.00033399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61230969\n",
      "Iteration 2, loss = 0.51795896\n",
      "Iteration 3, loss = 0.38987906\n",
      "Iteration 4, loss = 0.26426119\n",
      "Iteration 5, loss = 0.15042051\n",
      "Iteration 6, loss = 0.08559968\n",
      "Iteration 7, loss = 0.06121402\n",
      "Iteration 8, loss = 0.05096206\n",
      "Iteration 9, loss = 0.04549556\n",
      "Iteration 10, loss = 0.04164085\n",
      "Iteration 11, loss = 0.03893521\n",
      "Iteration 12, loss = 0.03696168\n",
      "Iteration 13, loss = 0.03489585\n",
      "Iteration 14, loss = 0.03312158\n",
      "Iteration 15, loss = 0.03161205\n",
      "Iteration 16, loss = 0.03001615\n",
      "Iteration 17, loss = 0.02873825\n",
      "Iteration 18, loss = 0.02711485\n",
      "Iteration 19, loss = 0.02550387\n",
      "Iteration 20, loss = 0.02410807\n",
      "Iteration 21, loss = 0.02328955\n",
      "Iteration 22, loss = 0.02229295\n",
      "Iteration 23, loss = 0.02169789\n",
      "Iteration 24, loss = 0.02045203\n",
      "Iteration 25, loss = 0.02046342\n",
      "Iteration 26, loss = 0.01942049\n",
      "Iteration 27, loss = 0.01877503\n",
      "Iteration 28, loss = 0.01807651\n",
      "Iteration 29, loss = 0.01722760\n",
      "Iteration 30, loss = 0.01687207\n",
      "Iteration 31, loss = 0.01629147\n",
      "Iteration 32, loss = 0.01591304\n",
      "Iteration 33, loss = 0.01516830\n",
      "Iteration 34, loss = 0.01547914\n",
      "Iteration 35, loss = 0.01433900\n",
      "Iteration 36, loss = 0.01386631\n",
      "Iteration 37, loss = 0.01343733\n",
      "Iteration 38, loss = 0.01332409\n",
      "Iteration 39, loss = 0.01252751\n",
      "Iteration 40, loss = 0.01249688\n",
      "Iteration 41, loss = 0.01198242\n",
      "Iteration 42, loss = 0.01156047\n",
      "Iteration 43, loss = 0.01113356\n",
      "Iteration 44, loss = 0.01082835\n",
      "Iteration 45, loss = 0.01042994\n",
      "Iteration 46, loss = 0.01011316\n",
      "Iteration 47, loss = 0.00960575\n",
      "Iteration 48, loss = 0.00925512\n",
      "Iteration 49, loss = 0.00909640\n",
      "Iteration 50, loss = 0.00875463\n",
      "Iteration 51, loss = 0.00840488\n",
      "Iteration 52, loss = 0.00855074\n",
      "Iteration 53, loss = 0.00813303\n",
      "Iteration 54, loss = 0.00784215\n",
      "Iteration 55, loss = 0.00750546\n",
      "Iteration 56, loss = 0.00727947\n",
      "Iteration 57, loss = 0.00713094\n",
      "Iteration 58, loss = 0.00666944\n",
      "Iteration 59, loss = 0.00652940\n",
      "Iteration 60, loss = 0.00652024\n",
      "Iteration 61, loss = 0.00675723\n",
      "Iteration 62, loss = 0.00659337\n",
      "Iteration 63, loss = 0.00586749\n",
      "Iteration 64, loss = 0.00569050\n",
      "Iteration 65, loss = 0.00533302\n",
      "Iteration 66, loss = 0.00491713\n",
      "Iteration 67, loss = 0.00478210\n",
      "Iteration 68, loss = 0.00465510\n",
      "Iteration 69, loss = 0.00454258\n",
      "Iteration 70, loss = 0.00447358\n",
      "Iteration 71, loss = 0.00430060\n",
      "Iteration 72, loss = 0.00423308\n",
      "Iteration 73, loss = 0.00420819\n",
      "Iteration 74, loss = 0.00406358\n",
      "Iteration 75, loss = 0.00393411\n",
      "Iteration 76, loss = 0.00384674\n",
      "Iteration 77, loss = 0.00402380\n",
      "Iteration 78, loss = 0.00475744\n",
      "Iteration 79, loss = 0.00384680\n",
      "Iteration 80, loss = 0.00318728\n",
      "Iteration 81, loss = 0.00292748\n",
      "Iteration 82, loss = 0.00281638\n",
      "Iteration 83, loss = 0.00256150\n",
      "Iteration 84, loss = 0.00225681\n",
      "Iteration 85, loss = 0.00200742\n",
      "Iteration 86, loss = 0.00179828\n",
      "Iteration 87, loss = 0.00169161\n",
      "Iteration 88, loss = 0.00163867\n",
      "Iteration 89, loss = 0.00144555\n",
      "Iteration 90, loss = 0.00140242\n",
      "Iteration 91, loss = 0.00131509\n",
      "Iteration 92, loss = 0.00128043\n",
      "Iteration 93, loss = 0.00120902\n",
      "Iteration 94, loss = 0.00119176\n",
      "Iteration 95, loss = 0.00113452\n",
      "Iteration 96, loss = 0.00109243\n",
      "Iteration 97, loss = 0.00105050\n",
      "Iteration 98, loss = 0.00101618\n",
      "Iteration 99, loss = 0.00099714\n",
      "Iteration 100, loss = 0.00086208\n",
      "Iteration 101, loss = 0.00071948\n",
      "Iteration 102, loss = 0.00066786\n",
      "Iteration 103, loss = 0.00063731\n",
      "Iteration 104, loss = 0.00061598\n",
      "Iteration 105, loss = 0.00058902\n",
      "Iteration 106, loss = 0.00056175\n",
      "Iteration 107, loss = 0.00055715\n",
      "Iteration 108, loss = 0.00050989\n",
      "Iteration 109, loss = 0.00047503\n",
      "Iteration 110, loss = 0.00044296\n",
      "Iteration 111, loss = 0.00035960\n",
      "Iteration 112, loss = 0.00026054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07471275\n",
      "Iteration 2, loss = 0.98097114\n",
      "Iteration 3, loss = 0.88583436\n",
      "Iteration 4, loss = 0.80936568\n",
      "Iteration 5, loss = 0.75395548\n",
      "Iteration 6, loss = 0.71903415\n",
      "Iteration 7, loss = 0.68863257\n",
      "Iteration 8, loss = 0.66020553\n",
      "Iteration 9, loss = 0.63356692\n",
      "Iteration 10, loss = 0.60853656\n",
      "Iteration 11, loss = 0.58478023\n",
      "Iteration 12, loss = 0.56207064\n",
      "Iteration 13, loss = 0.54054549\n",
      "Iteration 14, loss = 0.52007329\n",
      "Iteration 15, loss = 0.50019668\n",
      "Iteration 16, loss = 0.48098019\n",
      "Iteration 17, loss = 0.46281625\n",
      "Iteration 18, loss = 0.44524048\n",
      "Iteration 19, loss = 0.42865512\n",
      "Iteration 20, loss = 0.41266152\n",
      "Iteration 21, loss = 0.39736933\n",
      "Iteration 22, loss = 0.38295993\n",
      "Iteration 23, loss = 0.36920732\n",
      "Iteration 24, loss = 0.35555681\n",
      "Iteration 25, loss = 0.34295783\n",
      "Iteration 26, loss = 0.33076793\n",
      "Iteration 27, loss = 0.31887883\n",
      "Iteration 28, loss = 0.30771371\n",
      "Iteration 29, loss = 0.29654851\n",
      "Iteration 30, loss = 0.28608365\n",
      "Iteration 31, loss = 0.27656159\n",
      "Iteration 32, loss = 0.26691876\n",
      "Iteration 33, loss = 0.25754263\n",
      "Iteration 34, loss = 0.24858348\n",
      "Iteration 35, loss = 0.23998010\n",
      "Iteration 36, loss = 0.23183371\n",
      "Iteration 37, loss = 0.22387243\n",
      "Iteration 38, loss = 0.21642939\n",
      "Iteration 39, loss = 0.20927977\n",
      "Iteration 40, loss = 0.20232974\n",
      "Iteration 41, loss = 0.19573991\n",
      "Iteration 42, loss = 0.18944556\n",
      "Iteration 43, loss = 0.18323431\n",
      "Iteration 44, loss = 0.17741211\n",
      "Iteration 45, loss = 0.17178657\n",
      "Iteration 46, loss = 0.16648659\n",
      "Iteration 47, loss = 0.16133328\n",
      "Iteration 48, loss = 0.16074031\n",
      "Iteration 49, loss = 0.15245361\n",
      "Iteration 50, loss = 0.14735962\n",
      "Iteration 51, loss = 0.14267421\n",
      "Iteration 52, loss = 0.13838597\n",
      "Iteration 53, loss = 0.13419562\n",
      "Iteration 54, loss = 0.13023999\n",
      "Iteration 55, loss = 0.12639347\n",
      "Iteration 56, loss = 0.12280119\n",
      "Iteration 57, loss = 0.11934237\n",
      "Iteration 58, loss = 0.11592642\n",
      "Iteration 59, loss = 0.11264540\n",
      "Iteration 60, loss = 0.10946456\n",
      "Iteration 61, loss = 0.10650547\n",
      "Iteration 62, loss = 0.10356555\n",
      "Iteration 63, loss = 0.10074969\n",
      "Iteration 64, loss = 0.09803084\n",
      "Iteration 65, loss = 0.09541107\n",
      "Iteration 66, loss = 0.09294278\n",
      "Iteration 67, loss = 0.09055883\n",
      "Iteration 68, loss = 0.08824817\n",
      "Iteration 69, loss = 0.08597357\n",
      "Iteration 70, loss = 0.08378026\n",
      "Iteration 71, loss = 0.08170265\n",
      "Iteration 72, loss = 0.07971579\n",
      "Iteration 73, loss = 0.07773794\n",
      "Iteration 74, loss = 0.07582277\n",
      "Iteration 75, loss = 0.07400209\n",
      "Iteration 76, loss = 0.07223182\n",
      "Iteration 77, loss = 0.07050536\n",
      "Iteration 78, loss = 0.06884668\n",
      "Iteration 79, loss = 0.06723335\n",
      "Iteration 80, loss = 0.06571273\n",
      "Iteration 81, loss = 0.06462037\n",
      "Iteration 82, loss = 0.06329285\n",
      "Iteration 83, loss = 0.06191431\n",
      "Iteration 84, loss = 0.06057307\n",
      "Iteration 85, loss = 0.05931030\n",
      "Iteration 86, loss = 0.05807480\n",
      "Iteration 87, loss = 0.05685303\n",
      "Iteration 88, loss = 0.05569825\n",
      "Iteration 89, loss = 0.05457768\n",
      "Iteration 90, loss = 0.05349300\n",
      "Iteration 91, loss = 0.05241746\n",
      "Iteration 92, loss = 0.05136954\n",
      "Iteration 93, loss = 0.05037995\n",
      "Iteration 94, loss = 0.04940245\n",
      "Iteration 95, loss = 0.04847341\n",
      "Iteration 96, loss = 0.04755605\n",
      "Iteration 97, loss = 0.04666113\n",
      "Iteration 98, loss = 0.04581568\n",
      "Iteration 99, loss = 0.04497844\n",
      "Iteration 100, loss = 0.04418631\n",
      "Iteration 101, loss = 0.04341939\n",
      "Iteration 102, loss = 0.04266284\n",
      "Iteration 103, loss = 0.04193802\n",
      "Iteration 104, loss = 0.04122085\n",
      "Iteration 105, loss = 0.04052604\n",
      "Iteration 106, loss = 0.03985636\n",
      "Iteration 107, loss = 0.03922127\n",
      "Iteration 108, loss = 0.03859015\n",
      "Iteration 109, loss = 0.03797914\n",
      "Iteration 110, loss = 0.03738729\n",
      "Iteration 111, loss = 0.03681600\n",
      "Iteration 112, loss = 0.03626489\n",
      "Iteration 113, loss = 0.03573948\n",
      "Iteration 114, loss = 0.03521804\n",
      "Iteration 115, loss = 0.03471845\n",
      "Iteration 116, loss = 0.03422876\n",
      "Iteration 117, loss = 0.03376845\n",
      "Iteration 118, loss = 0.03332656\n",
      "Iteration 119, loss = 0.03288529\n",
      "Iteration 120, loss = 0.03245648\n",
      "Iteration 121, loss = 0.03204642\n",
      "Iteration 122, loss = 0.03164353\n",
      "Iteration 123, loss = 0.03125695\n",
      "Iteration 124, loss = 0.03089111\n",
      "Iteration 125, loss = 0.03052973\n",
      "Iteration 126, loss = 0.03017634\n",
      "Iteration 127, loss = 0.02984394\n",
      "Iteration 128, loss = 0.02952572\n",
      "Iteration 129, loss = 0.02921032\n",
      "Iteration 130, loss = 0.02889803\n",
      "Iteration 131, loss = 0.02860168\n",
      "Iteration 132, loss = 0.02832106\n",
      "Iteration 133, loss = 0.02805080\n",
      "Iteration 134, loss = 0.02778707\n",
      "Iteration 135, loss = 0.02752639\n",
      "Iteration 136, loss = 0.02727197\n",
      "Iteration 137, loss = 0.02703658\n",
      "Iteration 138, loss = 0.02681092\n",
      "Iteration 139, loss = 0.02658656\n",
      "Iteration 140, loss = 0.02637207\n",
      "Iteration 141, loss = 0.02616735\n",
      "Iteration 142, loss = 0.02596289\n",
      "Iteration 143, loss = 0.02576570\n",
      "Iteration 144, loss = 0.02557754\n",
      "Iteration 145, loss = 0.02540172\n",
      "Iteration 146, loss = 0.02522440\n",
      "Iteration 147, loss = 0.02505304\n",
      "Iteration 148, loss = 0.02489162\n",
      "Iteration 149, loss = 0.02473234\n",
      "Iteration 150, loss = 0.02458356\n",
      "Iteration 151, loss = 0.02443665\n",
      "Iteration 152, loss = 0.02429831\n",
      "Iteration 153, loss = 0.02416237\n",
      "Iteration 154, loss = 0.02471711\n",
      "Iteration 155, loss = 0.02291079\n",
      "Iteration 156, loss = 0.02172262\n",
      "Iteration 157, loss = 0.02124837\n",
      "Iteration 158, loss = 0.02104852\n",
      "Iteration 159, loss = 0.02090062\n",
      "Iteration 160, loss = 0.02076846\n",
      "Iteration 161, loss = 0.02064894\n",
      "Iteration 162, loss = 0.02053682\n",
      "Iteration 163, loss = 0.02042654\n",
      "Iteration 164, loss = 0.02032523\n",
      "Iteration 165, loss = 0.02022391\n",
      "Iteration 166, loss = 0.02012853\n",
      "Iteration 167, loss = 0.02003589\n",
      "Iteration 168, loss = 0.01994813\n",
      "Iteration 169, loss = 0.01986457\n",
      "Iteration 170, loss = 0.01978594\n",
      "Iteration 171, loss = 0.01971098\n",
      "Iteration 172, loss = 0.01964587\n",
      "Iteration 173, loss = 0.01957192\n",
      "Iteration 174, loss = 0.01950351\n",
      "Iteration 175, loss = 0.01943904\n",
      "Iteration 176, loss = 0.01937819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48439781\n",
      "Iteration 2, loss = 0.30649481\n",
      "Iteration 3, loss = 0.23805189\n",
      "Iteration 4, loss = 0.19647455\n",
      "Iteration 5, loss = 0.16728129\n",
      "Iteration 6, loss = 0.14449204\n",
      "Iteration 7, loss = 0.12570064\n",
      "Iteration 8, loss = 0.11158040\n",
      "Iteration 9, loss = 0.09601940\n",
      "Iteration 10, loss = 0.08278025\n",
      "Iteration 11, loss = 0.07041324\n",
      "Iteration 12, loss = 0.05851227\n",
      "Iteration 13, loss = 0.04681001\n",
      "Iteration 14, loss = 0.03737118\n",
      "Iteration 15, loss = 0.03047386\n",
      "Iteration 16, loss = 0.02638223\n",
      "Iteration 17, loss = 0.02344722\n",
      "Iteration 18, loss = 0.02154857\n",
      "Iteration 19, loss = 0.01975648\n",
      "Iteration 20, loss = 0.01840426\n",
      "Iteration 21, loss = 0.01708207\n",
      "Iteration 22, loss = 0.01588794\n",
      "Iteration 23, loss = 0.01490489\n",
      "Iteration 24, loss = 0.01391833\n",
      "Iteration 25, loss = 0.01310812\n",
      "Iteration 26, loss = 0.01201162\n",
      "Iteration 27, loss = 0.01117470\n",
      "Iteration 28, loss = 0.01039509\n",
      "Iteration 29, loss = 0.00979446\n",
      "Iteration 30, loss = 0.00900707\n",
      "Iteration 31, loss = 0.00811756\n",
      "Iteration 32, loss = 0.00757723\n",
      "Iteration 33, loss = 0.00700615\n",
      "Iteration 34, loss = 0.00658776\n",
      "Iteration 35, loss = 0.00629541\n",
      "Iteration 36, loss = 0.00568172\n",
      "Iteration 37, loss = 0.00868797\n",
      "Iteration 38, loss = 0.00545754\n",
      "Iteration 39, loss = 0.00475003\n",
      "Iteration 40, loss = 0.00433139\n",
      "Iteration 41, loss = 0.00396282\n",
      "Iteration 42, loss = 0.00394840\n",
      "Iteration 43, loss = 0.00350173\n",
      "Iteration 44, loss = 0.00325772\n",
      "Iteration 45, loss = 0.00297344\n",
      "Iteration 46, loss = 0.00283842\n",
      "Iteration 47, loss = 0.00265124\n",
      "Iteration 48, loss = 0.00251093\n",
      "Iteration 49, loss = 0.00224969\n",
      "Iteration 50, loss = 0.00208055\n",
      "Iteration 51, loss = 0.00192290\n",
      "Iteration 52, loss = 0.00184960\n",
      "Iteration 53, loss = 0.00170470\n",
      "Iteration 54, loss = 0.00161087\n",
      "Iteration 55, loss = 0.00145243\n",
      "Iteration 56, loss = 0.00136321\n",
      "Iteration 57, loss = 0.00126119\n",
      "Iteration 58, loss = 0.00122562\n",
      "Iteration 59, loss = 0.00110204\n",
      "Iteration 60, loss = 0.00105356\n",
      "Iteration 61, loss = 0.00096786\n",
      "Iteration 62, loss = 0.00092667\n",
      "Iteration 63, loss = 0.00093633\n",
      "Iteration 64, loss = 0.00081963\n",
      "Iteration 65, loss = 0.00079567\n",
      "Iteration 66, loss = 0.00075038\n",
      "Iteration 67, loss = 0.00070639\n",
      "Iteration 68, loss = 0.00066429\n",
      "Iteration 69, loss = 0.00062313\n",
      "Iteration 70, loss = 0.00057614\n",
      "Iteration 71, loss = 0.00055432\n",
      "Iteration 72, loss = 0.00052380\n",
      "Iteration 73, loss = 0.00050609\n",
      "Iteration 74, loss = 0.00047524\n",
      "Iteration 75, loss = 0.00046577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.88729822\n",
      "Iteration 2, loss = 0.71779314\n",
      "Iteration 3, loss = 0.57669266\n",
      "Iteration 4, loss = 0.46595950\n",
      "Iteration 5, loss = 0.41167626\n",
      "Iteration 6, loss = 0.37260621\n",
      "Iteration 7, loss = 0.33951397\n",
      "Iteration 8, loss = 0.31090604\n",
      "Iteration 9, loss = 0.28613672\n",
      "Iteration 10, loss = 0.26403163\n",
      "Iteration 11, loss = 0.24445357\n",
      "Iteration 12, loss = 0.22672762\n",
      "Iteration 13, loss = 0.21126193\n",
      "Iteration 14, loss = 0.19926274\n",
      "Iteration 15, loss = 0.18790607\n",
      "Iteration 16, loss = 0.17808382\n",
      "Iteration 17, loss = 0.17180589\n",
      "Iteration 18, loss = 0.16606117\n",
      "Iteration 19, loss = 0.16044555\n",
      "Iteration 20, loss = 0.15512242\n",
      "Iteration 21, loss = 0.14994352\n",
      "Iteration 22, loss = 0.14484790\n",
      "Iteration 23, loss = 0.14003464\n",
      "Iteration 24, loss = 0.13543095\n",
      "Iteration 25, loss = 0.13090760\n",
      "Iteration 26, loss = 0.12663854\n",
      "Iteration 27, loss = 0.12245763\n",
      "Iteration 28, loss = 0.11859683\n",
      "Iteration 29, loss = 0.11449308\n",
      "Iteration 30, loss = 0.11079011\n",
      "Iteration 31, loss = 0.10722573\n",
      "Iteration 32, loss = 0.10375516\n",
      "Iteration 33, loss = 0.10049369\n",
      "Iteration 34, loss = 0.09714940\n",
      "Iteration 35, loss = 0.09400307\n",
      "Iteration 36, loss = 0.09102666\n",
      "Iteration 37, loss = 0.08812132\n",
      "Iteration 38, loss = 0.08530670\n",
      "Iteration 39, loss = 0.08269846\n",
      "Iteration 40, loss = 0.08025060\n",
      "Iteration 41, loss = 0.07759840\n",
      "Iteration 42, loss = 0.07544506\n",
      "Iteration 43, loss = 0.07283849\n",
      "Iteration 44, loss = 0.07060846\n",
      "Iteration 45, loss = 0.06845734\n",
      "Iteration 46, loss = 0.06631845\n",
      "Iteration 47, loss = 0.06440915\n",
      "Iteration 48, loss = 0.06237090\n",
      "Iteration 49, loss = 0.06065199\n",
      "Iteration 50, loss = 0.05872463\n",
      "Iteration 51, loss = 0.05704190\n",
      "Iteration 52, loss = 0.05527064\n",
      "Iteration 53, loss = 0.05371874\n",
      "Iteration 54, loss = 0.05202571\n",
      "Iteration 55, loss = 0.05051581\n",
      "Iteration 56, loss = 0.04907972\n",
      "Iteration 57, loss = 0.04757636\n",
      "Iteration 58, loss = 0.04623802\n",
      "Iteration 59, loss = 0.04498722\n",
      "Iteration 60, loss = 0.04360067\n",
      "Iteration 61, loss = 0.04226776\n",
      "Iteration 62, loss = 0.04096857\n",
      "Iteration 63, loss = 0.03980054\n",
      "Iteration 64, loss = 0.03876407\n",
      "Iteration 65, loss = 0.03754087\n",
      "Iteration 66, loss = 0.03651292\n",
      "Iteration 67, loss = 0.03551133\n",
      "Iteration 68, loss = 0.03453602\n",
      "Iteration 69, loss = 0.03365836\n",
      "Iteration 70, loss = 0.03271300\n",
      "Iteration 71, loss = 0.03182581\n",
      "Iteration 72, loss = 0.03090393\n",
      "Iteration 73, loss = 0.03011431\n",
      "Iteration 74, loss = 0.02927920\n",
      "Iteration 75, loss = 0.02845924\n",
      "Iteration 76, loss = 0.02770743\n",
      "Iteration 77, loss = 0.02702198\n",
      "Iteration 78, loss = 0.02634671\n",
      "Iteration 79, loss = 0.02569093\n",
      "Iteration 80, loss = 0.02501075\n",
      "Iteration 81, loss = 0.02437087\n",
      "Iteration 82, loss = 0.02371969\n",
      "Iteration 83, loss = 0.02311370\n",
      "Iteration 84, loss = 0.02255307\n",
      "Iteration 85, loss = 0.02199899\n",
      "Iteration 86, loss = 0.02152939\n",
      "Iteration 87, loss = 0.02099717\n",
      "Iteration 88, loss = 0.02052357\n",
      "Iteration 89, loss = 0.02006071\n",
      "Iteration 90, loss = 0.01962998\n",
      "Iteration 91, loss = 0.01920756\n",
      "Iteration 92, loss = 0.01881026\n",
      "Iteration 93, loss = 0.01839429\n",
      "Iteration 94, loss = 0.01800520\n",
      "Iteration 95, loss = 0.01762724\n",
      "Iteration 96, loss = 0.01726344\n",
      "Iteration 97, loss = 0.01692764\n",
      "Iteration 98, loss = 0.01659557\n",
      "Iteration 99, loss = 0.01626172\n",
      "Iteration 100, loss = 0.01595116\n",
      "Iteration 101, loss = 0.01563666\n",
      "Iteration 102, loss = 0.01534941\n",
      "Iteration 103, loss = 0.01505668\n",
      "Iteration 104, loss = 0.01478140\n",
      "Iteration 105, loss = 0.01451590\n",
      "Iteration 106, loss = 0.01424712\n",
      "Iteration 107, loss = 0.01399697\n",
      "Iteration 108, loss = 0.01375302\n",
      "Iteration 109, loss = 0.01351417\n",
      "Iteration 110, loss = 0.01328429\n",
      "Iteration 111, loss = 0.01306522\n",
      "Iteration 112, loss = 0.01285148\n",
      "Iteration 113, loss = 0.01264342\n",
      "Iteration 114, loss = 0.01243939\n",
      "Iteration 115, loss = 0.01224604\n",
      "Iteration 116, loss = 0.01206042\n",
      "Iteration 117, loss = 0.01187412\n",
      "Iteration 118, loss = 0.01169510\n",
      "Iteration 119, loss = 0.01151950\n",
      "Iteration 120, loss = 0.01136107\n",
      "Iteration 121, loss = 0.01118937\n",
      "Iteration 122, loss = 0.01103253\n",
      "Iteration 123, loss = 0.01087778\n",
      "Iteration 124, loss = 0.01073882\n",
      "Iteration 125, loss = 0.01058783\n",
      "Iteration 126, loss = 0.01044956\n",
      "Iteration 127, loss = 0.01031392\n",
      "Iteration 128, loss = 0.01018178\n",
      "Iteration 129, loss = 0.01005673\n",
      "Iteration 130, loss = 0.00993237\n",
      "Iteration 131, loss = 0.00981344\n",
      "Iteration 132, loss = 0.00969787\n",
      "Iteration 133, loss = 0.00958876\n",
      "Iteration 134, loss = 0.00947933\n",
      "Iteration 135, loss = 0.00937237\n",
      "Iteration 136, loss = 0.00927247\n",
      "Iteration 137, loss = 0.00917453\n",
      "Iteration 138, loss = 0.00907717\n",
      "Iteration 139, loss = 0.00898982\n",
      "Iteration 140, loss = 0.00890324\n",
      "Iteration 141, loss = 0.00880981\n",
      "Iteration 142, loss = 0.00872568\n",
      "Iteration 143, loss = 0.00864289\n",
      "Iteration 144, loss = 0.00856476\n",
      "Iteration 145, loss = 0.00848689\n",
      "Iteration 146, loss = 0.00841211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69980301\n",
      "Iteration 2, loss = 0.67556411\n",
      "Iteration 3, loss = 0.61113353\n",
      "Iteration 4, loss = 0.53140344\n",
      "Iteration 5, loss = 0.47122470\n",
      "Iteration 6, loss = 0.43008237\n",
      "Iteration 7, loss = 0.39932644\n",
      "Iteration 8, loss = 0.37437806\n",
      "Iteration 9, loss = 0.35323182\n",
      "Iteration 10, loss = 0.33455079\n",
      "Iteration 11, loss = 0.31791292\n",
      "Iteration 12, loss = 0.30257873\n",
      "Iteration 13, loss = 0.28830096\n",
      "Iteration 14, loss = 0.27559993\n",
      "Iteration 15, loss = 0.26323446\n",
      "Iteration 16, loss = 0.25179652\n",
      "Iteration 17, loss = 0.24111153\n",
      "Iteration 18, loss = 0.23090520\n",
      "Iteration 19, loss = 0.22157669\n",
      "Iteration 20, loss = 0.21257116\n",
      "Iteration 21, loss = 0.20435174\n",
      "Iteration 22, loss = 0.19648048\n",
      "Iteration 23, loss = 0.18909691\n",
      "Iteration 24, loss = 0.18200217\n",
      "Iteration 25, loss = 0.17535181\n",
      "Iteration 26, loss = 0.16896358\n",
      "Iteration 27, loss = 0.16283019\n",
      "Iteration 28, loss = 0.15700422\n",
      "Iteration 29, loss = 0.15159953\n",
      "Iteration 30, loss = 0.14690865\n",
      "Iteration 31, loss = 0.14224937\n",
      "Iteration 32, loss = 0.13932830\n",
      "Iteration 33, loss = 0.13325832\n",
      "Iteration 34, loss = 0.12891816\n",
      "Iteration 35, loss = 0.12474645\n",
      "Iteration 36, loss = 0.12085420\n",
      "Iteration 37, loss = 0.11712634\n",
      "Iteration 38, loss = 0.11357165\n",
      "Iteration 39, loss = 0.11304200\n",
      "Iteration 40, loss = 0.10732099\n",
      "Iteration 41, loss = 0.10421758\n",
      "Iteration 42, loss = 0.10097301\n",
      "Iteration 43, loss = 0.09806440\n",
      "Iteration 44, loss = 0.09525137\n",
      "Iteration 45, loss = 0.09262464\n",
      "Iteration 46, loss = 0.09002964\n",
      "Iteration 47, loss = 0.08755816\n",
      "Iteration 48, loss = 0.08517923\n",
      "Iteration 49, loss = 0.08293146\n",
      "Iteration 50, loss = 0.08066865\n",
      "Iteration 51, loss = 0.07864948\n",
      "Iteration 52, loss = 0.07659121\n",
      "Iteration 53, loss = 0.07462923\n",
      "Iteration 54, loss = 0.07279761\n",
      "Iteration 55, loss = 0.07089112\n",
      "Iteration 56, loss = 0.06911577\n",
      "Iteration 57, loss = 0.06749469\n",
      "Iteration 58, loss = 0.06583435\n",
      "Iteration 59, loss = 0.06425429\n",
      "Iteration 60, loss = 0.06278376\n",
      "Iteration 61, loss = 0.06128518\n",
      "Iteration 62, loss = 0.05993118\n",
      "Iteration 63, loss = 0.05849605\n",
      "Iteration 64, loss = 0.05728986\n",
      "Iteration 65, loss = 0.05589668\n",
      "Iteration 66, loss = 0.05464818\n",
      "Iteration 67, loss = 0.05350376\n",
      "Iteration 68, loss = 0.05227491\n",
      "Iteration 69, loss = 0.05113629\n",
      "Iteration 70, loss = 0.05003017\n",
      "Iteration 71, loss = 0.04899580\n",
      "Iteration 72, loss = 0.04915619\n",
      "Iteration 73, loss = 0.04715325\n",
      "Iteration 74, loss = 0.04609905\n",
      "Iteration 75, loss = 0.04521772\n",
      "Iteration 76, loss = 0.04422523\n",
      "Iteration 77, loss = 0.04344977\n",
      "Iteration 78, loss = 0.04261738\n",
      "Iteration 79, loss = 0.04181082\n",
      "Iteration 80, loss = 0.04098362\n",
      "Iteration 81, loss = 0.04016711\n",
      "Iteration 82, loss = 0.03950264\n",
      "Iteration 83, loss = 0.03874788\n",
      "Iteration 84, loss = 0.03804149\n",
      "Iteration 85, loss = 0.03737711\n",
      "Iteration 86, loss = 0.03675650\n",
      "Iteration 87, loss = 0.03615949\n",
      "Iteration 88, loss = 0.03553550\n",
      "Iteration 89, loss = 0.03495725\n",
      "Iteration 90, loss = 0.03439555\n",
      "Iteration 91, loss = 0.03390089\n",
      "Iteration 92, loss = 0.03339922\n",
      "Iteration 93, loss = 0.03288045\n",
      "Iteration 94, loss = 0.03240305\n",
      "Iteration 95, loss = 0.03196318\n",
      "Iteration 96, loss = 0.03147760\n",
      "Iteration 97, loss = 0.03110311\n",
      "Iteration 98, loss = 0.03064736\n",
      "Iteration 99, loss = 0.03023877\n",
      "Iteration 100, loss = 0.02987624\n",
      "Iteration 101, loss = 0.02948380\n",
      "Iteration 102, loss = 0.02914356\n",
      "Iteration 103, loss = 0.02881086\n",
      "Iteration 104, loss = 0.02847495\n",
      "Iteration 105, loss = 0.02813811\n",
      "Iteration 106, loss = 0.02781720\n",
      "Iteration 107, loss = 0.02752488\n",
      "Iteration 108, loss = 0.02722696\n",
      "Iteration 109, loss = 0.02696017\n",
      "Iteration 110, loss = 0.02669856\n",
      "Iteration 111, loss = 0.02643786\n",
      "Iteration 112, loss = 0.02619736\n",
      "Iteration 113, loss = 0.02596495\n",
      "Iteration 114, loss = 0.02579173\n",
      "Iteration 115, loss = 0.02520166\n",
      "Iteration 116, loss = 0.02496829\n",
      "Iteration 117, loss = 0.02474511\n",
      "Iteration 118, loss = 0.02455508\n",
      "Iteration 119, loss = 0.02434480\n",
      "Iteration 120, loss = 0.02416201\n",
      "Iteration 121, loss = 0.02398396\n",
      "Iteration 122, loss = 0.02380308\n",
      "Iteration 123, loss = 0.02364137\n",
      "Iteration 124, loss = 0.02347133\n",
      "Iteration 125, loss = 0.02332678\n",
      "Iteration 126, loss = 0.02317283\n",
      "Iteration 127, loss = 0.02303741\n",
      "Iteration 128, loss = 0.02290155\n",
      "Iteration 129, loss = 0.02276589\n",
      "Iteration 130, loss = 0.02264170\n",
      "Iteration 131, loss = 0.02252617\n",
      "Iteration 132, loss = 0.02241081\n",
      "Iteration 133, loss = 0.02230608\n",
      "Iteration 134, loss = 0.02219564\n",
      "Iteration 135, loss = 0.02209261\n",
      "Iteration 136, loss = 0.02199384\n",
      "Iteration 137, loss = 0.02191132\n",
      "Iteration 138, loss = 0.02181707\n",
      "Iteration 139, loss = 0.02172950\n",
      "Iteration 140, loss = 0.02165033\n",
      "Iteration 141, loss = 0.02157121\n",
      "Iteration 142, loss = 0.02149249\n",
      "Iteration 143, loss = 0.02141982\n",
      "Iteration 144, loss = 0.02135191\n",
      "Iteration 145, loss = 0.02128563\n",
      "Iteration 146, loss = 0.02122455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63875203\n",
      "Iteration 2, loss = 0.54527419\n",
      "Iteration 3, loss = 0.45822074\n",
      "Iteration 4, loss = 0.37788066\n",
      "Iteration 5, loss = 0.30908524\n",
      "Iteration 6, loss = 0.26544446\n",
      "Iteration 7, loss = 0.23741413\n",
      "Iteration 8, loss = 0.21718098\n",
      "Iteration 9, loss = 0.20079209\n",
      "Iteration 10, loss = 0.18716150\n",
      "Iteration 11, loss = 0.17421913\n",
      "Iteration 12, loss = 0.14825175\n",
      "Iteration 13, loss = 0.11859955\n",
      "Iteration 14, loss = 0.09928479\n",
      "Iteration 15, loss = 0.08628382\n",
      "Iteration 16, loss = 0.07670386\n",
      "Iteration 17, loss = 0.06952956\n",
      "Iteration 18, loss = 0.06375999\n",
      "Iteration 19, loss = 0.05917830\n",
      "Iteration 20, loss = 0.05521792\n",
      "Iteration 21, loss = 0.05214770\n",
      "Iteration 22, loss = 0.04932433\n",
      "Iteration 23, loss = 0.04704628\n",
      "Iteration 24, loss = 0.04507807\n",
      "Iteration 25, loss = 0.04318226\n",
      "Iteration 26, loss = 0.04164439\n",
      "Iteration 27, loss = 0.04024366\n",
      "Iteration 28, loss = 0.03883552\n",
      "Iteration 29, loss = 0.03767460\n",
      "Iteration 30, loss = 0.03658277\n",
      "Iteration 31, loss = 0.03553492\n",
      "Iteration 32, loss = 0.03456727\n",
      "Iteration 33, loss = 0.03354015\n",
      "Iteration 34, loss = 0.03264992\n",
      "Iteration 35, loss = 0.03173277\n",
      "Iteration 36, loss = 0.03093386\n",
      "Iteration 37, loss = 0.03004741\n",
      "Iteration 38, loss = 0.02932384\n",
      "Iteration 39, loss = 0.02863666\n",
      "Iteration 40, loss = 0.02804574\n",
      "Iteration 41, loss = 0.02750371\n",
      "Iteration 42, loss = 0.02693613\n",
      "Iteration 43, loss = 0.02613302\n",
      "Iteration 44, loss = 0.02544440\n",
      "Iteration 45, loss = 0.02484233\n",
      "Iteration 46, loss = 0.02392437\n",
      "Iteration 47, loss = 0.02341837\n",
      "Iteration 48, loss = 0.02279296\n",
      "Iteration 49, loss = 0.02221301\n",
      "Iteration 50, loss = 0.02166997\n",
      "Iteration 51, loss = 0.02108954\n",
      "Iteration 52, loss = 0.02065792\n",
      "Iteration 53, loss = 0.02034219\n",
      "Iteration 54, loss = 0.01982360\n",
      "Iteration 55, loss = 0.01939781\n",
      "Iteration 56, loss = 0.01923176\n",
      "Iteration 57, loss = 0.01878183\n",
      "Iteration 58, loss = 0.01833185\n",
      "Iteration 59, loss = 0.01819691\n",
      "Iteration 60, loss = 0.01786789\n",
      "Iteration 61, loss = 0.01766675\n",
      "Iteration 62, loss = 0.01722756\n",
      "Iteration 63, loss = 0.01695418\n",
      "Iteration 64, loss = 0.01666429\n",
      "Iteration 65, loss = 0.01644122\n",
      "Iteration 66, loss = 0.01616125\n",
      "Iteration 67, loss = 0.01601444\n",
      "Iteration 68, loss = 0.01556909\n",
      "Iteration 69, loss = 0.01547439\n",
      "Iteration 70, loss = 0.01525668\n",
      "Iteration 71, loss = 0.01496036\n",
      "Iteration 72, loss = 0.01491365\n",
      "Iteration 73, loss = 0.01445126\n",
      "Iteration 74, loss = 0.01417530\n",
      "Iteration 75, loss = 0.01406584\n",
      "Iteration 76, loss = 0.01386095\n",
      "Iteration 77, loss = 0.01367664\n",
      "Iteration 78, loss = 0.01339649\n",
      "Iteration 79, loss = 0.01321812\n",
      "Iteration 80, loss = 0.01298074\n",
      "Iteration 81, loss = 0.01292504\n",
      "Iteration 82, loss = 0.01259557\n",
      "Iteration 83, loss = 0.01251148\n",
      "Iteration 84, loss = 0.01235434\n",
      "Iteration 85, loss = 0.01217379\n",
      "Iteration 86, loss = 0.01196106\n",
      "Iteration 87, loss = 0.01185459\n",
      "Iteration 88, loss = 0.01176177\n",
      "Iteration 89, loss = 0.01154847\n",
      "Iteration 90, loss = 0.01139458\n",
      "Iteration 91, loss = 0.01125635\n",
      "Iteration 92, loss = 0.01132286\n",
      "Iteration 93, loss = 0.01104324\n",
      "Iteration 94, loss = 0.01099671\n",
      "Iteration 95, loss = 0.01323550\n",
      "Iteration 96, loss = 0.01111910\n",
      "Iteration 97, loss = 0.01089827\n",
      "Iteration 98, loss = 0.01074219\n",
      "Iteration 99, loss = 0.01071527\n",
      "Iteration 100, loss = 0.01058535\n",
      "Iteration 101, loss = 0.01048972\n",
      "Iteration 102, loss = 0.01042517\n",
      "Iteration 103, loss = 0.01036297\n",
      "Iteration 104, loss = 0.01033240\n",
      "Iteration 105, loss = 0.01058060\n",
      "Iteration 106, loss = 0.01026916\n",
      "Iteration 107, loss = 0.01018293\n",
      "Iteration 108, loss = 0.01011741\n",
      "Iteration 109, loss = 0.01009336\n",
      "Iteration 110, loss = 0.01004672\n",
      "Iteration 111, loss = 0.01000375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12015732\n",
      "Iteration 2, loss = 1.09276425\n",
      "Iteration 3, loss = 1.06637302\n",
      "Iteration 4, loss = 1.04168410\n",
      "Iteration 5, loss = 1.01792305\n",
      "Iteration 6, loss = 0.99425945\n",
      "Iteration 7, loss = 0.97222801\n",
      "Iteration 8, loss = 0.95118401\n",
      "Iteration 9, loss = 0.93029359\n",
      "Iteration 10, loss = 0.91029590\n",
      "Iteration 11, loss = 0.89124466\n",
      "Iteration 12, loss = 0.87381041\n",
      "Iteration 13, loss = 0.85668675\n",
      "Iteration 14, loss = 0.84035392\n",
      "Iteration 15, loss = 0.82552451\n",
      "Iteration 16, loss = 0.81098720\n",
      "Iteration 17, loss = 0.79782196\n",
      "Iteration 18, loss = 0.78546717\n",
      "Iteration 19, loss = 0.77326709\n",
      "Iteration 20, loss = 0.76174599\n",
      "Iteration 21, loss = 0.75152658\n",
      "Iteration 22, loss = 0.74206265\n",
      "Iteration 23, loss = 0.73266920\n",
      "Iteration 24, loss = 0.72438358\n",
      "Iteration 25, loss = 0.71674424\n",
      "Iteration 26, loss = 0.70915675\n",
      "Iteration 27, loss = 0.70254597\n",
      "Iteration 28, loss = 0.69602288\n",
      "Iteration 29, loss = 0.69043428\n",
      "Iteration 30, loss = 0.68533876\n",
      "Iteration 31, loss = 0.68021734\n",
      "Iteration 32, loss = 0.67587973\n",
      "Iteration 33, loss = 0.67156454\n",
      "Iteration 34, loss = 0.66758920\n",
      "Iteration 35, loss = 0.66396453\n",
      "Iteration 36, loss = 0.66070655\n",
      "Iteration 37, loss = 0.65810050\n",
      "Iteration 38, loss = 0.65580559\n",
      "Iteration 39, loss = 0.65341010\n",
      "Iteration 40, loss = 0.65149939\n",
      "Iteration 41, loss = 0.64986670\n",
      "Iteration 42, loss = 0.64807306\n",
      "Iteration 43, loss = 0.64673492\n",
      "Iteration 44, loss = 0.64531644\n",
      "Iteration 45, loss = 0.64401073\n",
      "Iteration 46, loss = 0.64287515\n",
      "Iteration 47, loss = 0.64187992\n",
      "Iteration 48, loss = 0.64102103\n",
      "Iteration 49, loss = 0.64046908\n",
      "Iteration 50, loss = 0.63984104\n",
      "Iteration 51, loss = 0.63925012\n",
      "Iteration 52, loss = 0.63876491\n",
      "Iteration 53, loss = 0.63834044\n",
      "Iteration 54, loss = 0.63799610\n",
      "Iteration 55, loss = 0.63782246\n",
      "Iteration 56, loss = 0.63758912\n",
      "Iteration 57, loss = 0.63746441\n",
      "Iteration 58, loss = 0.63739106\n",
      "Iteration 59, loss = 0.63732727\n",
      "Iteration 60, loss = 0.63716000\n",
      "Iteration 61, loss = 0.63700412\n",
      "Iteration 62, loss = 0.63697417\n",
      "Iteration 63, loss = 0.63686693\n",
      "Iteration 64, loss = 0.63679038\n",
      "Iteration 65, loss = 0.63677551\n",
      "Iteration 66, loss = 0.63677753\n",
      "Iteration 67, loss = 0.63671897\n",
      "Iteration 68, loss = 0.63671585\n",
      "Iteration 69, loss = 0.63667621\n",
      "Iteration 70, loss = 0.63667569\n",
      "Iteration 71, loss = 0.63669795\n",
      "Iteration 72, loss = 0.63664903\n",
      "Iteration 73, loss = 0.63661131\n",
      "Iteration 74, loss = 0.63658171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81186047\n",
      "Iteration 2, loss = 0.72439751\n",
      "Iteration 3, loss = 0.55589561\n",
      "Iteration 4, loss = 0.38435241\n",
      "Iteration 5, loss = 0.28943891\n",
      "Iteration 6, loss = 0.24789989\n",
      "Iteration 7, loss = 0.22505400\n",
      "Iteration 8, loss = 0.20976464\n",
      "Iteration 9, loss = 0.19782580\n",
      "Iteration 10, loss = 0.18793293\n",
      "Iteration 11, loss = 0.17906450\n",
      "Iteration 12, loss = 0.17118337\n",
      "Iteration 13, loss = 0.16409447\n",
      "Iteration 14, loss = 0.15729068\n",
      "Iteration 15, loss = 0.15092834\n",
      "Iteration 16, loss = 0.14497042\n",
      "Iteration 17, loss = 0.13940101\n",
      "Iteration 18, loss = 0.13414564\n",
      "Iteration 19, loss = 0.12920399\n",
      "Iteration 20, loss = 0.12468986\n",
      "Iteration 21, loss = 0.12031604\n",
      "Iteration 22, loss = 0.11611960\n",
      "Iteration 23, loss = 0.11216477\n",
      "Iteration 24, loss = 0.10837050\n",
      "Iteration 25, loss = 0.10463462\n",
      "Iteration 26, loss = 0.10112759\n",
      "Iteration 27, loss = 0.09772564\n",
      "Iteration 28, loss = 0.09457207\n",
      "Iteration 29, loss = 0.09145574\n",
      "Iteration 30, loss = 0.08838704\n",
      "Iteration 31, loss = 0.08564023\n",
      "Iteration 32, loss = 0.08301038\n",
      "Iteration 33, loss = 0.08037107\n",
      "Iteration 34, loss = 0.07791667\n",
      "Iteration 35, loss = 0.07545673\n",
      "Iteration 36, loss = 0.07316159\n",
      "Iteration 37, loss = 0.07090817\n",
      "Iteration 38, loss = 0.06883128\n",
      "Iteration 39, loss = 0.06677937\n",
      "Iteration 40, loss = 0.06487076\n",
      "Iteration 41, loss = 0.06281700\n",
      "Iteration 42, loss = 0.06109023\n",
      "Iteration 43, loss = 0.05916839\n",
      "Iteration 44, loss = 0.05763357\n",
      "Iteration 45, loss = 0.05586350\n",
      "Iteration 46, loss = 0.05423643\n",
      "Iteration 47, loss = 0.05267435\n",
      "Iteration 48, loss = 0.05124654\n",
      "Iteration 49, loss = 0.04978787\n",
      "Iteration 50, loss = 0.04839219\n",
      "Iteration 51, loss = 0.04697263\n",
      "Iteration 52, loss = 0.04556918\n",
      "Iteration 53, loss = 0.04432405\n",
      "Iteration 54, loss = 0.04306670\n",
      "Iteration 55, loss = 0.04184459\n",
      "Iteration 56, loss = 0.04371395\n",
      "Iteration 57, loss = 0.04050463\n",
      "Iteration 58, loss = 0.03926975\n",
      "Iteration 59, loss = 0.03818094\n",
      "Iteration 60, loss = 0.03719972\n",
      "Iteration 61, loss = 0.03627946\n",
      "Iteration 62, loss = 0.03538606\n",
      "Iteration 63, loss = 0.03449829\n",
      "Iteration 64, loss = 0.03362254\n",
      "Iteration 65, loss = 0.03281108\n",
      "Iteration 66, loss = 0.03199863\n",
      "Iteration 67, loss = 0.03127802\n",
      "Iteration 68, loss = 0.03044231\n",
      "Iteration 69, loss = 0.02970606\n",
      "Iteration 70, loss = 0.02897627\n",
      "Iteration 71, loss = 0.02827521\n",
      "Iteration 72, loss = 0.02760317\n",
      "Iteration 73, loss = 0.02687512\n",
      "Iteration 74, loss = 0.02622389\n",
      "Iteration 75, loss = 0.02560442\n",
      "Iteration 76, loss = 0.02493580\n",
      "Iteration 77, loss = 0.02427074\n",
      "Iteration 78, loss = 0.02366283\n",
      "Iteration 79, loss = 0.02300374\n",
      "Iteration 80, loss = 0.02248374\n",
      "Iteration 81, loss = 0.02180332\n",
      "Iteration 82, loss = 0.02129585\n",
      "Iteration 83, loss = 0.02064555\n",
      "Iteration 84, loss = 0.02010063\n",
      "Iteration 85, loss = 0.01951470\n",
      "Iteration 86, loss = 0.01902718\n",
      "Iteration 87, loss = 0.01869262\n",
      "Iteration 88, loss = 0.01801738\n",
      "Iteration 89, loss = 0.01748758\n",
      "Iteration 90, loss = 0.01701879\n",
      "Iteration 91, loss = 0.01663080\n",
      "Iteration 92, loss = 0.01610102\n",
      "Iteration 93, loss = 0.01561612\n",
      "Iteration 94, loss = 0.01517864\n",
      "Iteration 95, loss = 0.01484891\n",
      "Iteration 96, loss = 0.01435880\n",
      "Iteration 97, loss = 0.01395015\n",
      "Iteration 98, loss = 0.01350895\n",
      "Iteration 99, loss = 0.01368549\n",
      "Iteration 100, loss = 0.01291370\n",
      "Iteration 101, loss = 0.01241322\n",
      "Iteration 102, loss = 0.01251084\n",
      "Iteration 103, loss = 0.01178482\n",
      "Iteration 104, loss = 0.01138551\n",
      "Iteration 105, loss = 0.01108086\n",
      "Iteration 106, loss = 0.01072726\n",
      "Iteration 107, loss = 0.01045021\n",
      "Iteration 108, loss = 0.01014113\n",
      "Iteration 109, loss = 0.00981842\n",
      "Iteration 110, loss = 0.00951402\n",
      "Iteration 111, loss = 0.00934491\n",
      "Iteration 112, loss = 0.00897536\n",
      "Iteration 113, loss = 0.00873290\n",
      "Iteration 114, loss = 0.00847106\n",
      "Iteration 115, loss = 0.00821435\n",
      "Iteration 116, loss = 0.00796069\n",
      "Iteration 117, loss = 0.00769100\n",
      "Iteration 118, loss = 0.00747419\n",
      "Iteration 119, loss = 0.00733122\n",
      "Iteration 120, loss = 0.00707851\n",
      "Iteration 121, loss = 0.00686385\n",
      "Iteration 122, loss = 0.00666854\n",
      "Iteration 123, loss = 0.00649298\n",
      "Iteration 124, loss = 0.00630297\n",
      "Iteration 125, loss = 0.00612067\n",
      "Iteration 126, loss = 0.00595153\n",
      "Iteration 127, loss = 0.00579891\n",
      "Iteration 128, loss = 0.00564734\n",
      "Iteration 129, loss = 0.00549561\n",
      "Iteration 130, loss = 0.00534478\n",
      "Iteration 131, loss = 0.00520126\n",
      "Iteration 132, loss = 0.00507060\n",
      "Iteration 133, loss = 0.00491774\n",
      "Iteration 134, loss = 0.00481079\n",
      "Iteration 135, loss = 0.00468292\n",
      "Iteration 136, loss = 0.00455361\n",
      "Iteration 137, loss = 0.00445468\n",
      "Iteration 138, loss = 0.00433179\n",
      "Iteration 139, loss = 0.00422431\n",
      "Iteration 140, loss = 0.00412429\n",
      "Iteration 141, loss = 0.00401321\n",
      "Iteration 142, loss = 0.00392157\n",
      "Iteration 143, loss = 0.00382670\n",
      "Iteration 144, loss = 0.00373962\n",
      "Iteration 145, loss = 0.00365032\n",
      "Iteration 146, loss = 0.00356094\n",
      "Iteration 147, loss = 0.00348286\n",
      "Iteration 148, loss = 0.00341088\n",
      "Iteration 149, loss = 0.00332016\n",
      "Iteration 150, loss = 0.00324386\n",
      "Iteration 151, loss = 0.00317962\n",
      "Iteration 152, loss = 0.00310776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67706303\n",
      "Iteration 2, loss = 0.64185697\n",
      "Iteration 3, loss = 0.62779259\n",
      "Iteration 4, loss = 0.61398338\n",
      "Iteration 5, loss = 0.59319232\n",
      "Iteration 6, loss = 0.56201835\n",
      "Iteration 7, loss = 0.51747865\n",
      "Iteration 8, loss = 0.45901754\n",
      "Iteration 9, loss = 0.39121352\n",
      "Iteration 10, loss = 0.32304899\n",
      "Iteration 11, loss = 0.26309615\n",
      "Iteration 12, loss = 0.21469600\n",
      "Iteration 13, loss = 0.17765339\n",
      "Iteration 14, loss = 0.14984054\n",
      "Iteration 15, loss = 0.12890565\n",
      "Iteration 16, loss = 0.11305564\n",
      "Iteration 17, loss = 0.10083303\n",
      "Iteration 18, loss = 0.09119106\n",
      "Iteration 19, loss = 0.08342329\n",
      "Iteration 20, loss = 0.07705931\n",
      "Iteration 21, loss = 0.07181596\n",
      "Iteration 22, loss = 0.06743533\n",
      "Iteration 23, loss = 0.06347267\n",
      "Iteration 24, loss = 0.06021465\n",
      "Iteration 25, loss = 0.05730720\n",
      "Iteration 26, loss = 0.05464809\n",
      "Iteration 27, loss = 0.05232750\n",
      "Iteration 28, loss = 0.05011973\n",
      "Iteration 29, loss = 0.04826350\n",
      "Iteration 30, loss = 0.04659550\n",
      "Iteration 31, loss = 0.04505264\n",
      "Iteration 32, loss = 0.04374469\n",
      "Iteration 33, loss = 0.04247485\n",
      "Iteration 34, loss = 0.04138719\n",
      "Iteration 35, loss = 0.04037535\n",
      "Iteration 36, loss = 0.03939226\n",
      "Iteration 37, loss = 0.03859744\n",
      "Iteration 38, loss = 0.03770936\n",
      "Iteration 39, loss = 0.03698153\n",
      "Iteration 40, loss = 0.03624615\n",
      "Iteration 41, loss = 0.03580877\n",
      "Iteration 42, loss = 0.03497272\n",
      "Iteration 43, loss = 0.03433849\n",
      "Iteration 44, loss = 0.03378830\n",
      "Iteration 45, loss = 0.03343402\n",
      "Iteration 46, loss = 0.03275641\n",
      "Iteration 47, loss = 0.03232791\n",
      "Iteration 48, loss = 0.03186937\n",
      "Iteration 49, loss = 0.03135873\n",
      "Iteration 50, loss = 0.03099388\n",
      "Iteration 51, loss = 0.03055093\n",
      "Iteration 52, loss = 0.03020851\n",
      "Iteration 53, loss = 0.02994308\n",
      "Iteration 54, loss = 0.02959648\n",
      "Iteration 55, loss = 0.02933433\n",
      "Iteration 56, loss = 0.02906433\n",
      "Iteration 57, loss = 0.02881115\n",
      "Iteration 58, loss = 0.02855642\n",
      "Iteration 59, loss = 0.02823038\n",
      "Iteration 60, loss = 0.02806856\n",
      "Iteration 61, loss = 0.02778084\n",
      "Iteration 62, loss = 0.02751050\n",
      "Iteration 63, loss = 0.02729319\n",
      "Iteration 64, loss = 0.02710214\n",
      "Iteration 65, loss = 0.02678013\n",
      "Iteration 66, loss = 0.02654453\n",
      "Iteration 67, loss = 0.02636676\n",
      "Iteration 68, loss = 0.02605202\n",
      "Iteration 69, loss = 0.02580518\n",
      "Iteration 70, loss = 0.02542713\n",
      "Iteration 71, loss = 0.02502722\n",
      "Iteration 72, loss = 0.02450763\n",
      "Iteration 73, loss = 0.02396968\n",
      "Iteration 74, loss = 0.02356410\n",
      "Iteration 75, loss = 0.02327282\n",
      "Iteration 76, loss = 0.02290308\n",
      "Iteration 77, loss = 0.02254004\n",
      "Iteration 78, loss = 0.02227905\n",
      "Iteration 79, loss = 0.02193438\n",
      "Iteration 80, loss = 0.02149238\n",
      "Iteration 81, loss = 0.02108533\n",
      "Iteration 82, loss = 0.02080274\n",
      "Iteration 83, loss = 0.02036195\n",
      "Iteration 84, loss = 0.02003087\n",
      "Iteration 85, loss = 0.01972603\n",
      "Iteration 86, loss = 0.01942249\n",
      "Iteration 87, loss = 0.01918026\n",
      "Iteration 88, loss = 0.01895322\n",
      "Iteration 89, loss = 0.01865850\n",
      "Iteration 90, loss = 0.01813880\n",
      "Iteration 91, loss = 0.01797371\n",
      "Iteration 92, loss = 0.01782331\n",
      "Iteration 93, loss = 0.01732450\n",
      "Iteration 94, loss = 0.01701561\n",
      "Iteration 95, loss = 0.01671051\n",
      "Iteration 96, loss = 0.01648263\n",
      "Iteration 97, loss = 0.01626711\n",
      "Iteration 98, loss = 0.01608372\n",
      "Iteration 99, loss = 0.01596283\n",
      "Iteration 100, loss = 0.01589853\n",
      "Iteration 101, loss = 0.01563601\n",
      "Iteration 102, loss = 0.01548219\n",
      "Iteration 103, loss = 0.01543012\n",
      "Iteration 104, loss = 0.01528576\n",
      "Iteration 105, loss = 0.01516049\n",
      "Iteration 106, loss = 0.01500017\n",
      "Iteration 107, loss = 0.01485425\n",
      "Iteration 108, loss = 0.01478126\n",
      "Iteration 109, loss = 0.01458479\n",
      "Iteration 110, loss = 0.01449779\n",
      "Iteration 111, loss = 0.01440916\n",
      "Iteration 112, loss = 0.01429660\n",
      "Iteration 113, loss = 0.01420333\n",
      "Iteration 114, loss = 0.01412959\n",
      "Iteration 115, loss = 0.01407427\n",
      "Iteration 116, loss = 0.01400763\n",
      "Iteration 117, loss = 0.01397145\n",
      "Iteration 118, loss = 0.01391476\n",
      "Iteration 119, loss = 0.01389646\n",
      "Iteration 120, loss = 0.01384062\n",
      "Iteration 121, loss = 0.01382331\n",
      "Iteration 122, loss = 0.01377636\n",
      "Iteration 123, loss = 0.01373011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.78070842\n",
      "Iteration 2, loss = 0.70087874\n",
      "Iteration 3, loss = 0.66004860\n",
      "Iteration 4, loss = 0.63647258\n",
      "Iteration 5, loss = 0.61822396\n",
      "Iteration 6, loss = 0.60169105\n",
      "Iteration 7, loss = 0.58232804\n",
      "Iteration 8, loss = 0.55742444\n",
      "Iteration 9, loss = 0.52645237\n",
      "Iteration 10, loss = 0.48963405\n",
      "Iteration 11, loss = 0.44752429\n",
      "Iteration 12, loss = 0.40179252\n",
      "Iteration 13, loss = 0.35688638\n",
      "Iteration 14, loss = 0.31510805\n",
      "Iteration 15, loss = 0.27785164\n",
      "Iteration 16, loss = 0.24515006\n",
      "Iteration 17, loss = 0.21653137\n",
      "Iteration 18, loss = 0.19225297\n",
      "Iteration 19, loss = 0.17206948\n",
      "Iteration 20, loss = 0.15655186\n",
      "Iteration 21, loss = 0.14233400\n",
      "Iteration 22, loss = 0.13001903\n",
      "Iteration 23, loss = 0.11945593\n",
      "Iteration 24, loss = 0.11045441\n",
      "Iteration 25, loss = 0.10237330\n",
      "Iteration 26, loss = 0.09531097\n",
      "Iteration 27, loss = 0.08916580\n",
      "Iteration 28, loss = 0.08367044\n",
      "Iteration 29, loss = 0.07878942\n",
      "Iteration 30, loss = 0.07443440\n",
      "Iteration 31, loss = 0.07045768\n",
      "Iteration 32, loss = 0.06695474\n",
      "Iteration 33, loss = 0.06412340\n",
      "Iteration 34, loss = 0.06093424\n",
      "Iteration 35, loss = 0.05825596\n",
      "Iteration 36, loss = 0.05584061\n",
      "Iteration 37, loss = 0.05624082\n",
      "Iteration 38, loss = 0.05315007\n",
      "Iteration 39, loss = 0.05138011\n",
      "Iteration 40, loss = 0.04993806\n",
      "Iteration 41, loss = 0.04861266\n",
      "Iteration 42, loss = 0.04736619\n",
      "Iteration 43, loss = 0.04620694\n",
      "Iteration 44, loss = 0.04514267\n",
      "Iteration 45, loss = 0.04409879\n",
      "Iteration 46, loss = 0.04313660\n",
      "Iteration 47, loss = 0.04223522\n",
      "Iteration 48, loss = 0.04140423\n",
      "Iteration 49, loss = 0.04061908\n",
      "Iteration 50, loss = 0.03987356\n",
      "Iteration 51, loss = 0.03918514\n",
      "Iteration 52, loss = 0.03846425\n",
      "Iteration 53, loss = 0.03784374\n",
      "Iteration 54, loss = 0.03723754\n",
      "Iteration 55, loss = 0.03666511\n",
      "Iteration 56, loss = 0.03611132\n",
      "Iteration 57, loss = 0.03559406\n",
      "Iteration 58, loss = 0.03508256\n",
      "Iteration 59, loss = 0.03460499\n",
      "Iteration 60, loss = 0.03415483\n",
      "Iteration 61, loss = 0.03373463\n",
      "Iteration 62, loss = 0.03330409\n",
      "Iteration 63, loss = 0.03291962\n",
      "Iteration 64, loss = 0.03252771\n",
      "Iteration 65, loss = 0.03217056\n",
      "Iteration 66, loss = 0.03182385\n",
      "Iteration 67, loss = 0.03143965\n",
      "Iteration 68, loss = 0.03116751\n",
      "Iteration 69, loss = 0.03083928\n",
      "Iteration 70, loss = 0.03047845\n",
      "Iteration 71, loss = 0.03022084\n",
      "Iteration 72, loss = 0.02992020\n",
      "Iteration 73, loss = 0.02961143\n",
      "Iteration 74, loss = 0.02939801\n",
      "Iteration 75, loss = 0.02908336\n",
      "Iteration 76, loss = 0.02886519\n",
      "Iteration 77, loss = 0.02859917\n",
      "Iteration 78, loss = 0.02835109\n",
      "Iteration 79, loss = 0.02811533\n",
      "Iteration 80, loss = 0.02789189\n",
      "Iteration 81, loss = 0.02767921\n",
      "Iteration 82, loss = 0.02741294\n",
      "Iteration 83, loss = 0.02715491\n",
      "Iteration 84, loss = 0.02689645\n",
      "Iteration 85, loss = 0.02668399\n",
      "Iteration 86, loss = 0.02624432\n",
      "Iteration 87, loss = 0.02599462\n",
      "Iteration 88, loss = 0.02567084\n",
      "Iteration 89, loss = 0.02540593\n",
      "Iteration 90, loss = 0.02513689\n",
      "Iteration 91, loss = 0.02481989\n",
      "Iteration 92, loss = 0.02456082\n",
      "Iteration 93, loss = 0.02429506\n",
      "Iteration 94, loss = 0.02393548\n",
      "Iteration 95, loss = 0.02367276\n",
      "Iteration 96, loss = 0.02335187\n",
      "Iteration 97, loss = 0.02304243\n",
      "Iteration 98, loss = 0.02281320\n",
      "Iteration 99, loss = 0.02263489\n",
      "Iteration 100, loss = 0.02225071\n",
      "Iteration 101, loss = 0.02191028\n",
      "Iteration 102, loss = 0.02146829\n",
      "Iteration 103, loss = 0.02370817\n",
      "Iteration 104, loss = 0.02150350\n",
      "Iteration 105, loss = 0.02106759\n",
      "Iteration 106, loss = 0.02087263\n",
      "Iteration 107, loss = 0.02064928\n",
      "Iteration 108, loss = 0.02046345\n",
      "Iteration 109, loss = 0.02032855\n",
      "Iteration 110, loss = 0.02018385\n",
      "Iteration 111, loss = 0.02003589\n",
      "Iteration 112, loss = 0.01990423\n",
      "Iteration 113, loss = 0.01980731\n",
      "Iteration 114, loss = 0.01964882\n",
      "Iteration 115, loss = 0.01952077\n",
      "Iteration 116, loss = 0.01940989\n",
      "Iteration 117, loss = 0.01919090\n",
      "Iteration 118, loss = 0.01906027\n",
      "Iteration 119, loss = 0.01890617\n",
      "Iteration 120, loss = 0.01880760\n",
      "Iteration 121, loss = 0.01862242\n",
      "Iteration 122, loss = 0.01841063\n",
      "Iteration 123, loss = 0.01820235\n",
      "Iteration 124, loss = 0.01800404\n",
      "Iteration 125, loss = 0.01776743\n",
      "Iteration 126, loss = 0.01751455\n",
      "Iteration 127, loss = 0.01712376\n",
      "Iteration 128, loss = 0.01691571\n",
      "Iteration 129, loss = 0.01656988\n",
      "Iteration 130, loss = 0.01630020\n",
      "Iteration 131, loss = 0.01613287\n",
      "Iteration 132, loss = 0.01596952\n",
      "Iteration 133, loss = 0.01564356\n",
      "Iteration 134, loss = 0.01548665\n",
      "Iteration 135, loss = 0.01531122\n",
      "Iteration 136, loss = 0.01523580\n",
      "Iteration 137, loss = 0.01506545\n",
      "Iteration 138, loss = 0.01500072\n",
      "Iteration 139, loss = 0.01489997\n",
      "Iteration 140, loss = 0.01475657\n",
      "Iteration 141, loss = 0.01464620\n",
      "Iteration 142, loss = 0.01450041\n",
      "Iteration 143, loss = 0.01436254\n",
      "Iteration 144, loss = 0.01430340\n",
      "Iteration 145, loss = 0.01416052\n",
      "Iteration 146, loss = 0.01403297\n",
      "Iteration 147, loss = 0.01385716\n",
      "Iteration 148, loss = 0.01374298\n",
      "Iteration 149, loss = 0.01363029\n",
      "Iteration 150, loss = 0.01348195\n",
      "Iteration 151, loss = 0.01343723\n",
      "Iteration 152, loss = 0.01326440\n",
      "Iteration 153, loss = 0.01314701\n",
      "Iteration 154, loss = 0.01303057\n",
      "Iteration 155, loss = 0.01301051\n",
      "Iteration 156, loss = 0.01287226\n",
      "Iteration 157, loss = 0.01278129\n",
      "Iteration 158, loss = 0.01262980\n",
      "Iteration 159, loss = 0.01267724\n",
      "Iteration 160, loss = 0.01246828\n",
      "Iteration 161, loss = 0.01239285\n",
      "Iteration 162, loss = 0.01236663\n",
      "Iteration 163, loss = 0.01226106\n",
      "Iteration 164, loss = 0.01217844\n",
      "Iteration 165, loss = 0.01211058\n",
      "Iteration 166, loss = 0.01206440\n",
      "Iteration 167, loss = 0.01195851\n",
      "Iteration 168, loss = 0.01193633\n",
      "Iteration 169, loss = 0.01181054\n",
      "Iteration 170, loss = 0.01175975\n",
      "Iteration 171, loss = 0.01170166\n",
      "Iteration 172, loss = 0.01163278\n",
      "Iteration 173, loss = 0.01157404\n",
      "Iteration 174, loss = 0.01155274\n",
      "Iteration 175, loss = 0.01149868\n",
      "Iteration 176, loss = 0.01144777\n",
      "Iteration 177, loss = 0.01143046\n",
      "Iteration 178, loss = 0.01139294\n",
      "Iteration 179, loss = 0.01134233\n",
      "Iteration 180, loss = 0.01130346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63669348\n",
      "Iteration 2, loss = 0.63311195\n",
      "Iteration 3, loss = 0.62296559\n",
      "Iteration 4, loss = 0.59725905\n",
      "Iteration 5, loss = 0.55311102\n",
      "Iteration 6, loss = 0.49043962\n",
      "Iteration 7, loss = 0.41514877\n",
      "Iteration 8, loss = 0.34207936\n",
      "Iteration 9, loss = 0.27909232\n",
      "Iteration 10, loss = 0.22845647\n",
      "Iteration 11, loss = 0.19412132\n",
      "Iteration 12, loss = 0.16666957\n",
      "Iteration 13, loss = 0.14500036\n",
      "Iteration 14, loss = 0.12839015\n",
      "Iteration 15, loss = 0.11506126\n",
      "Iteration 16, loss = 0.10426596\n",
      "Iteration 17, loss = 0.09524772\n",
      "Iteration 18, loss = 0.08791288\n",
      "Iteration 19, loss = 0.08160661\n",
      "Iteration 20, loss = 0.07635035\n",
      "Iteration 21, loss = 0.07160671\n",
      "Iteration 22, loss = 0.06767239\n",
      "Iteration 23, loss = 0.06666515\n",
      "Iteration 24, loss = 0.06172968\n",
      "Iteration 25, loss = 0.05942104\n",
      "Iteration 26, loss = 0.05734136\n",
      "Iteration 27, loss = 0.05547591\n",
      "Iteration 28, loss = 0.05377365\n",
      "Iteration 29, loss = 0.05225105\n",
      "Iteration 30, loss = 0.05082934\n",
      "Iteration 31, loss = 0.04952497\n",
      "Iteration 32, loss = 0.04828730\n",
      "Iteration 33, loss = 0.04726135\n",
      "Iteration 34, loss = 0.04608568\n",
      "Iteration 35, loss = 0.04497399\n",
      "Iteration 36, loss = 0.04402904\n",
      "Iteration 37, loss = 0.04309067\n",
      "Iteration 38, loss = 0.04224155\n",
      "Iteration 39, loss = 0.04137720\n",
      "Iteration 40, loss = 0.04062552\n",
      "Iteration 41, loss = 0.04110956\n",
      "Iteration 42, loss = 0.03976045\n",
      "Iteration 43, loss = 0.03914699\n",
      "Iteration 44, loss = 0.03847486\n",
      "Iteration 45, loss = 0.03786156\n",
      "Iteration 46, loss = 0.03734638\n",
      "Iteration 47, loss = 0.03684497\n",
      "Iteration 48, loss = 0.03632448\n",
      "Iteration 49, loss = 0.03590576\n",
      "Iteration 50, loss = 0.03533143\n",
      "Iteration 51, loss = 0.03485965\n",
      "Iteration 52, loss = 0.03438877\n",
      "Iteration 53, loss = 0.03397106\n",
      "Iteration 54, loss = 0.03354694\n",
      "Iteration 55, loss = 0.03322949\n",
      "Iteration 56, loss = 0.03276739\n",
      "Iteration 57, loss = 0.03239185\n",
      "Iteration 58, loss = 0.03195107\n",
      "Iteration 59, loss = 0.03170144\n",
      "Iteration 60, loss = 0.03123770\n",
      "Iteration 61, loss = 0.03089199\n",
      "Iteration 62, loss = 0.03049969\n",
      "Iteration 63, loss = 0.03023114\n",
      "Iteration 64, loss = 0.02986689\n",
      "Iteration 65, loss = 0.02965997\n",
      "Iteration 66, loss = 0.02926928\n",
      "Iteration 67, loss = 0.02900820\n",
      "Iteration 68, loss = 0.02887944\n",
      "Iteration 69, loss = 0.02839794\n",
      "Iteration 70, loss = 0.02806527\n",
      "Iteration 71, loss = 0.02778823\n",
      "Iteration 72, loss = 0.02747030\n",
      "Iteration 73, loss = 0.02713104\n",
      "Iteration 74, loss = 0.02674638\n",
      "Iteration 75, loss = 0.02646845\n",
      "Iteration 76, loss = 0.02606616\n",
      "Iteration 77, loss = 0.02583831\n",
      "Iteration 78, loss = 0.02514166\n",
      "Iteration 79, loss = 0.02457508\n",
      "Iteration 80, loss = 0.02432411\n",
      "Iteration 81, loss = 0.02383103\n",
      "Iteration 82, loss = 0.02335414\n",
      "Iteration 83, loss = 0.02297325\n",
      "Iteration 84, loss = 0.02249178\n",
      "Iteration 85, loss = 0.02215501\n",
      "Iteration 86, loss = 0.02174321\n",
      "Iteration 87, loss = 0.02145271\n",
      "Iteration 88, loss = 0.02134643\n",
      "Iteration 89, loss = 0.02098871\n",
      "Iteration 90, loss = 0.02070276\n",
      "Iteration 91, loss = 0.02040423\n",
      "Iteration 92, loss = 0.02018695\n",
      "Iteration 93, loss = 0.01988684\n",
      "Iteration 94, loss = 0.01960067\n",
      "Iteration 95, loss = 0.01939850\n",
      "Iteration 96, loss = 0.01916453\n",
      "Iteration 97, loss = 0.01889823\n",
      "Iteration 98, loss = 0.01864152\n",
      "Iteration 99, loss = 0.01843808\n",
      "Iteration 100, loss = 0.02321267\n",
      "Iteration 101, loss = 0.01890114\n",
      "Iteration 102, loss = 0.01849926\n",
      "Iteration 103, loss = 0.01824842\n",
      "Iteration 104, loss = 0.01807543\n",
      "Iteration 105, loss = 0.01791936\n",
      "Iteration 106, loss = 0.01771472\n",
      "Iteration 107, loss = 0.01757694\n",
      "Iteration 108, loss = 0.01748883\n",
      "Iteration 109, loss = 0.01723841\n",
      "Iteration 110, loss = 0.01713766\n",
      "Iteration 111, loss = 0.01701604\n",
      "Iteration 112, loss = 0.01696835\n",
      "Iteration 113, loss = 0.01684452\n",
      "Iteration 114, loss = 0.01679095\n",
      "Iteration 115, loss = 0.01672466\n",
      "Iteration 116, loss = 0.01665486\n",
      "Iteration 117, loss = 0.01657806\n",
      "Iteration 118, loss = 0.01652530\n",
      "Iteration 119, loss = 0.01651408\n",
      "Iteration 120, loss = 0.01644760\n",
      "Iteration 121, loss = 0.01635750\n",
      "Iteration 122, loss = 0.01629967\n",
      "Iteration 123, loss = 0.01623161\n",
      "Iteration 124, loss = 0.01616929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64325280\n",
      "Iteration 2, loss = 0.63357656\n",
      "Iteration 3, loss = 0.62655682\n",
      "Iteration 4, loss = 0.60808162\n",
      "Iteration 5, loss = 0.56568823\n",
      "Iteration 6, loss = 0.49628794\n",
      "Iteration 7, loss = 0.41217475\n",
      "Iteration 8, loss = 0.33226109\n",
      "Iteration 9, loss = 0.26555728\n",
      "Iteration 10, loss = 0.21758495\n",
      "Iteration 11, loss = 0.18252439\n",
      "Iteration 12, loss = 0.15664060\n",
      "Iteration 13, loss = 0.13992531\n",
      "Iteration 14, loss = 0.12546728\n",
      "Iteration 15, loss = 0.11430971\n",
      "Iteration 16, loss = 0.10492066\n",
      "Iteration 17, loss = 0.09733776\n",
      "Iteration 18, loss = 0.09089956\n",
      "Iteration 19, loss = 0.08547223\n",
      "Iteration 20, loss = 0.08075190\n",
      "Iteration 21, loss = 0.07665530\n",
      "Iteration 22, loss = 0.07300948\n",
      "Iteration 23, loss = 0.06978389\n",
      "Iteration 24, loss = 0.06694093\n",
      "Iteration 25, loss = 0.06434738\n",
      "Iteration 26, loss = 0.06197895\n",
      "Iteration 27, loss = 0.05981078\n",
      "Iteration 28, loss = 0.05778381\n",
      "Iteration 29, loss = 0.05606724\n",
      "Iteration 30, loss = 0.05425340\n",
      "Iteration 31, loss = 0.05263786\n",
      "Iteration 32, loss = 0.05207373\n",
      "Iteration 33, loss = 0.04965714\n",
      "Iteration 34, loss = 0.04821466\n",
      "Iteration 35, loss = 0.04695205\n",
      "Iteration 36, loss = 0.04577508\n",
      "Iteration 37, loss = 0.04459735\n",
      "Iteration 38, loss = 0.04340261\n",
      "Iteration 39, loss = 0.04238647\n",
      "Iteration 40, loss = 0.04136729\n",
      "Iteration 41, loss = 0.04017327\n",
      "Iteration 42, loss = 0.03934990\n",
      "Iteration 43, loss = 0.03834158\n",
      "Iteration 44, loss = 0.03736297\n",
      "Iteration 45, loss = 0.03665832\n",
      "Iteration 46, loss = 0.03566048\n",
      "Iteration 47, loss = 0.03491832\n",
      "Iteration 48, loss = 0.03423957\n",
      "Iteration 49, loss = 0.03348466\n",
      "Iteration 50, loss = 0.03290189\n",
      "Iteration 51, loss = 0.03231306\n",
      "Iteration 52, loss = 0.03162688\n",
      "Iteration 53, loss = 0.03103621\n",
      "Iteration 54, loss = 0.03057768\n",
      "Iteration 55, loss = 0.02994608\n",
      "Iteration 56, loss = 0.02941741\n",
      "Iteration 57, loss = 0.02887441\n",
      "Iteration 58, loss = 0.02842607\n",
      "Iteration 59, loss = 0.02786973\n",
      "Iteration 60, loss = 0.02733152\n",
      "Iteration 61, loss = 0.02681849\n",
      "Iteration 62, loss = 0.02655179\n",
      "Iteration 63, loss = 0.02573661\n",
      "Iteration 64, loss = 0.02542693\n",
      "Iteration 65, loss = 0.02473318\n",
      "Iteration 66, loss = 0.02409023\n",
      "Iteration 67, loss = 0.02354662\n",
      "Iteration 68, loss = 0.02302909\n",
      "Iteration 69, loss = 0.02243435\n",
      "Iteration 70, loss = 0.02184975\n",
      "Iteration 71, loss = 0.02137975\n",
      "Iteration 72, loss = 0.02075738\n",
      "Iteration 73, loss = 0.02030952\n",
      "Iteration 74, loss = 0.01996202\n",
      "Iteration 75, loss = 0.01949111\n",
      "Iteration 76, loss = 0.01895048\n",
      "Iteration 77, loss = 0.01859300\n",
      "Iteration 78, loss = 0.01801661\n",
      "Iteration 79, loss = 0.01762176\n",
      "Iteration 80, loss = 0.01725040\n",
      "Iteration 81, loss = 0.01697209\n",
      "Iteration 82, loss = 0.01654469\n",
      "Iteration 83, loss = 0.01617418\n",
      "Iteration 84, loss = 0.01587727\n",
      "Iteration 85, loss = 0.01550984\n",
      "Iteration 86, loss = 0.01536095\n",
      "Iteration 87, loss = 0.01500895\n",
      "Iteration 88, loss = 0.01476140\n",
      "Iteration 89, loss = 0.01460700\n",
      "Iteration 90, loss = 0.01423151\n",
      "Iteration 91, loss = 0.01408142\n",
      "Iteration 92, loss = 0.01396290\n",
      "Iteration 93, loss = 0.01348267\n",
      "Iteration 94, loss = 0.01342600\n",
      "Iteration 95, loss = 0.01315288\n",
      "Iteration 96, loss = 0.01298057\n",
      "Iteration 97, loss = 0.01276510\n",
      "Iteration 98, loss = 0.01259543\n",
      "Iteration 99, loss = 0.01237476\n",
      "Iteration 100, loss = 0.01231256\n",
      "Iteration 101, loss = 0.01199759\n",
      "Iteration 102, loss = 0.01194825\n",
      "Iteration 103, loss = 0.01164390\n",
      "Iteration 104, loss = 0.01155894\n",
      "Iteration 105, loss = 0.01132319\n",
      "Iteration 106, loss = 0.01123678\n",
      "Iteration 107, loss = 0.01101568\n",
      "Iteration 108, loss = 0.01087237\n",
      "Iteration 109, loss = 0.01077317\n",
      "Iteration 110, loss = 0.01068816\n",
      "Iteration 111, loss = 0.01054817\n",
      "Iteration 112, loss = 0.01044060\n",
      "Iteration 113, loss = 0.01032922\n",
      "Iteration 114, loss = 0.01032301\n",
      "Iteration 115, loss = 0.01021724\n",
      "Iteration 116, loss = 0.01008539\n",
      "Iteration 117, loss = 0.01002503\n",
      "Iteration 118, loss = 0.00989605\n",
      "Iteration 119, loss = 0.00970967\n",
      "Iteration 120, loss = 0.00959451\n",
      "Iteration 121, loss = 0.00943090\n",
      "Iteration 122, loss = 0.00924944\n",
      "Iteration 123, loss = 0.00910708\n",
      "Iteration 124, loss = 0.00891383\n",
      "Iteration 125, loss = 0.00872325\n",
      "Iteration 126, loss = 0.00850054\n",
      "Iteration 127, loss = 0.00833508\n",
      "Iteration 128, loss = 0.00821687\n",
      "Iteration 129, loss = 0.00816885\n",
      "Iteration 130, loss = 0.00808899\n",
      "Iteration 131, loss = 0.00798067\n",
      "Iteration 132, loss = 0.00800670\n",
      "Iteration 133, loss = 0.00796184\n",
      "Iteration 134, loss = 0.00787111\n",
      "Iteration 135, loss = 0.00782967\n",
      "Iteration 136, loss = 0.00772717\n",
      "Iteration 137, loss = 0.00763553\n",
      "Iteration 138, loss = 0.00743908\n",
      "Iteration 139, loss = 0.00725173\n",
      "Iteration 140, loss = 0.00719812\n",
      "Iteration 141, loss = 0.00717829\n",
      "Iteration 142, loss = 0.00711864\n",
      "Iteration 143, loss = 0.00704297\n",
      "Iteration 144, loss = 0.00701076\n",
      "Iteration 145, loss = 0.00695722\n",
      "Iteration 146, loss = 0.00686249\n",
      "Iteration 147, loss = 0.00678331\n",
      "Iteration 148, loss = 0.00659488\n",
      "Iteration 149, loss = 0.00637395\n",
      "Iteration 150, loss = 0.00606935\n",
      "Iteration 151, loss = 0.00599591\n",
      "Iteration 152, loss = 0.00594431\n",
      "Iteration 153, loss = 0.00583549\n",
      "Iteration 154, loss = 0.00589168\n",
      "Iteration 155, loss = 0.00575184\n",
      "Iteration 156, loss = 0.00557926\n",
      "Iteration 157, loss = 0.00547187\n",
      "Iteration 158, loss = 0.00538325\n",
      "Iteration 159, loss = 0.00536248\n",
      "Iteration 160, loss = 0.00538011\n",
      "Iteration 161, loss = 0.00532180\n",
      "Iteration 162, loss = 0.00526918\n",
      "Iteration 163, loss = 0.00524759\n",
      "Iteration 164, loss = 0.00522236\n",
      "Iteration 165, loss = 0.00520638\n",
      "Iteration 166, loss = 0.00520037\n",
      "Iteration 167, loss = 0.00516015\n",
      "Iteration 168, loss = 0.00514553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68317539\n",
      "Iteration 2, loss = 0.64668932\n",
      "Iteration 3, loss = 0.63424909\n",
      "Iteration 4, loss = 0.62572715\n",
      "Iteration 5, loss = 0.60727004\n",
      "Iteration 6, loss = 0.56805481\n",
      "Iteration 7, loss = 0.50411573\n",
      "Iteration 8, loss = 0.42477664\n",
      "Iteration 9, loss = 0.34404197\n",
      "Iteration 10, loss = 0.27529802\n",
      "Iteration 11, loss = 0.22233271\n",
      "Iteration 12, loss = 0.18310794\n",
      "Iteration 13, loss = 0.15365251\n",
      "Iteration 14, loss = 0.13170859\n",
      "Iteration 15, loss = 0.11522805\n",
      "Iteration 16, loss = 0.10235581\n",
      "Iteration 17, loss = 0.09199987\n",
      "Iteration 18, loss = 0.08350885\n",
      "Iteration 19, loss = 0.07650718\n",
      "Iteration 20, loss = 0.07060260\n",
      "Iteration 21, loss = 0.06545737\n",
      "Iteration 22, loss = 0.06114935\n",
      "Iteration 23, loss = 0.05733342\n",
      "Iteration 24, loss = 0.05404165\n",
      "Iteration 25, loss = 0.05118269\n",
      "Iteration 26, loss = 0.04887455\n",
      "Iteration 27, loss = 0.04651335\n",
      "Iteration 28, loss = 0.04445063\n",
      "Iteration 29, loss = 0.04261394\n",
      "Iteration 30, loss = 0.04090824\n",
      "Iteration 31, loss = 0.03933872\n",
      "Iteration 32, loss = 0.03799918\n",
      "Iteration 33, loss = 0.03671316\n",
      "Iteration 34, loss = 0.03544842\n",
      "Iteration 35, loss = 0.03434760\n",
      "Iteration 36, loss = 0.03338003\n",
      "Iteration 37, loss = 0.03247349\n",
      "Iteration 38, loss = 0.03159205\n",
      "Iteration 39, loss = 0.03075971\n",
      "Iteration 40, loss = 0.03003510\n",
      "Iteration 41, loss = 0.02931389\n",
      "Iteration 42, loss = 0.02854966\n",
      "Iteration 43, loss = 0.02803415\n",
      "Iteration 44, loss = 0.02720774\n",
      "Iteration 45, loss = 0.02651027\n",
      "Iteration 46, loss = 0.02583618\n",
      "Iteration 47, loss = 0.02520764\n",
      "Iteration 48, loss = 0.02464533\n",
      "Iteration 49, loss = 0.02405042\n",
      "Iteration 50, loss = 0.02348528\n",
      "Iteration 51, loss = 0.02296238\n",
      "Iteration 52, loss = 0.02250523\n",
      "Iteration 53, loss = 0.02235516\n",
      "Iteration 54, loss = 0.02154011\n",
      "Iteration 55, loss = 0.02112694\n",
      "Iteration 56, loss = 0.02060155\n",
      "Iteration 57, loss = 0.02017193\n",
      "Iteration 58, loss = 0.01968099\n",
      "Iteration 59, loss = 0.01931938\n",
      "Iteration 60, loss = 0.01898605\n",
      "Iteration 61, loss = 0.01861486\n",
      "Iteration 62, loss = 0.01836261\n",
      "Iteration 63, loss = 0.01797199\n",
      "Iteration 64, loss = 0.01795525\n",
      "Iteration 65, loss = 0.01749435\n",
      "Iteration 66, loss = 0.01717187\n",
      "Iteration 67, loss = 0.01685574\n",
      "Iteration 68, loss = 0.01642856\n",
      "Iteration 69, loss = 0.01593612\n",
      "Iteration 70, loss = 0.01573035\n",
      "Iteration 71, loss = 0.01658914\n",
      "Iteration 72, loss = 0.01547325\n",
      "Iteration 73, loss = 0.01524095\n",
      "Iteration 74, loss = 0.01506327\n",
      "Iteration 75, loss = 0.01493687\n",
      "Iteration 76, loss = 0.01469289\n",
      "Iteration 77, loss = 0.01451509\n",
      "Iteration 78, loss = 0.01537320\n",
      "Iteration 79, loss = 0.01438572\n",
      "Iteration 80, loss = 0.01417452\n",
      "Iteration 81, loss = 0.01403008\n",
      "Iteration 82, loss = 0.01389064\n",
      "Iteration 83, loss = 0.01380584\n",
      "Iteration 84, loss = 0.01363166\n",
      "Iteration 85, loss = 0.01348910\n",
      "Iteration 86, loss = 0.01345457\n",
      "Iteration 87, loss = 0.01324849\n",
      "Iteration 88, loss = 0.01317159\n",
      "Iteration 89, loss = 0.01304372\n",
      "Iteration 90, loss = 0.01290349\n",
      "Iteration 91, loss = 0.01280418\n",
      "Iteration 92, loss = 0.01268855\n",
      "Iteration 93, loss = 0.01262123\n",
      "Iteration 94, loss = 0.01251461\n",
      "Iteration 95, loss = 0.01246906\n",
      "Iteration 96, loss = 0.01232778\n",
      "Iteration 97, loss = 0.01223315\n",
      "Iteration 98, loss = 0.01218651\n",
      "Iteration 99, loss = 0.01210049\n",
      "Iteration 100, loss = 0.01202257\n",
      "Iteration 101, loss = 0.01193513\n",
      "Iteration 102, loss = 0.01178255\n",
      "Iteration 103, loss = 0.01166030\n",
      "Iteration 104, loss = 0.01162332\n",
      "Iteration 105, loss = 0.01147145\n",
      "Iteration 106, loss = 0.01131128\n",
      "Iteration 107, loss = 0.01119748\n",
      "Iteration 108, loss = 0.01105137\n",
      "Iteration 109, loss = 0.01093175\n",
      "Iteration 110, loss = 0.01083263\n",
      "Iteration 111, loss = 0.01075642\n",
      "Iteration 112, loss = 0.01071663\n",
      "Iteration 113, loss = 0.01063047\n",
      "Iteration 114, loss = 0.01059348\n",
      "Iteration 115, loss = 0.01053960\n",
      "Iteration 116, loss = 0.01048194\n",
      "Iteration 117, loss = 0.01044712\n",
      "Iteration 118, loss = 0.01038205\n",
      "Iteration 119, loss = 0.01032787\n",
      "Iteration 120, loss = 0.01032784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70851442\n",
      "Iteration 2, loss = 0.65602725\n",
      "Iteration 3, loss = 0.63573134\n",
      "Iteration 4, loss = 0.62588910\n",
      "Iteration 5, loss = 0.61624536\n",
      "Iteration 6, loss = 0.60356082\n",
      "Iteration 7, loss = 0.58606997\n",
      "Iteration 8, loss = 0.56130755\n",
      "Iteration 9, loss = 0.52482798\n",
      "Iteration 10, loss = 0.47399451\n",
      "Iteration 11, loss = 0.41111907\n",
      "Iteration 12, loss = 0.34502378\n",
      "Iteration 13, loss = 0.28466579\n",
      "Iteration 14, loss = 0.23480695\n",
      "Iteration 15, loss = 0.19594881\n",
      "Iteration 16, loss = 0.16621047\n",
      "Iteration 17, loss = 0.14349204\n",
      "Iteration 18, loss = 0.12603313\n",
      "Iteration 19, loss = 0.11238350\n",
      "Iteration 20, loss = 0.10164124\n",
      "Iteration 21, loss = 0.09275116\n",
      "Iteration 22, loss = 0.08567887\n",
      "Iteration 23, loss = 0.07957292\n",
      "Iteration 24, loss = 0.07447969\n",
      "Iteration 25, loss = 0.07006099\n",
      "Iteration 26, loss = 0.06628533\n",
      "Iteration 27, loss = 0.06289691\n",
      "Iteration 28, loss = 0.05993533\n",
      "Iteration 29, loss = 0.05732006\n",
      "Iteration 30, loss = 0.05486614\n",
      "Iteration 31, loss = 0.05276599\n",
      "Iteration 32, loss = 0.05076931\n",
      "Iteration 33, loss = 0.04901973\n",
      "Iteration 34, loss = 0.04735430\n",
      "Iteration 35, loss = 0.04595438\n",
      "Iteration 36, loss = 0.04462384\n",
      "Iteration 37, loss = 0.04341469\n",
      "Iteration 38, loss = 0.04232213\n",
      "Iteration 39, loss = 0.04134733\n",
      "Iteration 40, loss = 0.04037773\n",
      "Iteration 41, loss = 0.03958421\n",
      "Iteration 42, loss = 0.03886495\n",
      "Iteration 43, loss = 0.03824883\n",
      "Iteration 44, loss = 0.03740780\n",
      "Iteration 45, loss = 0.03674825\n",
      "Iteration 46, loss = 0.03616764\n",
      "Iteration 47, loss = 0.03559343\n",
      "Iteration 48, loss = 0.03506617\n",
      "Iteration 49, loss = 0.03457692\n",
      "Iteration 50, loss = 0.03395918\n",
      "Iteration 51, loss = 0.03351291\n",
      "Iteration 52, loss = 0.03308538\n",
      "Iteration 53, loss = 0.03264367\n",
      "Iteration 54, loss = 0.03224287\n",
      "Iteration 55, loss = 0.03180291\n",
      "Iteration 56, loss = 0.03138921\n",
      "Iteration 57, loss = 0.03103764\n",
      "Iteration 58, loss = 0.03069571\n",
      "Iteration 59, loss = 0.03033555\n",
      "Iteration 60, loss = 0.03013311\n",
      "Iteration 61, loss = 0.02974102\n",
      "Iteration 62, loss = 0.02951458\n",
      "Iteration 63, loss = 0.02921378\n",
      "Iteration 64, loss = 0.02898130\n",
      "Iteration 65, loss = 0.02875656\n",
      "Iteration 66, loss = 0.02845522\n",
      "Iteration 67, loss = 0.02831144\n",
      "Iteration 68, loss = 0.02808038\n",
      "Iteration 69, loss = 0.02788311\n",
      "Iteration 70, loss = 0.02770814\n",
      "Iteration 71, loss = 0.02746002\n",
      "Iteration 72, loss = 0.02729072\n",
      "Iteration 73, loss = 0.02705017\n",
      "Iteration 74, loss = 0.02686667\n",
      "Iteration 75, loss = 0.02664015\n",
      "Iteration 76, loss = 0.02651544\n",
      "Iteration 77, loss = 0.02639679\n",
      "Iteration 78, loss = 0.02617646\n",
      "Iteration 79, loss = 0.02607065\n",
      "Iteration 80, loss = 0.02585387\n",
      "Iteration 81, loss = 0.02570617\n",
      "Iteration 82, loss = 0.02540582\n",
      "Iteration 83, loss = 0.02506376\n",
      "Iteration 84, loss = 0.02470952\n",
      "Iteration 85, loss = 0.02431138\n",
      "Iteration 86, loss = 0.02395521\n",
      "Iteration 87, loss = 0.02361056\n",
      "Iteration 88, loss = 0.02313107\n",
      "Iteration 89, loss = 0.02278755\n",
      "Iteration 90, loss = 0.02255075\n",
      "Iteration 91, loss = 0.02235539\n",
      "Iteration 92, loss = 0.02211306\n",
      "Iteration 93, loss = 0.02185465\n",
      "Iteration 94, loss = 0.02160964\n",
      "Iteration 95, loss = 0.02136326\n",
      "Iteration 96, loss = 0.02099447\n",
      "Iteration 97, loss = 0.02061199\n",
      "Iteration 98, loss = 0.02022302\n",
      "Iteration 99, loss = 0.01986708\n",
      "Iteration 100, loss = 0.01965559\n",
      "Iteration 101, loss = 0.01943171\n",
      "Iteration 102, loss = 0.01924226\n",
      "Iteration 103, loss = 0.01903962\n",
      "Iteration 104, loss = 0.01884089\n",
      "Iteration 105, loss = 0.01861917\n",
      "Iteration 106, loss = 0.01836795\n",
      "Iteration 107, loss = 0.01807751\n",
      "Iteration 108, loss = 0.01788696\n",
      "Iteration 109, loss = 0.01773484\n",
      "Iteration 110, loss = 0.01742639\n",
      "Iteration 111, loss = 0.01721312\n",
      "Iteration 112, loss = 0.01700939\n",
      "Iteration 113, loss = 0.01673855\n",
      "Iteration 114, loss = 0.01655206\n",
      "Iteration 115, loss = 0.01639740\n",
      "Iteration 116, loss = 0.01620133\n",
      "Iteration 117, loss = 0.01603178\n",
      "Iteration 118, loss = 0.01587165\n",
      "Iteration 119, loss = 0.01575390\n",
      "Iteration 120, loss = 0.01552420\n",
      "Iteration 121, loss = 0.01540855\n",
      "Iteration 122, loss = 0.01525669\n",
      "Iteration 123, loss = 0.01521185\n",
      "Iteration 124, loss = 0.01502083\n",
      "Iteration 125, loss = 0.01492880\n",
      "Iteration 126, loss = 0.01482983\n",
      "Iteration 127, loss = 0.01470113\n",
      "Iteration 128, loss = 0.01457938\n",
      "Iteration 129, loss = 0.01449434\n",
      "Iteration 130, loss = 0.01439560\n",
      "Iteration 131, loss = 0.01425756\n",
      "Iteration 132, loss = 0.01406686\n",
      "Iteration 133, loss = 0.01389363\n",
      "Iteration 134, loss = 0.01383495\n",
      "Iteration 135, loss = 0.01379471\n",
      "Iteration 136, loss = 0.01375485\n",
      "Iteration 137, loss = 0.01366542\n",
      "Iteration 138, loss = 0.01363297\n",
      "Iteration 139, loss = 0.01359457\n",
      "Iteration 140, loss = 0.01357160\n",
      "Iteration 141, loss = 0.01351749\n",
      "Iteration 142, loss = 0.01350402\n",
      "Iteration 143, loss = 0.01346571\n",
      "Iteration 144, loss = 0.01343718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63668872\n",
      "Iteration 2, loss = 0.63480604\n",
      "Iteration 3, loss = 0.62950621\n",
      "Iteration 4, loss = 0.61404186\n",
      "Iteration 5, loss = 0.57594494\n",
      "Iteration 6, loss = 0.50691160\n",
      "Iteration 7, loss = 0.41838017\n",
      "Iteration 8, loss = 0.33249959\n",
      "Iteration 9, loss = 0.26426585\n",
      "Iteration 10, loss = 0.21360829\n",
      "Iteration 11, loss = 0.17728842\n",
      "Iteration 12, loss = 0.15085536\n",
      "Iteration 13, loss = 0.13133379\n",
      "Iteration 14, loss = 0.11621056\n",
      "Iteration 15, loss = 0.10451596\n",
      "Iteration 16, loss = 0.09494574\n",
      "Iteration 17, loss = 0.08700849\n",
      "Iteration 18, loss = 0.08054418\n",
      "Iteration 19, loss = 0.07508833\n",
      "Iteration 20, loss = 0.07032646\n",
      "Iteration 21, loss = 0.06610698\n",
      "Iteration 22, loss = 0.06258868\n",
      "Iteration 23, loss = 0.05944844\n",
      "Iteration 24, loss = 0.05658278\n",
      "Iteration 25, loss = 0.05402544\n",
      "Iteration 26, loss = 0.05171135\n",
      "Iteration 27, loss = 0.04949145\n",
      "Iteration 28, loss = 0.04756521\n",
      "Iteration 29, loss = 0.04596738\n",
      "Iteration 30, loss = 0.04427824\n",
      "Iteration 31, loss = 0.04282047\n",
      "Iteration 32, loss = 0.04144423\n",
      "Iteration 33, loss = 0.04022546\n",
      "Iteration 34, loss = 0.03906524\n",
      "Iteration 35, loss = 0.03801060\n",
      "Iteration 36, loss = 0.03708975\n",
      "Iteration 37, loss = 0.03630131\n",
      "Iteration 38, loss = 0.03541556\n",
      "Iteration 39, loss = 0.03443623\n",
      "Iteration 40, loss = 0.03369128\n",
      "Iteration 41, loss = 0.03313228\n",
      "Iteration 42, loss = 0.03236620\n",
      "Iteration 43, loss = 0.03169023\n",
      "Iteration 44, loss = 0.03112435\n",
      "Iteration 45, loss = 0.03053825\n",
      "Iteration 46, loss = 0.02991953\n",
      "Iteration 47, loss = 0.02941592\n",
      "Iteration 48, loss = 0.02887024\n",
      "Iteration 49, loss = 0.02844504\n",
      "Iteration 50, loss = 0.02784589\n",
      "Iteration 51, loss = 0.02740108\n",
      "Iteration 52, loss = 0.02679286\n",
      "Iteration 53, loss = 0.02635587\n",
      "Iteration 54, loss = 0.02577318\n",
      "Iteration 55, loss = 0.02535235\n",
      "Iteration 56, loss = 0.02472832\n",
      "Iteration 57, loss = 0.02424509\n",
      "Iteration 58, loss = 0.02415321\n",
      "Iteration 59, loss = 0.02318822\n",
      "Iteration 60, loss = 0.02267834\n",
      "Iteration 61, loss = 0.02242446\n",
      "Iteration 62, loss = 0.02183986\n",
      "Iteration 63, loss = 0.02144842\n",
      "Iteration 64, loss = 0.02107368\n",
      "Iteration 65, loss = 0.02070438\n",
      "Iteration 66, loss = 0.02047305\n",
      "Iteration 67, loss = 0.02010759\n",
      "Iteration 68, loss = 0.01986711\n",
      "Iteration 69, loss = 0.01958597\n",
      "Iteration 70, loss = 0.01922391\n",
      "Iteration 71, loss = 0.01901275\n",
      "Iteration 72, loss = 0.01874426\n",
      "Iteration 73, loss = 0.01851767\n",
      "Iteration 74, loss = 0.01825139\n",
      "Iteration 75, loss = 0.01807105\n",
      "Iteration 76, loss = 0.01787011\n",
      "Iteration 77, loss = 0.01773297\n",
      "Iteration 78, loss = 0.01750590\n",
      "Iteration 79, loss = 0.01728801\n",
      "Iteration 80, loss = 0.01710680\n",
      "Iteration 81, loss = 0.01691059\n",
      "Iteration 82, loss = 0.01680448\n",
      "Iteration 83, loss = 0.01658231\n",
      "Iteration 84, loss = 0.01627191\n",
      "Iteration 85, loss = 0.01604403\n",
      "Iteration 86, loss = 0.01583436\n",
      "Iteration 87, loss = 0.01566882\n",
      "Iteration 88, loss = 0.01543820\n",
      "Iteration 89, loss = 0.01527845\n",
      "Iteration 90, loss = 0.01508122\n",
      "Iteration 91, loss = 0.01501619\n",
      "Iteration 92, loss = 0.01488882\n",
      "Iteration 93, loss = 0.01466313\n",
      "Iteration 94, loss = 0.01449524\n",
      "Iteration 95, loss = 0.01442214\n",
      "Iteration 96, loss = 0.01427582\n",
      "Iteration 97, loss = 0.01416141\n",
      "Iteration 98, loss = 0.01403683\n",
      "Iteration 99, loss = 0.01392586\n",
      "Iteration 100, loss = 0.01376876\n",
      "Iteration 101, loss = 0.01364089\n",
      "Iteration 102, loss = 0.01359855\n",
      "Iteration 103, loss = 0.01342519\n",
      "Iteration 104, loss = 0.01331118\n",
      "Iteration 105, loss = 0.01317721\n",
      "Iteration 106, loss = 0.01309873\n",
      "Iteration 107, loss = 0.01304599\n",
      "Iteration 108, loss = 0.01294762\n",
      "Iteration 109, loss = 0.01284251\n",
      "Iteration 110, loss = 0.01283378\n",
      "Iteration 111, loss = 0.01272560\n",
      "Iteration 112, loss = 0.01262093\n",
      "Iteration 113, loss = 0.01257827\n",
      "Iteration 114, loss = 0.01243886\n",
      "Iteration 115, loss = 0.01237648\n",
      "Iteration 116, loss = 0.01237361\n",
      "Iteration 117, loss = 0.01224480\n",
      "Iteration 118, loss = 0.01213008\n",
      "Iteration 119, loss = 0.01205346\n",
      "Iteration 120, loss = 0.01197268\n",
      "Iteration 121, loss = 0.01189698\n",
      "Iteration 122, loss = 0.01187210\n",
      "Iteration 123, loss = 0.01176409\n",
      "Iteration 124, loss = 0.01171536\n",
      "Iteration 125, loss = 0.01171125\n",
      "Iteration 126, loss = 0.01162971\n",
      "Iteration 127, loss = 0.01154323\n",
      "Iteration 128, loss = 0.01141792\n",
      "Iteration 129, loss = 0.01130723\n",
      "Iteration 130, loss = 0.01120011\n",
      "Iteration 131, loss = 0.01113164\n",
      "Iteration 132, loss = 0.01106916\n",
      "Iteration 133, loss = 0.01099918\n",
      "Iteration 134, loss = 0.01098058\n",
      "Iteration 135, loss = 0.01091214\n",
      "Iteration 136, loss = 0.01090248\n",
      "Iteration 137, loss = 0.01066312\n",
      "Iteration 138, loss = 0.01062131\n",
      "Iteration 139, loss = 0.01052457\n",
      "Iteration 140, loss = 0.01049010\n",
      "Iteration 141, loss = 0.01040958\n",
      "Iteration 142, loss = 0.01034584\n",
      "Iteration 143, loss = 0.01029269\n",
      "Iteration 144, loss = 0.01016840\n",
      "Iteration 145, loss = 0.01004026\n",
      "Iteration 146, loss = 0.00996325\n",
      "Iteration 147, loss = 0.00992419\n",
      "Iteration 148, loss = 0.00987555\n",
      "Iteration 149, loss = 0.00984803\n",
      "Iteration 150, loss = 0.00983406\n",
      "Iteration 151, loss = 0.00980940\n",
      "Iteration 152, loss = 0.00977967\n",
      "Iteration 153, loss = 0.00975175\n",
      "Iteration 154, loss = 0.00972416\n",
      "Iteration 155, loss = 0.00965854\n",
      "Iteration 156, loss = 0.00951177\n",
      "Iteration 157, loss = 0.00924876\n",
      "Iteration 158, loss = 0.00921442\n",
      "Iteration 159, loss = 0.00918879\n",
      "Iteration 160, loss = 0.00917758\n",
      "Iteration 161, loss = 0.00916490\n",
      "Iteration 162, loss = 0.00914144\n",
      "Iteration 163, loss = 0.00913773\n",
      "Iteration 164, loss = 0.00913474\n",
      "Iteration 165, loss = 0.00947081\n",
      "Iteration 166, loss = 0.00913185\n",
      "Iteration 167, loss = 0.00909548\n",
      "Iteration 168, loss = 0.00907469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63661886\n",
      "Iteration 2, loss = 0.63440889\n",
      "Iteration 3, loss = 0.62782901\n",
      "Iteration 4, loss = 0.61019021\n",
      "Iteration 5, loss = 0.57178794\n",
      "Iteration 6, loss = 0.50532498\n",
      "Iteration 7, loss = 0.41913190\n",
      "Iteration 8, loss = 0.33041841\n",
      "Iteration 9, loss = 0.25767095\n",
      "Iteration 10, loss = 0.20523405\n",
      "Iteration 11, loss = 0.16856630\n",
      "Iteration 12, loss = 0.14296690\n",
      "Iteration 13, loss = 0.12436647\n",
      "Iteration 14, loss = 0.11061820\n",
      "Iteration 15, loss = 0.09991230\n",
      "Iteration 16, loss = 0.09251754\n",
      "Iteration 17, loss = 0.08597755\n",
      "Iteration 18, loss = 0.08110762\n",
      "Iteration 19, loss = 0.07690114\n",
      "Iteration 20, loss = 0.07319211\n",
      "Iteration 21, loss = 0.07004052\n",
      "Iteration 22, loss = 0.06721740\n",
      "Iteration 23, loss = 0.06469048\n",
      "Iteration 24, loss = 0.06235158\n",
      "Iteration 25, loss = 0.06024541\n",
      "Iteration 26, loss = 0.05834556\n",
      "Iteration 27, loss = 0.05654944\n",
      "Iteration 28, loss = 0.05489517\n",
      "Iteration 29, loss = 0.05329759\n",
      "Iteration 30, loss = 0.05197709\n",
      "Iteration 31, loss = 0.05053443\n",
      "Iteration 32, loss = 0.04917722\n",
      "Iteration 33, loss = 0.04790104\n",
      "Iteration 34, loss = 0.04675928\n",
      "Iteration 35, loss = 0.04573311\n",
      "Iteration 36, loss = 0.04466512\n",
      "Iteration 37, loss = 0.04368951\n",
      "Iteration 38, loss = 0.04274958\n",
      "Iteration 39, loss = 0.04188208\n",
      "Iteration 40, loss = 0.04107910\n",
      "Iteration 41, loss = 0.04032551\n",
      "Iteration 42, loss = 0.03952855\n",
      "Iteration 43, loss = 0.03884892\n",
      "Iteration 44, loss = 0.03816574\n",
      "Iteration 45, loss = 0.03781630\n",
      "Iteration 46, loss = 0.03702934\n",
      "Iteration 47, loss = 0.03641436\n",
      "Iteration 48, loss = 0.03587368\n",
      "Iteration 49, loss = 0.03538657\n",
      "Iteration 50, loss = 0.03492214\n",
      "Iteration 51, loss = 0.03443430\n",
      "Iteration 52, loss = 0.03390006\n",
      "Iteration 53, loss = 0.03355520\n",
      "Iteration 54, loss = 0.03312960\n",
      "Iteration 55, loss = 0.03263325\n",
      "Iteration 56, loss = 0.03229766\n",
      "Iteration 57, loss = 0.03182684\n",
      "Iteration 58, loss = 0.03147918\n",
      "Iteration 59, loss = 0.03109453\n",
      "Iteration 60, loss = 0.03077688\n",
      "Iteration 61, loss = 0.03041958\n",
      "Iteration 62, loss = 0.02998955\n",
      "Iteration 63, loss = 0.02965388\n",
      "Iteration 64, loss = 0.02927800\n",
      "Iteration 65, loss = 0.02890771\n",
      "Iteration 66, loss = 0.02849987\n",
      "Iteration 67, loss = 0.02810970\n",
      "Iteration 68, loss = 0.02772710\n",
      "Iteration 69, loss = 0.02729643\n",
      "Iteration 70, loss = 0.02682134\n",
      "Iteration 71, loss = 0.02634243\n",
      "Iteration 72, loss = 0.02594029\n",
      "Iteration 73, loss = 0.02547656\n",
      "Iteration 74, loss = 0.02486453\n",
      "Iteration 75, loss = 0.02437345\n",
      "Iteration 76, loss = 0.02394065\n",
      "Iteration 77, loss = 0.02362045\n",
      "Iteration 78, loss = 0.02324038\n",
      "Iteration 79, loss = 0.02282704\n",
      "Iteration 80, loss = 0.02246682\n",
      "Iteration 81, loss = 0.02238480\n",
      "Iteration 82, loss = 0.02189434\n",
      "Iteration 83, loss = 0.02165168\n",
      "Iteration 84, loss = 0.02142526\n",
      "Iteration 85, loss = 0.02113664\n",
      "Iteration 86, loss = 0.02086850\n",
      "Iteration 87, loss = 0.02062635\n",
      "Iteration 88, loss = 0.02036793\n",
      "Iteration 89, loss = 0.02010474\n",
      "Iteration 90, loss = 0.01996100\n",
      "Iteration 91, loss = 0.01970572\n",
      "Iteration 92, loss = 0.01938626\n",
      "Iteration 93, loss = 0.01920925\n",
      "Iteration 94, loss = 0.01884674\n",
      "Iteration 95, loss = 0.01854252\n",
      "Iteration 96, loss = 0.01839203\n",
      "Iteration 97, loss = 0.01810672\n",
      "Iteration 98, loss = 0.01794060\n",
      "Iteration 99, loss = 0.01781855\n",
      "Iteration 100, loss = 0.01763755\n",
      "Iteration 101, loss = 0.01759224\n",
      "Iteration 102, loss = 0.01739902\n",
      "Iteration 103, loss = 0.01721212\n",
      "Iteration 104, loss = 0.01713757\n",
      "Iteration 105, loss = 0.01687422\n",
      "Iteration 106, loss = 0.01685077\n",
      "Iteration 107, loss = 0.01672632\n",
      "Iteration 108, loss = 0.01654406\n",
      "Iteration 109, loss = 0.01651972\n",
      "Iteration 110, loss = 0.01644002\n",
      "Iteration 111, loss = 0.01621084\n",
      "Iteration 112, loss = 0.01606845\n",
      "Iteration 113, loss = 0.01602631\n",
      "Iteration 114, loss = 0.01591918\n",
      "Iteration 115, loss = 0.01579644\n",
      "Iteration 116, loss = 0.01571440\n",
      "Iteration 117, loss = 0.01561515\n",
      "Iteration 118, loss = 0.01687123\n",
      "Iteration 119, loss = 0.01586710\n",
      "Iteration 120, loss = 0.01567417\n",
      "Iteration 121, loss = 0.01585540\n",
      "Iteration 122, loss = 0.01554898\n",
      "Iteration 123, loss = 0.01542564\n",
      "Iteration 124, loss = 0.01535457\n",
      "Iteration 125, loss = 0.01531768\n",
      "Iteration 126, loss = 0.01522610\n",
      "Iteration 127, loss = 0.01521696\n",
      "Iteration 128, loss = 0.01513560\n",
      "Iteration 129, loss = 0.01510734\n",
      "Iteration 130, loss = 0.01505649\n",
      "Iteration 131, loss = 0.01501650\n",
      "Iteration 132, loss = 0.01499639\n",
      "Iteration 133, loss = 0.01493118\n",
      "Iteration 134, loss = 0.01490608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77525369\n",
      "Iteration 2, loss = 0.69749779\n",
      "Iteration 3, loss = 0.65621569\n",
      "Iteration 4, loss = 0.63622684\n",
      "Iteration 5, loss = 0.62311863\n",
      "Iteration 6, loss = 0.61045491\n",
      "Iteration 7, loss = 0.59211946\n",
      "Iteration 8, loss = 0.56439287\n",
      "Iteration 9, loss = 0.52323057\n",
      "Iteration 10, loss = 0.46713143\n",
      "Iteration 11, loss = 0.39971767\n",
      "Iteration 12, loss = 0.33446416\n",
      "Iteration 13, loss = 0.27518651\n",
      "Iteration 14, loss = 0.22773747\n",
      "Iteration 15, loss = 0.19122406\n",
      "Iteration 16, loss = 0.16346468\n",
      "Iteration 17, loss = 0.14245182\n",
      "Iteration 18, loss = 0.12600210\n",
      "Iteration 19, loss = 0.11316076\n",
      "Iteration 20, loss = 0.10311802\n",
      "Iteration 21, loss = 0.09498771\n",
      "Iteration 22, loss = 0.08815045\n",
      "Iteration 23, loss = 0.08254655\n",
      "Iteration 24, loss = 0.07761035\n",
      "Iteration 25, loss = 0.07346026\n",
      "Iteration 26, loss = 0.06965809\n",
      "Iteration 27, loss = 0.06636407\n",
      "Iteration 28, loss = 0.06353968\n",
      "Iteration 29, loss = 0.06078296\n",
      "Iteration 30, loss = 0.05869738\n",
      "Iteration 31, loss = 0.05629352\n",
      "Iteration 32, loss = 0.05425860\n",
      "Iteration 33, loss = 0.05238723\n",
      "Iteration 34, loss = 0.05059278\n",
      "Iteration 35, loss = 0.04901507\n",
      "Iteration 36, loss = 0.04744136\n",
      "Iteration 37, loss = 0.04619586\n",
      "Iteration 38, loss = 0.04479743\n",
      "Iteration 39, loss = 0.04350656\n",
      "Iteration 40, loss = 0.04236003\n",
      "Iteration 41, loss = 0.04109799\n",
      "Iteration 42, loss = 0.04008179\n",
      "Iteration 43, loss = 0.03903843\n",
      "Iteration 44, loss = 0.03819769\n",
      "Iteration 45, loss = 0.03737709\n",
      "Iteration 46, loss = 0.03642415\n",
      "Iteration 47, loss = 0.03578276\n",
      "Iteration 48, loss = 0.03497063\n",
      "Iteration 49, loss = 0.03438962\n",
      "Iteration 50, loss = 0.03364477\n",
      "Iteration 51, loss = 0.03304204\n",
      "Iteration 52, loss = 0.03254629\n",
      "Iteration 53, loss = 0.03196791\n",
      "Iteration 54, loss = 0.03137011\n",
      "Iteration 55, loss = 0.03088986\n",
      "Iteration 56, loss = 0.03038531\n",
      "Iteration 57, loss = 0.02994117\n",
      "Iteration 58, loss = 0.02949334\n",
      "Iteration 59, loss = 0.02913136\n",
      "Iteration 60, loss = 0.02870756\n",
      "Iteration 61, loss = 0.02826490\n",
      "Iteration 62, loss = 0.02797082\n",
      "Iteration 63, loss = 0.02752434\n",
      "Iteration 64, loss = 0.02711987\n",
      "Iteration 65, loss = 0.02678894\n",
      "Iteration 66, loss = 0.02642207\n",
      "Iteration 67, loss = 0.02605839\n",
      "Iteration 68, loss = 0.02558783\n",
      "Iteration 69, loss = 0.02529498\n",
      "Iteration 70, loss = 0.02476611\n",
      "Iteration 71, loss = 0.02433123\n",
      "Iteration 72, loss = 0.02392764\n",
      "Iteration 73, loss = 0.02331062\n",
      "Iteration 74, loss = 0.02273890\n",
      "Iteration 75, loss = 0.02218656\n",
      "Iteration 76, loss = 0.02171008\n",
      "Iteration 77, loss = 0.02129904\n",
      "Iteration 78, loss = 0.02085919\n",
      "Iteration 79, loss = 0.02026516\n",
      "Iteration 80, loss = 0.01979431\n",
      "Iteration 81, loss = 0.01939789\n",
      "Iteration 82, loss = 0.01912539\n",
      "Iteration 83, loss = 0.01866316\n",
      "Iteration 84, loss = 0.01909343\n",
      "Iteration 85, loss = 0.01811889\n",
      "Iteration 86, loss = 0.01776561\n",
      "Iteration 87, loss = 0.01745825\n",
      "Iteration 88, loss = 0.02028309\n",
      "Iteration 89, loss = 0.01734473\n",
      "Iteration 90, loss = 0.01704230\n",
      "Iteration 91, loss = 0.01677283\n",
      "Iteration 92, loss = 0.01662444\n",
      "Iteration 93, loss = 0.01642950\n",
      "Iteration 94, loss = 0.01623455\n",
      "Iteration 95, loss = 0.01608973\n",
      "Iteration 96, loss = 0.01591980\n",
      "Iteration 97, loss = 0.01575019\n",
      "Iteration 98, loss = 0.01565575\n",
      "Iteration 99, loss = 0.01550938\n",
      "Iteration 100, loss = 0.01540069\n",
      "Iteration 101, loss = 0.01528132\n",
      "Iteration 102, loss = 0.01513732\n",
      "Iteration 103, loss = 0.01501001\n",
      "Iteration 104, loss = 0.01491456\n",
      "Iteration 105, loss = 0.01481115\n",
      "Iteration 106, loss = 0.01470789\n",
      "Iteration 107, loss = 0.01456029\n",
      "Iteration 108, loss = 0.01446448\n",
      "Iteration 109, loss = 0.01435163\n",
      "Iteration 110, loss = 0.01426037\n",
      "Iteration 111, loss = 0.01705022\n",
      "Iteration 112, loss = 0.01469925\n",
      "Iteration 113, loss = 0.01452157\n",
      "Iteration 114, loss = 0.01434933\n",
      "Iteration 115, loss = 0.01420840\n",
      "Iteration 116, loss = 0.01412672\n",
      "Iteration 117, loss = 0.01401631\n",
      "Iteration 118, loss = 0.01832528\n",
      "Iteration 119, loss = 0.01451528\n",
      "Iteration 120, loss = 0.01434157\n",
      "Iteration 121, loss = 0.01413692\n",
      "Iteration 122, loss = 0.01407140\n",
      "Iteration 123, loss = 0.01393640\n",
      "Iteration 124, loss = 0.01388862\n",
      "Iteration 125, loss = 0.01378730\n",
      "Iteration 126, loss = 0.01371230\n",
      "Iteration 127, loss = 0.01365941\n",
      "Iteration 128, loss = 0.01359839\n",
      "Iteration 129, loss = 0.01352339\n",
      "Iteration 130, loss = 0.01347271\n",
      "Iteration 131, loss = 0.01341855\n",
      "Iteration 132, loss = 0.01334445\n",
      "Iteration 133, loss = 0.01327970\n",
      "Iteration 134, loss = 0.01321447\n",
      "Iteration 135, loss = 0.01315886\n",
      "Iteration 136, loss = 0.01310383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64848156\n",
      "Iteration 2, loss = 0.63760232\n",
      "Iteration 3, loss = 0.63313888\n",
      "Iteration 4, loss = 0.62743830\n",
      "Iteration 5, loss = 0.61787615\n",
      "Iteration 6, loss = 0.60249918\n",
      "Iteration 7, loss = 0.57929871\n",
      "Iteration 8, loss = 0.54452132\n",
      "Iteration 9, loss = 0.49612169\n",
      "Iteration 10, loss = 0.43916788\n",
      "Iteration 11, loss = 0.38215685\n",
      "Iteration 12, loss = 0.33026100\n",
      "Iteration 13, loss = 0.28689611\n",
      "Iteration 14, loss = 0.25041629\n",
      "Iteration 15, loss = 0.22040159\n",
      "Iteration 16, loss = 0.19590408\n",
      "Iteration 17, loss = 0.17624298\n",
      "Iteration 18, loss = 0.15948041\n",
      "Iteration 19, loss = 0.14529435\n",
      "Iteration 20, loss = 0.13325399\n",
      "Iteration 21, loss = 0.12348263\n",
      "Iteration 22, loss = 0.11456178\n",
      "Iteration 23, loss = 0.10727107\n",
      "Iteration 24, loss = 0.10077911\n",
      "Iteration 25, loss = 0.09485918\n",
      "Iteration 26, loss = 0.08972299\n",
      "Iteration 27, loss = 0.08519890\n",
      "Iteration 28, loss = 0.08098753\n",
      "Iteration 29, loss = 0.07721341\n",
      "Iteration 30, loss = 0.07373497\n",
      "Iteration 31, loss = 0.07058030\n",
      "Iteration 32, loss = 0.06767967\n",
      "Iteration 33, loss = 0.06528202\n",
      "Iteration 34, loss = 0.06258818\n",
      "Iteration 35, loss = 0.06024920\n",
      "Iteration 36, loss = 0.05807586\n",
      "Iteration 37, loss = 0.05606791\n",
      "Iteration 38, loss = 0.05416446\n",
      "Iteration 39, loss = 0.05233587\n",
      "Iteration 40, loss = 0.05065469\n",
      "Iteration 41, loss = 0.04909870\n",
      "Iteration 42, loss = 0.04759340\n",
      "Iteration 43, loss = 0.04624231\n",
      "Iteration 44, loss = 0.04493192\n",
      "Iteration 45, loss = 0.04376746\n",
      "Iteration 46, loss = 0.04260758\n",
      "Iteration 47, loss = 0.04148831\n",
      "Iteration 48, loss = 0.04053418\n",
      "Iteration 49, loss = 0.03955404\n",
      "Iteration 50, loss = 0.03861278\n",
      "Iteration 51, loss = 0.03772099\n",
      "Iteration 52, loss = 0.03690187\n",
      "Iteration 53, loss = 0.03614357\n",
      "Iteration 54, loss = 0.03532364\n",
      "Iteration 55, loss = 0.03458971\n",
      "Iteration 56, loss = 0.03399568\n",
      "Iteration 57, loss = 0.03332914\n",
      "Iteration 58, loss = 0.03258341\n",
      "Iteration 59, loss = 0.03190550\n",
      "Iteration 60, loss = 0.03126793\n",
      "Iteration 61, loss = 0.03067340\n",
      "Iteration 62, loss = 0.03015260\n",
      "Iteration 63, loss = 0.02955597\n",
      "Iteration 64, loss = 0.02903922\n",
      "Iteration 65, loss = 0.02855282\n",
      "Iteration 66, loss = 0.02811541\n",
      "Iteration 67, loss = 0.02774183\n",
      "Iteration 68, loss = 0.02738762\n",
      "Iteration 69, loss = 0.02704399\n",
      "Iteration 70, loss = 0.02671668\n",
      "Iteration 71, loss = 0.02640674\n",
      "Iteration 72, loss = 0.02609736\n",
      "Iteration 73, loss = 0.02583655\n",
      "Iteration 74, loss = 0.02545639\n",
      "Iteration 75, loss = 0.02510560\n",
      "Iteration 76, loss = 0.02462663\n",
      "Iteration 77, loss = 0.02432152\n",
      "Iteration 78, loss = 0.02387062\n",
      "Iteration 79, loss = 0.02351160\n",
      "Iteration 80, loss = 0.02308098\n",
      "Iteration 81, loss = 0.02270182\n",
      "Iteration 82, loss = 0.02248634\n",
      "Iteration 83, loss = 0.02202158\n",
      "Iteration 84, loss = 0.02168353\n",
      "Iteration 85, loss = 0.02137621\n",
      "Iteration 86, loss = 0.02100965\n",
      "Iteration 87, loss = 0.02075636\n",
      "Iteration 88, loss = 0.02054818\n",
      "Iteration 89, loss = 0.02030422\n",
      "Iteration 90, loss = 0.02009286\n",
      "Iteration 91, loss = 0.01986624\n",
      "Iteration 92, loss = 0.02054730\n",
      "Iteration 93, loss = 0.01961879\n",
      "Iteration 94, loss = 0.01934054\n",
      "Iteration 95, loss = 0.01894587\n",
      "Iteration 96, loss = 0.01855859\n",
      "Iteration 97, loss = 0.01826111\n",
      "Iteration 98, loss = 0.01806344\n",
      "Iteration 99, loss = 0.01781987\n",
      "Iteration 100, loss = 0.01757361\n",
      "Iteration 101, loss = 0.01739216\n",
      "Iteration 102, loss = 0.01715945\n",
      "Iteration 103, loss = 0.01680745\n",
      "Iteration 104, loss = 0.01649373\n",
      "Iteration 105, loss = 0.01630713\n",
      "Iteration 106, loss = 0.01598907\n",
      "Iteration 107, loss = 0.01577546\n",
      "Iteration 108, loss = 0.01563320\n",
      "Iteration 109, loss = 0.01542703\n",
      "Iteration 110, loss = 0.01529850\n",
      "Iteration 111, loss = 0.01516196\n",
      "Iteration 112, loss = 0.01500250\n",
      "Iteration 113, loss = 0.01484882\n",
      "Iteration 114, loss = 0.01472458\n",
      "Iteration 115, loss = 0.01461792\n",
      "Iteration 116, loss = 0.01447604\n",
      "Iteration 117, loss = 0.01439758\n",
      "Iteration 118, loss = 0.01425206\n",
      "Iteration 119, loss = 0.01416335\n",
      "Iteration 120, loss = 0.01404176\n",
      "Iteration 121, loss = 0.01401340\n",
      "Iteration 122, loss = 0.01385222\n",
      "Iteration 123, loss = 0.01369613\n",
      "Iteration 124, loss = 0.01359325\n",
      "Iteration 125, loss = 0.01345066\n",
      "Iteration 126, loss = 0.01329708\n",
      "Iteration 127, loss = 0.01318673\n",
      "Iteration 128, loss = 0.01305042\n",
      "Iteration 129, loss = 0.01293321\n",
      "Iteration 130, loss = 0.01279373\n",
      "Iteration 131, loss = 0.01266890\n",
      "Iteration 132, loss = 0.01257610\n",
      "Iteration 133, loss = 0.01251392\n",
      "Iteration 134, loss = 0.01240052\n",
      "Iteration 135, loss = 0.01230430\n",
      "Iteration 136, loss = 0.01216979\n",
      "Iteration 137, loss = 0.01209622\n",
      "Iteration 138, loss = 0.01197884\n",
      "Iteration 139, loss = 0.01187708\n",
      "Iteration 140, loss = 0.01181705\n",
      "Iteration 141, loss = 0.01176889\n",
      "Iteration 142, loss = 0.01171554\n",
      "Iteration 143, loss = 0.01167869\n",
      "Iteration 144, loss = 0.01162239\n",
      "Iteration 145, loss = 0.01154549\n",
      "Iteration 146, loss = 0.01150048\n",
      "Iteration 147, loss = 0.01140603\n",
      "Iteration 148, loss = 0.01135490\n",
      "Iteration 149, loss = 0.01119460\n",
      "Iteration 150, loss = 0.01116729\n",
      "Iteration 151, loss = 0.01105536\n",
      "Iteration 152, loss = 0.01097791\n",
      "Iteration 153, loss = 0.01090063\n",
      "Iteration 154, loss = 0.01079152\n",
      "Iteration 155, loss = 0.01072078\n",
      "Iteration 156, loss = 0.01065546\n",
      "Iteration 157, loss = 0.01057183\n",
      "Iteration 158, loss = 0.01051987\n",
      "Iteration 159, loss = 0.01048459\n",
      "Iteration 160, loss = 0.01045801\n",
      "Iteration 161, loss = 0.01042780\n",
      "Iteration 162, loss = 0.01038921\n",
      "Iteration 163, loss = 0.01036436\n",
      "Iteration 164, loss = 0.01034759\n",
      "Iteration 165, loss = 0.01031973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60068624\n",
      "Iteration 2, loss = 0.28744215\n",
      "Iteration 3, loss = 0.13865909\n",
      "Iteration 4, loss = 0.08345961\n",
      "Iteration 5, loss = 0.06158128\n",
      "Iteration 6, loss = 0.05170201\n",
      "Iteration 7, loss = 0.04552510\n",
      "Iteration 8, loss = 0.04116028\n",
      "Iteration 9, loss = 0.03776242\n",
      "Iteration 10, loss = 0.03511802\n",
      "Iteration 11, loss = 0.03275850\n",
      "Iteration 12, loss = 0.03050661\n",
      "Iteration 13, loss = 0.02902885\n",
      "Iteration 14, loss = 0.02736949\n",
      "Iteration 15, loss = 0.02641364\n",
      "Iteration 16, loss = 0.02461612\n",
      "Iteration 17, loss = 0.02331725\n",
      "Iteration 18, loss = 0.02220848\n",
      "Iteration 19, loss = 0.02110778\n",
      "Iteration 20, loss = 0.01997481\n",
      "Iteration 21, loss = 0.01934402\n",
      "Iteration 22, loss = 0.01857664\n",
      "Iteration 23, loss = 0.01774909\n",
      "Iteration 24, loss = 0.01685784\n",
      "Iteration 25, loss = 0.01629946\n",
      "Iteration 26, loss = 0.01558979\n",
      "Iteration 27, loss = 0.01463978\n",
      "Iteration 28, loss = 0.01383869\n",
      "Iteration 29, loss = 0.01345227\n",
      "Iteration 30, loss = 0.01305120\n",
      "Iteration 31, loss = 0.01226701\n",
      "Iteration 32, loss = 0.01240901\n",
      "Iteration 33, loss = 0.01159215\n",
      "Iteration 34, loss = 0.01109580\n",
      "Iteration 35, loss = 0.01045160\n",
      "Iteration 36, loss = 0.01002479\n",
      "Iteration 37, loss = 0.00979808\n",
      "Iteration 38, loss = 0.00923949\n",
      "Iteration 39, loss = 0.00877269\n",
      "Iteration 40, loss = 0.00833299\n",
      "Iteration 41, loss = 0.00791850\n",
      "Iteration 42, loss = 0.00746694\n",
      "Iteration 43, loss = 0.00706514\n",
      "Iteration 44, loss = 0.00676041\n",
      "Iteration 45, loss = 0.00623081\n",
      "Iteration 46, loss = 0.00570247\n",
      "Iteration 47, loss = 0.00605168\n",
      "Iteration 48, loss = 0.00570485\n",
      "Iteration 49, loss = 0.00477567\n",
      "Iteration 50, loss = 0.00449274\n",
      "Iteration 51, loss = 0.00408579\n",
      "Iteration 52, loss = 0.00400595\n",
      "Iteration 53, loss = 0.00348216\n",
      "Iteration 54, loss = 0.00339453\n",
      "Iteration 55, loss = 0.00330532\n",
      "Iteration 56, loss = 0.00296924\n",
      "Iteration 57, loss = 0.00282779\n",
      "Iteration 58, loss = 0.00263160\n",
      "Iteration 59, loss = 0.00256319\n",
      "Iteration 60, loss = 0.00192614\n",
      "Iteration 61, loss = 0.00160606\n",
      "Iteration 62, loss = 0.00145794\n",
      "Iteration 63, loss = 0.00119747\n",
      "Iteration 64, loss = 0.00112116\n",
      "Iteration 65, loss = 0.00098597\n",
      "Iteration 66, loss = 0.00083864\n",
      "Iteration 67, loss = 0.00075800\n",
      "Iteration 68, loss = 0.00077222\n",
      "Iteration 69, loss = 0.00065568\n",
      "Iteration 70, loss = 0.00058467\n",
      "Iteration 71, loss = 0.00055988\n",
      "Iteration 72, loss = 0.00051956\n",
      "Iteration 73, loss = 0.00048810\n",
      "Iteration 74, loss = 0.00046753\n",
      "Iteration 75, loss = 0.00045093\n",
      "Iteration 76, loss = 0.00040535\n",
      "Iteration 77, loss = 0.00037972\n",
      "Iteration 78, loss = 0.00035651\n",
      "Iteration 79, loss = 0.00033316\n",
      "Iteration 80, loss = 0.00032016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.89161781\n",
      "Iteration 2, loss = 0.66172275\n",
      "Iteration 3, loss = 0.24013436\n",
      "Iteration 4, loss = 0.11702067\n",
      "Iteration 5, loss = 0.07850791\n",
      "Iteration 6, loss = 0.06146539\n",
      "Iteration 7, loss = 0.05536136\n",
      "Iteration 8, loss = 0.04821463\n",
      "Iteration 9, loss = 0.04321739\n",
      "Iteration 10, loss = 0.03851960\n",
      "Iteration 11, loss = 0.03490946\n",
      "Iteration 12, loss = 0.03187453\n",
      "Iteration 13, loss = 0.02908331\n",
      "Iteration 14, loss = 0.02664764\n",
      "Iteration 15, loss = 0.02439493\n",
      "Iteration 16, loss = 0.02236249\n",
      "Iteration 17, loss = 0.02140319\n",
      "Iteration 18, loss = 0.01952727\n",
      "Iteration 19, loss = 0.01811925\n",
      "Iteration 20, loss = 0.01708102\n",
      "Iteration 21, loss = 0.01609222\n",
      "Iteration 22, loss = 0.01495213\n",
      "Iteration 23, loss = 0.01424095\n",
      "Iteration 24, loss = 0.01309121\n",
      "Iteration 25, loss = 0.01195002\n",
      "Iteration 26, loss = 0.01128999\n",
      "Iteration 27, loss = 0.01058652\n",
      "Iteration 28, loss = 0.00986994\n",
      "Iteration 29, loss = 0.00908207\n",
      "Iteration 30, loss = 0.00861017\n",
      "Iteration 31, loss = 0.00790155\n",
      "Iteration 32, loss = 0.00766066\n",
      "Iteration 33, loss = 0.00701894\n",
      "Iteration 34, loss = 0.00657733\n",
      "Iteration 35, loss = 0.00585968\n",
      "Iteration 36, loss = 0.01193934\n",
      "Iteration 37, loss = 0.00688870\n",
      "Iteration 38, loss = 0.00576221\n",
      "Iteration 39, loss = 0.00535142\n",
      "Iteration 40, loss = 0.00485796\n",
      "Iteration 41, loss = 0.00449713\n",
      "Iteration 42, loss = 0.00416267\n",
      "Iteration 43, loss = 0.00390615\n",
      "Iteration 44, loss = 0.00368593\n",
      "Iteration 45, loss = 0.00350198\n",
      "Iteration 46, loss = 0.00328886\n",
      "Iteration 47, loss = 0.00313949\n",
      "Iteration 48, loss = 0.00296202\n",
      "Iteration 49, loss = 0.00283330\n",
      "Iteration 50, loss = 0.00267740\n",
      "Iteration 51, loss = 0.00249238\n",
      "Iteration 52, loss = 0.00241083\n",
      "Iteration 53, loss = 0.00221872\n",
      "Iteration 54, loss = 0.00208647\n",
      "Iteration 55, loss = 0.00196886\n",
      "Iteration 56, loss = 0.00185883\n",
      "Iteration 57, loss = 0.00167963\n",
      "Iteration 58, loss = 0.00159177\n",
      "Iteration 59, loss = 0.00149827\n",
      "Iteration 60, loss = 0.00146506\n",
      "Iteration 61, loss = 0.00133018\n",
      "Iteration 62, loss = 0.00111122\n",
      "Iteration 63, loss = 0.00101021\n",
      "Iteration 64, loss = 0.00089892\n",
      "Iteration 65, loss = 0.00082210\n",
      "Iteration 66, loss = 0.00079459\n",
      "Iteration 67, loss = 0.00073433\n",
      "Iteration 68, loss = 0.00066850\n",
      "Iteration 69, loss = 0.00064795\n",
      "Iteration 70, loss = 0.00056981\n",
      "Iteration 71, loss = 0.00054759\n",
      "Iteration 72, loss = 0.00051867\n",
      "Iteration 73, loss = 0.00048623\n",
      "Iteration 74, loss = 0.00046778\n",
      "Iteration 75, loss = 0.00043210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60210373\n",
      "Iteration 2, loss = 0.36798429\n",
      "Iteration 3, loss = 0.20716455\n",
      "Iteration 4, loss = 0.12122101\n",
      "Iteration 5, loss = 0.07972776\n",
      "Iteration 6, loss = 0.06005245\n",
      "Iteration 7, loss = 0.05031719\n",
      "Iteration 8, loss = 0.04386046\n",
      "Iteration 9, loss = 0.03901951\n",
      "Iteration 10, loss = 0.03515901\n",
      "Iteration 11, loss = 0.03184349\n",
      "Iteration 12, loss = 0.02875794\n",
      "Iteration 13, loss = 0.02603660\n",
      "Iteration 14, loss = 0.02377555\n",
      "Iteration 15, loss = 0.02468246\n",
      "Iteration 16, loss = 0.02030490\n",
      "Iteration 17, loss = 0.01871432\n",
      "Iteration 18, loss = 0.01708865\n",
      "Iteration 19, loss = 0.01580421\n",
      "Iteration 20, loss = 0.01466876\n",
      "Iteration 21, loss = 0.01353912\n",
      "Iteration 22, loss = 0.01223942\n",
      "Iteration 23, loss = 0.01122537\n",
      "Iteration 24, loss = 0.02171351\n",
      "Iteration 25, loss = 0.01216404\n",
      "Iteration 26, loss = 0.01067558\n",
      "Iteration 27, loss = 0.00982063\n",
      "Iteration 28, loss = 0.00901896\n",
      "Iteration 29, loss = 0.00845124\n",
      "Iteration 30, loss = 0.00793939\n",
      "Iteration 31, loss = 0.00743557\n",
      "Iteration 32, loss = 0.00709623\n",
      "Iteration 33, loss = 0.00645722\n",
      "Iteration 34, loss = 0.00611575\n",
      "Iteration 35, loss = 0.00570924\n",
      "Iteration 36, loss = 0.00524555\n",
      "Iteration 37, loss = 0.00486391\n",
      "Iteration 38, loss = 0.00450451\n",
      "Iteration 39, loss = 0.00431464\n",
      "Iteration 40, loss = 0.00400742\n",
      "Iteration 41, loss = 0.00381421\n",
      "Iteration 42, loss = 0.00340463\n",
      "Iteration 43, loss = 0.00326965\n",
      "Iteration 44, loss = 0.00307150\n",
      "Iteration 45, loss = 0.00283285\n",
      "Iteration 46, loss = 0.00269323\n",
      "Iteration 47, loss = 0.00252334\n",
      "Iteration 48, loss = 0.00252197\n",
      "Iteration 49, loss = 0.00233089\n",
      "Iteration 50, loss = 0.00217947\n",
      "Iteration 51, loss = 0.00209827\n",
      "Iteration 52, loss = 0.00193452\n",
      "Iteration 53, loss = 0.00184356\n",
      "Iteration 54, loss = 0.00176185\n",
      "Iteration 55, loss = 0.00169580\n",
      "Iteration 56, loss = 0.00162359\n",
      "Iteration 57, loss = 0.00157127\n",
      "Iteration 58, loss = 0.00153928\n",
      "Iteration 59, loss = 0.00146667\n",
      "Iteration 60, loss = 0.00140090\n",
      "Iteration 61, loss = 0.00137658\n",
      "Iteration 62, loss = 0.00132478\n",
      "Iteration 63, loss = 0.00127464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81534040\n",
      "Iteration 2, loss = 0.74244317\n",
      "Iteration 3, loss = 0.64613182\n",
      "Iteration 4, loss = 0.54459982\n",
      "Iteration 5, loss = 0.32738048\n",
      "Iteration 6, loss = 0.11784998\n",
      "Iteration 7, loss = 0.06734360\n",
      "Iteration 8, loss = 0.05315226\n",
      "Iteration 9, loss = 0.04554695\n",
      "Iteration 10, loss = 0.04034229\n",
      "Iteration 11, loss = 0.03677187\n",
      "Iteration 12, loss = 0.03386255\n",
      "Iteration 13, loss = 0.03147084\n",
      "Iteration 14, loss = 0.03986797\n",
      "Iteration 15, loss = 0.03007472\n",
      "Iteration 16, loss = 0.04507209\n",
      "Iteration 17, loss = 0.03029794\n",
      "Iteration 18, loss = 0.02914751\n",
      "Iteration 19, loss = 0.02811847\n",
      "Iteration 20, loss = 0.02715356\n",
      "Iteration 21, loss = 0.02635310\n",
      "Iteration 22, loss = 0.02554348\n",
      "Iteration 23, loss = 0.02486103\n",
      "Iteration 24, loss = 0.02416021\n",
      "Iteration 25, loss = 0.02352150\n",
      "Iteration 26, loss = 0.02285098\n",
      "Iteration 27, loss = 0.02226346\n",
      "Iteration 28, loss = 0.02166421\n",
      "Iteration 29, loss = 0.02108650\n",
      "Iteration 30, loss = 0.02056680\n",
      "Iteration 31, loss = 0.02003908\n",
      "Iteration 32, loss = 0.01947396\n",
      "Iteration 33, loss = 0.01901862\n",
      "Iteration 34, loss = 0.01847253\n",
      "Iteration 35, loss = 0.01832965\n",
      "Iteration 36, loss = 0.01772319\n",
      "Iteration 37, loss = 0.01713901\n",
      "Iteration 38, loss = 0.01681970\n",
      "Iteration 39, loss = 0.01632409\n",
      "Iteration 40, loss = 0.01584915\n",
      "Iteration 41, loss = 0.01553507\n",
      "Iteration 42, loss = 0.01510189\n",
      "Iteration 43, loss = 0.01466598\n",
      "Iteration 44, loss = 0.01428459\n",
      "Iteration 45, loss = 0.01389440\n",
      "Iteration 46, loss = 0.01357188\n",
      "Iteration 47, loss = 0.01305947\n",
      "Iteration 48, loss = 0.01269947\n",
      "Iteration 49, loss = 0.01253850\n",
      "Iteration 50, loss = 0.01211195\n",
      "Iteration 51, loss = 0.01169833\n",
      "Iteration 52, loss = 0.01148805\n",
      "Iteration 53, loss = 0.01114048\n",
      "Iteration 54, loss = 0.01063677\n",
      "Iteration 55, loss = 0.01033247\n",
      "Iteration 56, loss = 0.01016938\n",
      "Iteration 57, loss = 0.00971627\n",
      "Iteration 58, loss = 0.00963984\n",
      "Iteration 59, loss = 0.00921351\n",
      "Iteration 60, loss = 0.00885978\n",
      "Iteration 61, loss = 0.00865078\n",
      "Iteration 62, loss = 0.00834851\n",
      "Iteration 63, loss = 0.00809346\n",
      "Iteration 64, loss = 0.00773195\n",
      "Iteration 65, loss = 0.00775911\n",
      "Iteration 66, loss = 0.00729092\n",
      "Iteration 67, loss = 0.00716124\n",
      "Iteration 68, loss = 0.00715778\n",
      "Iteration 69, loss = 0.00653007\n",
      "Iteration 70, loss = 0.00641925\n",
      "Iteration 71, loss = 0.00607856\n",
      "Iteration 72, loss = 0.00593763\n",
      "Iteration 73, loss = 0.00563237\n",
      "Iteration 74, loss = 0.00544960\n",
      "Iteration 75, loss = 0.00531116\n",
      "Iteration 76, loss = 0.00502568\n",
      "Iteration 77, loss = 0.00487446\n",
      "Iteration 78, loss = 0.00482199\n",
      "Iteration 79, loss = 0.00461194\n",
      "Iteration 80, loss = 0.00442373\n",
      "Iteration 81, loss = 0.00428383\n",
      "Iteration 82, loss = 0.00414066\n",
      "Iteration 83, loss = 0.00393874\n",
      "Iteration 84, loss = 0.00381755\n",
      "Iteration 85, loss = 0.00373632\n",
      "Iteration 86, loss = 0.00356149\n",
      "Iteration 87, loss = 0.00344992\n",
      "Iteration 88, loss = 0.00334105\n",
      "Iteration 89, loss = 0.00553380\n",
      "Iteration 90, loss = 0.00338910\n",
      "Iteration 91, loss = 0.00313232\n",
      "Iteration 92, loss = 0.00304307\n",
      "Iteration 93, loss = 0.00294909\n",
      "Iteration 94, loss = 0.00281176\n",
      "Iteration 95, loss = 0.00271622\n",
      "Iteration 96, loss = 0.00264377\n",
      "Iteration 97, loss = 0.00256471\n",
      "Iteration 98, loss = 0.00248188\n",
      "Iteration 99, loss = 0.00240492\n",
      "Iteration 100, loss = 0.00232076\n",
      "Iteration 101, loss = 0.00223786\n",
      "Iteration 102, loss = 0.00222466\n",
      "Iteration 103, loss = 0.00209488\n",
      "Iteration 104, loss = 0.00195746\n",
      "Iteration 105, loss = 0.00181863\n",
      "Iteration 106, loss = 0.00159848\n",
      "Iteration 107, loss = 0.00145078\n",
      "Iteration 108, loss = 0.00134936\n",
      "Iteration 109, loss = 0.00131243\n",
      "Iteration 110, loss = 0.00117027\n",
      "Iteration 111, loss = 0.00110334\n",
      "Iteration 112, loss = 0.00099690\n",
      "Iteration 113, loss = 0.00089047\n",
      "Iteration 114, loss = 0.00089656\n",
      "Iteration 115, loss = 0.00080315\n",
      "Iteration 116, loss = 0.00078003\n",
      "Iteration 117, loss = 0.00073577\n",
      "Iteration 118, loss = 0.00070244\n",
      "Iteration 119, loss = 0.00067175\n",
      "Iteration 120, loss = 0.00065035\n",
      "Iteration 121, loss = 0.00063126\n",
      "Iteration 122, loss = 0.00060123\n",
      "Iteration 123, loss = 0.00058303\n",
      "Iteration 124, loss = 0.00055841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.89768124\n",
      "Iteration 2, loss = 0.61323362\n",
      "Iteration 3, loss = 0.25930213\n",
      "Iteration 4, loss = 0.10497269\n",
      "Iteration 5, loss = 0.07136403\n",
      "Iteration 6, loss = 0.05634193\n",
      "Iteration 7, loss = 0.04642315\n",
      "Iteration 8, loss = 0.03929329\n",
      "Iteration 9, loss = 0.03411153\n",
      "Iteration 10, loss = 0.03027875\n",
      "Iteration 11, loss = 0.02743145\n",
      "Iteration 12, loss = 0.02507660\n",
      "Iteration 13, loss = 0.02283377\n",
      "Iteration 14, loss = 0.02137892\n",
      "Iteration 15, loss = 0.01995692\n",
      "Iteration 16, loss = 0.01841561\n",
      "Iteration 17, loss = 0.01727402\n",
      "Iteration 18, loss = 0.01992374\n",
      "Iteration 19, loss = 0.01745685\n",
      "Iteration 20, loss = 0.01647808\n",
      "Iteration 21, loss = 0.01564968\n",
      "Iteration 22, loss = 0.01442875\n",
      "Iteration 23, loss = 0.01369197\n",
      "Iteration 24, loss = 0.01293884\n",
      "Iteration 25, loss = 0.01254087\n",
      "Iteration 26, loss = 0.01199897\n",
      "Iteration 27, loss = 0.01141536\n",
      "Iteration 28, loss = 0.01068401\n",
      "Iteration 29, loss = 0.01023847\n",
      "Iteration 30, loss = 0.00975107\n",
      "Iteration 31, loss = 0.00925780\n",
      "Iteration 32, loss = 0.00900478\n",
      "Iteration 33, loss = 0.00834473\n",
      "Iteration 34, loss = 0.00780936\n",
      "Iteration 35, loss = 0.00733696\n",
      "Iteration 36, loss = 0.00703716\n",
      "Iteration 37, loss = 0.00668607\n",
      "Iteration 38, loss = 0.00626733\n",
      "Iteration 39, loss = 0.00590008\n",
      "Iteration 40, loss = 0.00546108\n",
      "Iteration 41, loss = 0.00512825\n",
      "Iteration 42, loss = 0.00479936\n",
      "Iteration 43, loss = 0.00420448\n",
      "Iteration 44, loss = 0.00396785\n",
      "Iteration 45, loss = 0.00360267\n",
      "Iteration 46, loss = 0.00331368\n",
      "Iteration 47, loss = 0.00297725\n",
      "Iteration 48, loss = 0.00271613\n",
      "Iteration 49, loss = 0.00236864\n",
      "Iteration 50, loss = 0.00218099\n",
      "Iteration 51, loss = 0.00197753\n",
      "Iteration 52, loss = 0.00187651\n",
      "Iteration 53, loss = 0.00146185\n",
      "Iteration 54, loss = 0.00124576\n",
      "Iteration 55, loss = 0.00113887\n",
      "Iteration 56, loss = 0.00100413\n",
      "Iteration 57, loss = 0.00087671\n",
      "Iteration 58, loss = 0.00077991\n",
      "Iteration 59, loss = 0.00073965\n",
      "Iteration 60, loss = 0.00066503\n",
      "Iteration 61, loss = 0.00060074\n",
      "Iteration 62, loss = 0.00056348\n",
      "Iteration 63, loss = 0.00050781\n",
      "Iteration 64, loss = 0.00046130\n",
      "Iteration 65, loss = 0.00043938\n",
      "Iteration 66, loss = 0.00040482\n",
      "Iteration 67, loss = 0.00038868\n",
      "Iteration 68, loss = 0.00036484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64092083\n",
      "Iteration 2, loss = 0.63547247\n",
      "Iteration 3, loss = 0.63327199\n",
      "Iteration 4, loss = 0.62942446\n",
      "Iteration 5, loss = 0.62273483\n",
      "Iteration 6, loss = 0.61139365\n",
      "Iteration 7, loss = 0.59282661\n",
      "Iteration 8, loss = 0.56332995\n",
      "Iteration 9, loss = 0.52033865\n",
      "Iteration 10, loss = 0.46460167\n",
      "Iteration 11, loss = 0.40134223\n",
      "Iteration 12, loss = 0.33811142\n",
      "Iteration 13, loss = 0.28110602\n",
      "Iteration 14, loss = 0.23331330\n",
      "Iteration 15, loss = 0.19518529\n",
      "Iteration 16, loss = 0.16538858\n",
      "Iteration 17, loss = 0.14238416\n",
      "Iteration 18, loss = 0.12454655\n",
      "Iteration 19, loss = 0.11059704\n",
      "Iteration 20, loss = 0.09943664\n",
      "Iteration 21, loss = 0.09054665\n",
      "Iteration 22, loss = 0.08323385\n",
      "Iteration 23, loss = 0.07725535\n",
      "Iteration 24, loss = 0.07221698\n",
      "Iteration 25, loss = 0.06792002\n",
      "Iteration 26, loss = 0.06428158\n",
      "Iteration 27, loss = 0.06106234\n",
      "Iteration 28, loss = 0.05827340\n",
      "Iteration 29, loss = 0.05581260\n",
      "Iteration 30, loss = 0.05358736\n",
      "Iteration 31, loss = 0.05159420\n",
      "Iteration 32, loss = 0.04991279\n",
      "Iteration 33, loss = 0.04820061\n",
      "Iteration 34, loss = 0.04666983\n",
      "Iteration 35, loss = 0.04541211\n",
      "Iteration 36, loss = 0.04409780\n",
      "Iteration 37, loss = 0.04296325\n",
      "Iteration 38, loss = 0.04195601\n",
      "Iteration 39, loss = 0.04107639\n",
      "Iteration 40, loss = 0.04019710\n",
      "Iteration 41, loss = 0.03932286\n",
      "Iteration 42, loss = 0.03864141\n",
      "Iteration 43, loss = 0.03786076\n",
      "Iteration 44, loss = 0.03722077\n",
      "Iteration 45, loss = 0.03662057\n",
      "Iteration 46, loss = 0.03595351\n",
      "Iteration 47, loss = 0.03543929\n",
      "Iteration 48, loss = 0.03486186\n",
      "Iteration 49, loss = 0.03442217\n",
      "Iteration 50, loss = 0.03389955\n",
      "Iteration 51, loss = 0.03337591\n",
      "Iteration 52, loss = 0.03297879\n",
      "Iteration 53, loss = 0.03265862\n",
      "Iteration 54, loss = 0.03214953\n",
      "Iteration 55, loss = 0.03177620\n",
      "Iteration 56, loss = 0.03140626\n",
      "Iteration 57, loss = 0.03099404\n",
      "Iteration 58, loss = 0.03075304\n",
      "Iteration 59, loss = 0.03032367\n",
      "Iteration 60, loss = 0.02997191\n",
      "Iteration 61, loss = 0.02968860\n",
      "Iteration 62, loss = 0.02929078\n",
      "Iteration 63, loss = 0.02895673\n",
      "Iteration 64, loss = 0.02859739\n",
      "Iteration 65, loss = 0.02832065\n",
      "Iteration 66, loss = 0.02793023\n",
      "Iteration 67, loss = 0.02778422\n",
      "Iteration 68, loss = 0.02718593\n",
      "Iteration 69, loss = 0.02681176\n",
      "Iteration 70, loss = 0.02642271\n",
      "Iteration 71, loss = 0.02604909\n",
      "Iteration 72, loss = 0.02568645\n",
      "Iteration 73, loss = 0.02518197\n",
      "Iteration 74, loss = 0.02479255\n",
      "Iteration 75, loss = 0.02433244\n",
      "Iteration 76, loss = 0.02389811\n",
      "Iteration 77, loss = 0.02355955\n",
      "Iteration 78, loss = 0.02316244\n",
      "Iteration 79, loss = 0.02285177\n",
      "Iteration 80, loss = 0.02253475\n",
      "Iteration 81, loss = 0.02228875\n",
      "Iteration 82, loss = 0.02194781\n",
      "Iteration 83, loss = 0.02172680\n",
      "Iteration 84, loss = 0.02144195\n",
      "Iteration 85, loss = 0.02121603\n",
      "Iteration 86, loss = 0.02101295\n",
      "Iteration 87, loss = 0.02075066\n",
      "Iteration 88, loss = 0.02060881\n",
      "Iteration 89, loss = 0.02037803\n",
      "Iteration 90, loss = 0.02023130\n",
      "Iteration 91, loss = 0.01992549\n",
      "Iteration 92, loss = 0.01977061\n",
      "Iteration 93, loss = 0.01954037\n",
      "Iteration 94, loss = 0.01935156\n",
      "Iteration 95, loss = 0.01925256\n",
      "Iteration 96, loss = 0.01896056\n",
      "Iteration 97, loss = 0.01880855\n",
      "Iteration 98, loss = 0.01859811\n",
      "Iteration 99, loss = 0.01842639\n",
      "Iteration 100, loss = 0.01829130\n",
      "Iteration 101, loss = 0.01816652\n",
      "Iteration 102, loss = 0.01801586\n",
      "Iteration 103, loss = 0.01780678\n",
      "Iteration 104, loss = 0.01769553\n",
      "Iteration 105, loss = 0.01756176\n",
      "Iteration 106, loss = 0.01735398\n",
      "Iteration 107, loss = 0.01721289\n",
      "Iteration 108, loss = 0.01709212\n",
      "Iteration 109, loss = 0.01692610\n",
      "Iteration 110, loss = 0.01677089\n",
      "Iteration 111, loss = 0.01665149\n",
      "Iteration 112, loss = 0.01650008\n",
      "Iteration 113, loss = 0.01642742\n",
      "Iteration 114, loss = 0.01627523\n",
      "Iteration 115, loss = 0.01617002\n",
      "Iteration 116, loss = 0.01599321\n",
      "Iteration 117, loss = 0.01584573\n",
      "Iteration 118, loss = 0.01574766\n",
      "Iteration 119, loss = 0.01562375\n",
      "Iteration 120, loss = 0.01556741\n",
      "Iteration 121, loss = 0.01544337\n",
      "Iteration 122, loss = 0.01531719\n",
      "Iteration 123, loss = 0.01525837\n",
      "Iteration 124, loss = 0.01518133\n",
      "Iteration 125, loss = 0.01505033\n",
      "Iteration 126, loss = 0.01493585\n",
      "Iteration 127, loss = 0.01487899\n",
      "Iteration 128, loss = 0.01482952\n",
      "Iteration 129, loss = 0.01477633\n",
      "Iteration 130, loss = 0.01470106\n",
      "Iteration 131, loss = 0.01461865\n",
      "Iteration 132, loss = 0.01453816\n",
      "Iteration 133, loss = 0.01449245\n",
      "Iteration 134, loss = 0.01442451\n",
      "Iteration 135, loss = 0.01437594\n",
      "Iteration 136, loss = 0.01433592\n",
      "Iteration 137, loss = 0.01427819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63655655\n",
      "Iteration 2, loss = 0.63520254\n",
      "Iteration 3, loss = 0.63368747\n",
      "Iteration 4, loss = 0.63122616\n",
      "Iteration 5, loss = 0.62691587\n",
      "Iteration 6, loss = 0.62027890\n",
      "Iteration 7, loss = 0.60964688\n",
      "Iteration 8, loss = 0.59382012\n",
      "Iteration 9, loss = 0.57094502\n",
      "Iteration 10, loss = 0.53858512\n",
      "Iteration 11, loss = 0.49592630\n",
      "Iteration 12, loss = 0.44561316\n",
      "Iteration 13, loss = 0.39125609\n",
      "Iteration 14, loss = 0.33865454\n",
      "Iteration 15, loss = 0.28996159\n",
      "Iteration 16, loss = 0.24835841\n",
      "Iteration 17, loss = 0.21350277\n",
      "Iteration 18, loss = 0.18448640\n",
      "Iteration 19, loss = 0.16377106\n",
      "Iteration 20, loss = 0.14587236\n",
      "Iteration 21, loss = 0.13094445\n",
      "Iteration 22, loss = 0.11845620\n",
      "Iteration 23, loss = 0.10804229\n",
      "Iteration 24, loss = 0.09919582\n",
      "Iteration 25, loss = 0.09170052\n",
      "Iteration 26, loss = 0.08545136\n",
      "Iteration 27, loss = 0.07966528\n",
      "Iteration 28, loss = 0.07493746\n",
      "Iteration 29, loss = 0.07083212\n",
      "Iteration 30, loss = 0.06727319\n",
      "Iteration 31, loss = 0.06408684\n",
      "Iteration 32, loss = 0.06123284\n",
      "Iteration 33, loss = 0.05872622\n",
      "Iteration 34, loss = 0.05627128\n",
      "Iteration 35, loss = 0.05411765\n",
      "Iteration 36, loss = 0.05216399\n",
      "Iteration 37, loss = 0.05044862\n",
      "Iteration 38, loss = 0.04872890\n",
      "Iteration 39, loss = 0.04715100\n",
      "Iteration 40, loss = 0.04577053\n",
      "Iteration 41, loss = 0.04443011\n",
      "Iteration 42, loss = 0.04318287\n",
      "Iteration 43, loss = 0.04200727\n",
      "Iteration 44, loss = 0.04095380\n",
      "Iteration 45, loss = 0.03996316\n",
      "Iteration 46, loss = 0.03898734\n",
      "Iteration 47, loss = 0.03813191\n",
      "Iteration 48, loss = 0.03728666\n",
      "Iteration 49, loss = 0.03656484\n",
      "Iteration 50, loss = 0.03588430\n",
      "Iteration 51, loss = 0.03524633\n",
      "Iteration 52, loss = 0.03453828\n",
      "Iteration 53, loss = 0.03390158\n",
      "Iteration 54, loss = 0.03335623\n",
      "Iteration 55, loss = 0.03290146\n",
      "Iteration 56, loss = 0.03230466\n",
      "Iteration 57, loss = 0.03182171\n",
      "Iteration 58, loss = 0.03135046\n",
      "Iteration 59, loss = 0.03089064\n",
      "Iteration 60, loss = 0.03054189\n",
      "Iteration 61, loss = 0.03002491\n",
      "Iteration 62, loss = 0.02969297\n",
      "Iteration 63, loss = 0.02920518\n",
      "Iteration 64, loss = 0.02882721\n",
      "Iteration 65, loss = 0.02847325\n",
      "Iteration 66, loss = 0.02809017\n",
      "Iteration 67, loss = 0.02771174\n",
      "Iteration 68, loss = 0.02733975\n",
      "Iteration 69, loss = 0.02704086\n",
      "Iteration 70, loss = 0.02661851\n",
      "Iteration 71, loss = 0.02618920\n",
      "Iteration 72, loss = 0.02606621\n",
      "Iteration 73, loss = 0.02552578\n",
      "Iteration 74, loss = 0.02520510\n",
      "Iteration 75, loss = 0.02490007\n",
      "Iteration 76, loss = 0.02456318\n",
      "Iteration 77, loss = 0.02428992\n",
      "Iteration 78, loss = 0.02397598\n",
      "Iteration 79, loss = 0.02359966\n",
      "Iteration 80, loss = 0.02326942\n",
      "Iteration 81, loss = 0.02293295\n",
      "Iteration 82, loss = 0.02260704\n",
      "Iteration 83, loss = 0.02226983\n",
      "Iteration 84, loss = 0.02190623\n",
      "Iteration 85, loss = 0.02157342\n",
      "Iteration 86, loss = 0.02124817\n",
      "Iteration 87, loss = 0.02100253\n",
      "Iteration 88, loss = 0.02064781\n",
      "Iteration 89, loss = 0.02042729\n",
      "Iteration 90, loss = 0.02021586\n",
      "Iteration 91, loss = 0.01991157\n",
      "Iteration 92, loss = 0.01963844\n",
      "Iteration 93, loss = 0.01941027\n",
      "Iteration 94, loss = 0.01935178\n",
      "Iteration 95, loss = 0.01910236\n",
      "Iteration 96, loss = 0.01890958\n",
      "Iteration 97, loss = 0.01870723\n",
      "Iteration 98, loss = 0.01855291\n",
      "Iteration 99, loss = 0.01837071\n",
      "Iteration 100, loss = 0.01839476\n",
      "Iteration 101, loss = 0.01807625\n",
      "Iteration 102, loss = 0.01788575\n",
      "Iteration 103, loss = 0.01774774\n",
      "Iteration 104, loss = 0.01761894\n",
      "Iteration 105, loss = 0.01744302\n",
      "Iteration 106, loss = 0.01722710\n",
      "Iteration 107, loss = 0.01706503\n",
      "Iteration 108, loss = 0.01691198\n",
      "Iteration 109, loss = 0.01667079\n",
      "Iteration 110, loss = 0.01654630\n",
      "Iteration 111, loss = 0.01634361\n",
      "Iteration 112, loss = 0.01617308\n",
      "Iteration 113, loss = 0.01606007\n",
      "Iteration 114, loss = 0.01586396\n",
      "Iteration 115, loss = 0.01575049\n",
      "Iteration 116, loss = 0.01561329\n",
      "Iteration 117, loss = 0.01550026\n",
      "Iteration 118, loss = 0.01530510\n",
      "Iteration 119, loss = 0.01522652\n",
      "Iteration 120, loss = 0.01507036\n",
      "Iteration 121, loss = 0.01497202\n",
      "Iteration 122, loss = 0.01490993\n",
      "Iteration 123, loss = 0.01478404\n",
      "Iteration 124, loss = 0.01471482\n",
      "Iteration 125, loss = 0.01462420\n",
      "Iteration 126, loss = 0.01450101\n",
      "Iteration 127, loss = 0.01443102\n",
      "Iteration 128, loss = 0.01425364\n",
      "Iteration 129, loss = 0.01417722\n",
      "Iteration 130, loss = 0.01413185\n",
      "Iteration 131, loss = 0.01400999\n",
      "Iteration 132, loss = 0.01391603\n",
      "Iteration 133, loss = 0.01385074\n",
      "Iteration 134, loss = 0.01372045\n",
      "Iteration 135, loss = 0.01364453\n",
      "Iteration 136, loss = 0.01356082\n",
      "Iteration 137, loss = 0.01350176\n",
      "Iteration 138, loss = 0.01334002\n",
      "Iteration 139, loss = 0.01326907\n",
      "Iteration 140, loss = 0.01315912\n",
      "Iteration 141, loss = 0.01307871\n",
      "Iteration 142, loss = 0.01301702\n",
      "Iteration 143, loss = 0.01288591\n",
      "Iteration 144, loss = 0.01286540\n",
      "Iteration 145, loss = 0.01280076\n",
      "Iteration 146, loss = 0.01268309\n",
      "Iteration 147, loss = 0.01258158\n",
      "Iteration 148, loss = 0.01248975\n",
      "Iteration 149, loss = 0.01238359\n",
      "Iteration 150, loss = 0.01244908\n",
      "Iteration 151, loss = 0.01220190\n",
      "Iteration 152, loss = 0.01214254\n",
      "Iteration 153, loss = 0.01205089\n",
      "Iteration 154, loss = 0.01200410\n",
      "Iteration 155, loss = 0.01193310\n",
      "Iteration 156, loss = 0.01180013\n",
      "Iteration 157, loss = 0.01171809\n",
      "Iteration 158, loss = 0.01172328\n",
      "Iteration 159, loss = 0.01160666\n",
      "Iteration 160, loss = 0.01155244\n",
      "Iteration 161, loss = 0.01148027\n",
      "Iteration 162, loss = 0.01142957\n",
      "Iteration 163, loss = 0.01137360\n",
      "Iteration 164, loss = 0.01130531\n",
      "Iteration 165, loss = 0.01122463\n",
      "Iteration 166, loss = 0.01121135\n",
      "Iteration 167, loss = 0.01110160\n",
      "Iteration 168, loss = 0.01110429\n",
      "Iteration 169, loss = 0.01103212\n",
      "Iteration 170, loss = 0.01097053\n",
      "Iteration 171, loss = 0.01089021\n",
      "Iteration 172, loss = 0.01083670\n",
      "Iteration 173, loss = 0.01080068\n",
      "Iteration 174, loss = 0.01074157\n",
      "Iteration 175, loss = 0.01066937\n",
      "Iteration 176, loss = 0.01055692\n",
      "Iteration 177, loss = 0.01041080\n",
      "Iteration 178, loss = 0.01027541\n",
      "Iteration 179, loss = 0.01014442\n",
      "Iteration 180, loss = 0.01006434\n",
      "Iteration 181, loss = 0.01002711\n",
      "Iteration 182, loss = 0.00994705\n",
      "Iteration 183, loss = 0.00987009\n",
      "Iteration 184, loss = 0.00980508\n",
      "Iteration 185, loss = 0.00973371\n",
      "Iteration 186, loss = 0.00966545\n",
      "Iteration 187, loss = 0.00962106\n",
      "Iteration 188, loss = 0.00960228\n",
      "Iteration 189, loss = 0.00956258\n",
      "Iteration 190, loss = 0.00951668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64571678\n",
      "Iteration 2, loss = 0.63677799\n",
      "Iteration 3, loss = 0.63551956\n",
      "Iteration 4, loss = 0.63376479\n",
      "Iteration 5, loss = 0.63095688\n",
      "Iteration 6, loss = 0.62587006\n",
      "Iteration 7, loss = 0.61766908\n",
      "Iteration 8, loss = 0.60457165\n",
      "Iteration 9, loss = 0.58346124\n",
      "Iteration 10, loss = 0.55171579\n",
      "Iteration 11, loss = 0.50853929\n",
      "Iteration 12, loss = 0.45532703\n",
      "Iteration 13, loss = 0.39711459\n",
      "Iteration 14, loss = 0.33944542\n",
      "Iteration 15, loss = 0.28850420\n",
      "Iteration 16, loss = 0.24480632\n",
      "Iteration 17, loss = 0.20953458\n",
      "Iteration 18, loss = 0.18141160\n",
      "Iteration 19, loss = 0.15930388\n",
      "Iteration 20, loss = 0.14153372\n",
      "Iteration 21, loss = 0.12713371\n",
      "Iteration 22, loss = 0.11531427\n",
      "Iteration 23, loss = 0.10552772\n",
      "Iteration 24, loss = 0.09737850\n",
      "Iteration 25, loss = 0.09052997\n",
      "Iteration 26, loss = 0.08450183\n",
      "Iteration 27, loss = 0.07960637\n",
      "Iteration 28, loss = 0.07518791\n",
      "Iteration 29, loss = 0.07131535\n",
      "Iteration 30, loss = 0.06796847\n",
      "Iteration 31, loss = 0.06492378\n",
      "Iteration 32, loss = 0.06233896\n",
      "Iteration 33, loss = 0.06063427\n",
      "Iteration 34, loss = 0.05865848\n",
      "Iteration 35, loss = 0.05672036\n",
      "Iteration 36, loss = 0.05495067\n",
      "Iteration 37, loss = 0.05333738\n",
      "Iteration 38, loss = 0.05173070\n",
      "Iteration 39, loss = 0.05032815\n",
      "Iteration 40, loss = 0.04899265\n",
      "Iteration 41, loss = 0.04777657\n",
      "Iteration 42, loss = 0.04656580\n",
      "Iteration 43, loss = 0.04553738\n",
      "Iteration 44, loss = 0.04452031\n",
      "Iteration 45, loss = 0.04352991\n",
      "Iteration 46, loss = 0.04257037\n",
      "Iteration 47, loss = 0.04173270\n",
      "Iteration 48, loss = 0.04096990\n",
      "Iteration 49, loss = 0.04022057\n",
      "Iteration 50, loss = 0.03947572\n",
      "Iteration 51, loss = 0.03880080\n",
      "Iteration 52, loss = 0.03810586\n",
      "Iteration 53, loss = 0.03753808\n",
      "Iteration 54, loss = 0.03691591\n",
      "Iteration 55, loss = 0.03631282\n",
      "Iteration 56, loss = 0.03584368\n",
      "Iteration 57, loss = 0.03531426\n",
      "Iteration 58, loss = 0.03474043\n",
      "Iteration 59, loss = 0.03425621\n",
      "Iteration 60, loss = 0.03382527\n",
      "Iteration 61, loss = 0.03343645\n",
      "Iteration 62, loss = 0.03292862\n",
      "Iteration 63, loss = 0.03252913\n",
      "Iteration 64, loss = 0.03214296\n",
      "Iteration 65, loss = 0.03173327\n",
      "Iteration 66, loss = 0.03143271\n",
      "Iteration 67, loss = 0.03105872\n",
      "Iteration 68, loss = 0.03061272\n",
      "Iteration 69, loss = 0.03026829\n",
      "Iteration 70, loss = 0.02990390\n",
      "Iteration 71, loss = 0.02958759\n",
      "Iteration 72, loss = 0.02919289\n",
      "Iteration 73, loss = 0.02871742\n",
      "Iteration 74, loss = 0.02836657\n",
      "Iteration 75, loss = 0.02791027\n",
      "Iteration 76, loss = 0.02749898\n",
      "Iteration 77, loss = 0.02710466\n",
      "Iteration 78, loss = 0.02667163\n",
      "Iteration 79, loss = 0.02642456\n",
      "Iteration 80, loss = 0.02601770\n",
      "Iteration 81, loss = 0.02571233\n",
      "Iteration 82, loss = 0.02532901\n",
      "Iteration 83, loss = 0.02500683\n",
      "Iteration 84, loss = 0.02461132\n",
      "Iteration 85, loss = 0.02429608\n",
      "Iteration 86, loss = 0.02391795\n",
      "Iteration 87, loss = 0.02351724\n",
      "Iteration 88, loss = 0.02326062\n",
      "Iteration 89, loss = 0.02281722\n",
      "Iteration 90, loss = 0.02256917\n",
      "Iteration 91, loss = 0.02215525\n",
      "Iteration 92, loss = 0.02202008\n",
      "Iteration 93, loss = 0.02164559\n",
      "Iteration 94, loss = 0.02138880\n",
      "Iteration 95, loss = 0.02115940\n",
      "Iteration 96, loss = 0.02096444\n",
      "Iteration 97, loss = 0.02073866\n",
      "Iteration 98, loss = 0.02052073\n",
      "Iteration 99, loss = 0.02038682\n",
      "Iteration 100, loss = 0.02019204\n",
      "Iteration 101, loss = 0.01998569\n",
      "Iteration 102, loss = 0.01982431\n",
      "Iteration 103, loss = 0.01963215\n",
      "Iteration 104, loss = 0.01946994\n",
      "Iteration 105, loss = 0.01932627\n",
      "Iteration 106, loss = 0.01913803\n",
      "Iteration 107, loss = 0.01894402\n",
      "Iteration 108, loss = 0.01881152\n",
      "Iteration 109, loss = 0.01868419\n",
      "Iteration 110, loss = 0.01850177\n",
      "Iteration 111, loss = 0.01835980\n",
      "Iteration 112, loss = 0.01824109\n",
      "Iteration 113, loss = 0.01811125\n",
      "Iteration 114, loss = 0.01802296\n",
      "Iteration 115, loss = 0.01785624\n",
      "Iteration 116, loss = 0.01774833\n",
      "Iteration 117, loss = 0.01766090\n",
      "Iteration 118, loss = 0.01745109\n",
      "Iteration 119, loss = 0.01734041\n",
      "Iteration 120, loss = 0.01723264\n",
      "Iteration 121, loss = 0.01712820\n",
      "Iteration 122, loss = 0.01696764\n",
      "Iteration 123, loss = 0.01706286\n",
      "Iteration 124, loss = 0.01683881\n",
      "Iteration 125, loss = 0.01674575\n",
      "Iteration 126, loss = 0.01664215\n",
      "Iteration 127, loss = 0.01657806\n",
      "Iteration 128, loss = 0.01649516\n",
      "Iteration 129, loss = 0.01633706\n",
      "Iteration 130, loss = 0.01628417\n",
      "Iteration 131, loss = 0.01617346\n",
      "Iteration 132, loss = 0.01605736\n",
      "Iteration 133, loss = 0.01604637\n",
      "Iteration 134, loss = 0.01594876\n",
      "Iteration 135, loss = 0.01582352\n",
      "Iteration 136, loss = 0.01573671\n",
      "Iteration 137, loss = 0.01568517\n",
      "Iteration 138, loss = 0.01561296\n",
      "Iteration 139, loss = 0.01552314\n",
      "Iteration 140, loss = 0.01548071\n",
      "Iteration 141, loss = 0.01542917\n",
      "Iteration 142, loss = 0.01532190\n",
      "Iteration 143, loss = 0.01527844\n",
      "Iteration 144, loss = 0.01521548\n",
      "Iteration 145, loss = 0.01516432\n",
      "Iteration 146, loss = 0.01507524\n",
      "Iteration 147, loss = 0.01502525\n",
      "Iteration 148, loss = 0.01502806\n",
      "Iteration 149, loss = 0.01491788\n",
      "Iteration 150, loss = 0.01485929\n",
      "Iteration 151, loss = 0.01477024\n",
      "Iteration 152, loss = 0.01470608\n",
      "Iteration 153, loss = 0.01461468\n",
      "Iteration 154, loss = 0.01454998\n",
      "Iteration 155, loss = 0.01449844\n",
      "Iteration 156, loss = 0.01443465\n",
      "Iteration 157, loss = 0.01426766\n",
      "Iteration 158, loss = 0.01420007\n",
      "Iteration 159, loss = 0.01415771\n",
      "Iteration 160, loss = 0.01407473\n",
      "Iteration 161, loss = 0.01403024\n",
      "Iteration 162, loss = 0.01396868\n",
      "Iteration 163, loss = 0.01394635\n",
      "Iteration 164, loss = 0.01391699\n",
      "Iteration 165, loss = 0.01386482\n",
      "Iteration 166, loss = 0.01385046\n",
      "Iteration 167, loss = 0.01378618\n",
      "Iteration 168, loss = 0.01370755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65236507\n",
      "Iteration 2, loss = 0.63759366\n",
      "Iteration 3, loss = 0.63424529\n",
      "Iteration 4, loss = 0.63231746\n",
      "Iteration 5, loss = 0.62926777\n",
      "Iteration 6, loss = 0.62563049\n",
      "Iteration 7, loss = 0.62082072\n",
      "Iteration 8, loss = 0.61362029\n",
      "Iteration 9, loss = 0.60434732\n",
      "Iteration 10, loss = 0.59206459\n",
      "Iteration 11, loss = 0.57546276\n",
      "Iteration 12, loss = 0.55453697\n",
      "Iteration 13, loss = 0.52824810\n",
      "Iteration 14, loss = 0.49632300\n",
      "Iteration 15, loss = 0.45933553\n",
      "Iteration 16, loss = 0.41885851\n",
      "Iteration 17, loss = 0.37715502\n",
      "Iteration 18, loss = 0.33652375\n",
      "Iteration 19, loss = 0.29816905\n",
      "Iteration 20, loss = 0.26518307\n",
      "Iteration 21, loss = 0.23627292\n",
      "Iteration 22, loss = 0.21082369\n",
      "Iteration 23, loss = 0.18902683\n",
      "Iteration 24, loss = 0.17068130\n",
      "Iteration 25, loss = 0.15475135\n",
      "Iteration 26, loss = 0.14102919\n",
      "Iteration 27, loss = 0.12934135\n",
      "Iteration 28, loss = 0.11936744\n",
      "Iteration 29, loss = 0.11082736\n",
      "Iteration 30, loss = 0.10331182\n",
      "Iteration 31, loss = 0.09673477\n",
      "Iteration 32, loss = 0.09078624\n",
      "Iteration 33, loss = 0.08572391\n",
      "Iteration 34, loss = 0.08128589\n",
      "Iteration 35, loss = 0.07704815\n",
      "Iteration 36, loss = 0.07352826\n",
      "Iteration 37, loss = 0.07031066\n",
      "Iteration 38, loss = 0.06742288\n",
      "Iteration 39, loss = 0.06470723\n",
      "Iteration 40, loss = 0.06226364\n",
      "Iteration 41, loss = 0.06005835\n",
      "Iteration 42, loss = 0.05795053\n",
      "Iteration 43, loss = 0.05608271\n",
      "Iteration 44, loss = 0.05426399\n",
      "Iteration 45, loss = 0.05256255\n",
      "Iteration 46, loss = 0.05098734\n",
      "Iteration 47, loss = 0.05002632\n",
      "Iteration 48, loss = 0.04830999\n",
      "Iteration 49, loss = 0.04799099\n",
      "Iteration 50, loss = 0.04678365\n",
      "Iteration 51, loss = 0.04578864\n",
      "Iteration 52, loss = 0.04487486\n",
      "Iteration 53, loss = 0.04401871\n",
      "Iteration 54, loss = 0.04315445\n",
      "Iteration 55, loss = 0.04229747\n",
      "Iteration 56, loss = 0.04148077\n",
      "Iteration 57, loss = 0.04071694\n",
      "Iteration 58, loss = 0.03994442\n",
      "Iteration 59, loss = 0.03926891\n",
      "Iteration 60, loss = 0.03857728\n",
      "Iteration 61, loss = 0.03794300\n",
      "Iteration 62, loss = 0.03729571\n",
      "Iteration 63, loss = 0.03666546\n",
      "Iteration 64, loss = 0.03616709\n",
      "Iteration 65, loss = 0.03553327\n",
      "Iteration 66, loss = 0.03502430\n",
      "Iteration 67, loss = 0.03446609\n",
      "Iteration 68, loss = 0.03401112\n",
      "Iteration 69, loss = 0.03362200\n",
      "Iteration 70, loss = 0.03304556\n",
      "Iteration 71, loss = 0.03255438\n",
      "Iteration 72, loss = 0.03210298\n",
      "Iteration 73, loss = 0.03166171\n",
      "Iteration 74, loss = 0.03121389\n",
      "Iteration 75, loss = 0.03084683\n",
      "Iteration 76, loss = 0.03040362\n",
      "Iteration 77, loss = 0.02999386\n",
      "Iteration 78, loss = 0.02961029\n",
      "Iteration 79, loss = 0.02921435\n",
      "Iteration 80, loss = 0.02887456\n",
      "Iteration 81, loss = 0.02846916\n",
      "Iteration 82, loss = 0.02808187\n",
      "Iteration 83, loss = 0.02764871\n",
      "Iteration 84, loss = 0.02725522\n",
      "Iteration 85, loss = 0.02684112\n",
      "Iteration 86, loss = 0.02655369\n",
      "Iteration 87, loss = 0.02601978\n",
      "Iteration 88, loss = 0.02553457\n",
      "Iteration 89, loss = 0.02512352\n",
      "Iteration 90, loss = 0.02465235\n",
      "Iteration 91, loss = 0.02419786\n",
      "Iteration 92, loss = 0.02379170\n",
      "Iteration 93, loss = 0.02343565\n",
      "Iteration 94, loss = 0.02301978\n",
      "Iteration 95, loss = 0.02267994\n",
      "Iteration 96, loss = 0.02242675\n",
      "Iteration 97, loss = 0.02205160\n",
      "Iteration 98, loss = 0.02168022\n",
      "Iteration 99, loss = 0.02140575\n",
      "Iteration 100, loss = 0.02112719\n",
      "Iteration 101, loss = 0.02078058\n",
      "Iteration 102, loss = 0.02063698\n",
      "Iteration 103, loss = 0.02023510\n",
      "Iteration 104, loss = 0.01999484\n",
      "Iteration 105, loss = 0.01972738\n",
      "Iteration 106, loss = 0.01944018\n",
      "Iteration 107, loss = 0.01905147\n",
      "Iteration 108, loss = 0.01876418\n",
      "Iteration 109, loss = 0.01856533\n",
      "Iteration 110, loss = 0.01824292\n",
      "Iteration 111, loss = 0.01798077\n",
      "Iteration 112, loss = 0.01776731\n",
      "Iteration 113, loss = 0.01749503\n",
      "Iteration 114, loss = 0.01732559\n",
      "Iteration 115, loss = 0.02013336\n",
      "Iteration 116, loss = 0.01739690\n",
      "Iteration 117, loss = 0.01709207\n",
      "Iteration 118, loss = 0.01694516\n",
      "Iteration 119, loss = 0.01684236\n",
      "Iteration 120, loss = 0.01667533\n",
      "Iteration 121, loss = 0.01657686\n",
      "Iteration 122, loss = 0.01647495\n",
      "Iteration 123, loss = 0.01637776\n",
      "Iteration 124, loss = 0.01625097\n",
      "Iteration 125, loss = 0.01614824\n",
      "Iteration 126, loss = 0.01606197\n",
      "Iteration 127, loss = 0.01596644\n",
      "Iteration 128, loss = 0.01588727\n",
      "Iteration 129, loss = 0.01576376\n",
      "Iteration 130, loss = 0.01567784\n",
      "Iteration 131, loss = 0.01560911\n",
      "Iteration 132, loss = 0.01553375\n",
      "Iteration 133, loss = 0.01542502\n",
      "Iteration 134, loss = 0.01907103\n",
      "Iteration 135, loss = 0.01672053\n",
      "Iteration 136, loss = 0.01641953\n",
      "Iteration 137, loss = 0.01592368\n",
      "Iteration 138, loss = 0.01573152\n",
      "Iteration 139, loss = 0.01563707\n",
      "Iteration 140, loss = 0.01554183\n",
      "Iteration 141, loss = 0.01547684\n",
      "Iteration 142, loss = 0.01540629\n",
      "Iteration 143, loss = 0.01534706\n",
      "Iteration 144, loss = 0.01529048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67664332\n",
      "Iteration 2, loss = 0.64845713\n",
      "Iteration 3, loss = 0.63831525\n",
      "Iteration 4, loss = 0.63631824\n",
      "Iteration 5, loss = 0.63477034\n",
      "Iteration 6, loss = 0.63249125\n",
      "Iteration 7, loss = 0.62876537\n",
      "Iteration 8, loss = 0.62279799\n",
      "Iteration 9, loss = 0.61370910\n",
      "Iteration 10, loss = 0.60056732\n",
      "Iteration 11, loss = 0.58133202\n",
      "Iteration 12, loss = 0.55432755\n",
      "Iteration 13, loss = 0.51808086\n",
      "Iteration 14, loss = 0.47377140\n",
      "Iteration 15, loss = 0.42356305\n",
      "Iteration 16, loss = 0.37130940\n",
      "Iteration 17, loss = 0.32131949\n",
      "Iteration 18, loss = 0.27546373\n",
      "Iteration 19, loss = 0.23571614\n",
      "Iteration 20, loss = 0.20280067\n",
      "Iteration 21, loss = 0.17570938\n",
      "Iteration 22, loss = 0.15364823\n",
      "Iteration 23, loss = 0.13600750\n",
      "Iteration 24, loss = 0.12152883\n",
      "Iteration 25, loss = 0.10976827\n",
      "Iteration 26, loss = 0.10007685\n",
      "Iteration 27, loss = 0.09234028\n",
      "Iteration 28, loss = 0.08532357\n",
      "Iteration 29, loss = 0.07967959\n",
      "Iteration 30, loss = 0.07475832\n",
      "Iteration 31, loss = 0.07054575\n",
      "Iteration 32, loss = 0.06828823\n",
      "Iteration 33, loss = 0.06511864\n",
      "Iteration 34, loss = 0.06231256\n",
      "Iteration 35, loss = 0.05996990\n",
      "Iteration 36, loss = 0.05791950\n",
      "Iteration 37, loss = 0.05604907\n",
      "Iteration 38, loss = 0.05436758\n",
      "Iteration 39, loss = 0.05283441\n",
      "Iteration 40, loss = 0.05138334\n",
      "Iteration 41, loss = 0.05005675\n",
      "Iteration 42, loss = 0.04881138\n",
      "Iteration 43, loss = 0.04767117\n",
      "Iteration 44, loss = 0.04660378\n",
      "Iteration 45, loss = 0.04562764\n",
      "Iteration 46, loss = 0.04472281\n",
      "Iteration 47, loss = 0.04382854\n",
      "Iteration 48, loss = 0.04300385\n",
      "Iteration 49, loss = 0.04220891\n",
      "Iteration 50, loss = 0.04146764\n",
      "Iteration 51, loss = 0.04075449\n",
      "Iteration 52, loss = 0.04006859\n",
      "Iteration 53, loss = 0.03941648\n",
      "Iteration 54, loss = 0.03883244\n",
      "Iteration 55, loss = 0.03823979\n",
      "Iteration 56, loss = 0.03766139\n",
      "Iteration 57, loss = 0.03710709\n",
      "Iteration 58, loss = 0.03660477\n",
      "Iteration 59, loss = 0.03606006\n",
      "Iteration 60, loss = 0.03563422\n",
      "Iteration 61, loss = 0.03515026\n",
      "Iteration 62, loss = 0.03468519\n",
      "Iteration 63, loss = 0.03430503\n",
      "Iteration 64, loss = 0.03386073\n",
      "Iteration 65, loss = 0.03343313\n",
      "Iteration 66, loss = 0.03312592\n",
      "Iteration 67, loss = 0.03268708\n",
      "Iteration 68, loss = 0.03232078\n",
      "Iteration 69, loss = 0.03197085\n",
      "Iteration 70, loss = 0.03161928\n",
      "Iteration 71, loss = 0.03129519\n",
      "Iteration 72, loss = 0.03096158\n",
      "Iteration 73, loss = 0.03061419\n",
      "Iteration 74, loss = 0.03032438\n",
      "Iteration 75, loss = 0.03004030\n",
      "Iteration 76, loss = 0.02966082\n",
      "Iteration 77, loss = 0.02937795\n",
      "Iteration 78, loss = 0.02908649\n",
      "Iteration 79, loss = 0.02880179\n",
      "Iteration 80, loss = 0.02845153\n",
      "Iteration 81, loss = 0.02818522\n",
      "Iteration 82, loss = 0.02786012\n",
      "Iteration 83, loss = 0.02756219\n",
      "Iteration 84, loss = 0.02730929\n",
      "Iteration 85, loss = 0.02699494\n",
      "Iteration 86, loss = 0.02675489\n",
      "Iteration 87, loss = 0.02649835\n",
      "Iteration 88, loss = 0.02621494\n",
      "Iteration 89, loss = 0.02594024\n",
      "Iteration 90, loss = 0.02571136\n",
      "Iteration 91, loss = 0.02547011\n",
      "Iteration 92, loss = 0.02520089\n",
      "Iteration 93, loss = 0.02498720\n",
      "Iteration 94, loss = 0.02473023\n",
      "Iteration 95, loss = 0.02447099\n",
      "Iteration 96, loss = 0.02420837\n",
      "Iteration 97, loss = 0.02388033\n",
      "Iteration 98, loss = 0.02367263\n",
      "Iteration 99, loss = 0.02338322\n",
      "Iteration 100, loss = 0.02311602\n",
      "Iteration 101, loss = 0.02281805\n",
      "Iteration 102, loss = 0.02248339\n",
      "Iteration 103, loss = 0.02219782\n",
      "Iteration 104, loss = 0.02188426\n",
      "Iteration 105, loss = 0.02160860\n",
      "Iteration 106, loss = 0.02138593\n",
      "Iteration 107, loss = 0.02100963\n",
      "Iteration 108, loss = 0.02070582\n",
      "Iteration 109, loss = 0.02046072\n",
      "Iteration 110, loss = 0.02007572\n",
      "Iteration 111, loss = 0.01979376\n",
      "Iteration 112, loss = 0.01942333\n",
      "Iteration 113, loss = 0.01907508\n",
      "Iteration 114, loss = 0.01878350\n",
      "Iteration 115, loss = 0.01833857\n",
      "Iteration 116, loss = 0.01806619\n",
      "Iteration 117, loss = 0.01773743\n",
      "Iteration 118, loss = 0.01751950\n",
      "Iteration 119, loss = 0.01718077\n",
      "Iteration 120, loss = 0.01688372\n",
      "Iteration 121, loss = 0.01686270\n",
      "Iteration 122, loss = 0.01648489\n",
      "Iteration 123, loss = 0.01630490\n",
      "Iteration 124, loss = 0.01609029\n",
      "Iteration 125, loss = 0.01757885\n",
      "Iteration 126, loss = 0.01609033\n",
      "Iteration 127, loss = 0.01596972\n",
      "Iteration 128, loss = 0.01588565\n",
      "Iteration 129, loss = 0.01578275\n",
      "Iteration 130, loss = 0.01568494\n",
      "Iteration 131, loss = 0.01563047\n",
      "Iteration 132, loss = 0.01552815\n",
      "Iteration 133, loss = 0.01543326\n",
      "Iteration 134, loss = 0.01537844\n",
      "Iteration 135, loss = 0.01529599\n",
      "Iteration 136, loss = 0.01521076\n",
      "Iteration 137, loss = 0.01510728\n",
      "Iteration 138, loss = 0.01505502\n",
      "Iteration 139, loss = 0.01493777\n",
      "Iteration 140, loss = 0.01484482\n",
      "Iteration 141, loss = 0.01478947\n",
      "Iteration 142, loss = 0.01471696\n",
      "Iteration 143, loss = 0.01461555\n",
      "Iteration 144, loss = 0.01451939\n",
      "Iteration 145, loss = 0.01442754\n",
      "Iteration 146, loss = 0.01440311\n",
      "Iteration 147, loss = 0.01428498\n",
      "Iteration 148, loss = 0.01420807\n",
      "Iteration 149, loss = 0.01417223\n",
      "Iteration 150, loss = 0.01407921\n",
      "Iteration 151, loss = 0.01402655\n",
      "Iteration 152, loss = 0.01397790\n",
      "Iteration 153, loss = 0.01391909\n",
      "Iteration 154, loss = 0.01385604\n",
      "Iteration 155, loss = 0.01377769\n",
      "Iteration 156, loss = 0.01370834\n",
      "Iteration 157, loss = 0.01364826\n",
      "Iteration 158, loss = 0.01359611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72061608\n",
      "Iteration 2, loss = 0.52251496\n",
      "Iteration 3, loss = 0.36495969\n",
      "Iteration 4, loss = 0.19852289\n",
      "Iteration 5, loss = 0.09392043\n",
      "Iteration 6, loss = 0.05473148\n",
      "Iteration 7, loss = 0.04548238\n",
      "Iteration 8, loss = 0.04082389\n",
      "Iteration 9, loss = 0.03757729\n",
      "Iteration 10, loss = 0.03454038\n",
      "Iteration 11, loss = 0.03259196\n",
      "Iteration 12, loss = 0.03026585\n",
      "Iteration 13, loss = 0.02874788\n",
      "Iteration 14, loss = 0.02677759\n",
      "Iteration 15, loss = 0.02531780\n",
      "Iteration 16, loss = 0.02405202\n",
      "Iteration 17, loss = 0.02277322\n",
      "Iteration 18, loss = 0.02169826\n",
      "Iteration 19, loss = 0.02063589\n",
      "Iteration 20, loss = 0.02016104\n",
      "Iteration 21, loss = 0.01921864\n",
      "Iteration 22, loss = 0.01793487\n",
      "Iteration 23, loss = 0.01722079\n",
      "Iteration 24, loss = 0.01652830\n",
      "Iteration 25, loss = 0.01587099\n",
      "Iteration 26, loss = 0.01515673\n",
      "Iteration 27, loss = 0.01459462\n",
      "Iteration 28, loss = 0.01403446\n",
      "Iteration 29, loss = 0.01362345\n",
      "Iteration 30, loss = 0.01295306\n",
      "Iteration 31, loss = 0.01254595\n",
      "Iteration 32, loss = 0.01215092\n",
      "Iteration 33, loss = 0.01175180\n",
      "Iteration 34, loss = 0.01106378\n",
      "Iteration 35, loss = 0.01075229\n",
      "Iteration 36, loss = 0.01017141\n",
      "Iteration 37, loss = 0.00977711\n",
      "Iteration 38, loss = 0.00920907\n",
      "Iteration 39, loss = 0.00895938\n",
      "Iteration 40, loss = 0.00859654\n",
      "Iteration 41, loss = 0.00801120\n",
      "Iteration 42, loss = 0.00773372\n",
      "Iteration 43, loss = 0.00726784\n",
      "Iteration 44, loss = 0.00699278\n",
      "Iteration 45, loss = 0.00659303\n",
      "Iteration 46, loss = 0.00652687\n",
      "Iteration 47, loss = 0.00614738\n",
      "Iteration 48, loss = 0.00569368\n",
      "Iteration 49, loss = 0.00529151\n",
      "Iteration 50, loss = 0.00492594\n",
      "Iteration 51, loss = 0.00471320\n",
      "Iteration 52, loss = 0.00447180\n",
      "Iteration 53, loss = 0.00425249\n",
      "Iteration 54, loss = 0.00402186\n",
      "Iteration 55, loss = 0.00391229\n",
      "Iteration 56, loss = 0.00370747\n",
      "Iteration 57, loss = 0.00331501\n",
      "Iteration 58, loss = 0.00313639\n",
      "Iteration 59, loss = 0.00274034\n",
      "Iteration 60, loss = 0.00262214\n",
      "Iteration 61, loss = 0.00239698\n",
      "Iteration 62, loss = 0.00222871\n",
      "Iteration 63, loss = 0.00209029\n",
      "Iteration 64, loss = 0.00190884\n",
      "Iteration 65, loss = 0.00174390\n",
      "Iteration 66, loss = 0.00150317\n",
      "Iteration 67, loss = 0.00126830\n",
      "Iteration 68, loss = 0.00114729\n",
      "Iteration 69, loss = 0.00115949\n",
      "Iteration 70, loss = 0.00101649\n",
      "Iteration 71, loss = 0.00100107\n",
      "Iteration 72, loss = 0.00092149\n",
      "Iteration 73, loss = 0.00087891\n",
      "Iteration 74, loss = 0.00085284\n",
      "Iteration 75, loss = 0.00080583\n",
      "Iteration 76, loss = 0.00076286\n",
      "Iteration 77, loss = 0.00073605\n",
      "Iteration 78, loss = 0.00070896\n",
      "Iteration 79, loss = 0.00066789\n",
      "Iteration 80, loss = 0.00065096\n",
      "Iteration 81, loss = 0.00060506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54529261\n",
      "Iteration 2, loss = 0.31737460\n",
      "Iteration 3, loss = 0.13169795\n",
      "Iteration 4, loss = 0.07424984\n",
      "Iteration 5, loss = 0.05630764\n",
      "Iteration 6, loss = 0.04972518\n",
      "Iteration 7, loss = 0.04410458\n",
      "Iteration 8, loss = 0.04046799\n",
      "Iteration 9, loss = 0.03781892\n",
      "Iteration 10, loss = 0.03525800\n",
      "Iteration 11, loss = 0.03322461\n",
      "Iteration 12, loss = 0.03117747\n",
      "Iteration 13, loss = 0.02970032\n",
      "Iteration 14, loss = 0.02808772\n",
      "Iteration 15, loss = 0.02644389\n",
      "Iteration 16, loss = 0.02510303\n",
      "Iteration 17, loss = 0.02377894\n",
      "Iteration 18, loss = 0.02259043\n",
      "Iteration 19, loss = 0.02165310\n",
      "Iteration 20, loss = 0.02045602\n",
      "Iteration 21, loss = 0.01940403\n",
      "Iteration 22, loss = 0.01853437\n",
      "Iteration 23, loss = 0.01769069\n",
      "Iteration 24, loss = 0.01689653\n",
      "Iteration 25, loss = 0.01610402\n",
      "Iteration 26, loss = 0.01571540\n",
      "Iteration 27, loss = 0.01489714\n",
      "Iteration 28, loss = 0.01405696\n",
      "Iteration 29, loss = 0.01327034\n",
      "Iteration 30, loss = 0.01276559\n",
      "Iteration 31, loss = 0.01243741\n",
      "Iteration 32, loss = 0.01151332\n",
      "Iteration 33, loss = 0.01127831\n",
      "Iteration 34, loss = 0.01063037\n",
      "Iteration 35, loss = 0.01032182\n",
      "Iteration 36, loss = 0.00958818\n",
      "Iteration 37, loss = 0.00940832\n",
      "Iteration 38, loss = 0.00907524\n",
      "Iteration 39, loss = 0.00849025\n",
      "Iteration 40, loss = 0.00872675\n",
      "Iteration 41, loss = 0.00802453\n",
      "Iteration 42, loss = 0.00738390\n",
      "Iteration 43, loss = 0.00679793\n",
      "Iteration 44, loss = 0.00682463\n",
      "Iteration 45, loss = 0.00621971\n",
      "Iteration 46, loss = 0.00581653\n",
      "Iteration 47, loss = 0.00568493\n",
      "Iteration 48, loss = 0.00524978\n",
      "Iteration 49, loss = 0.00496340\n",
      "Iteration 50, loss = 0.00486461\n",
      "Iteration 51, loss = 0.00442670\n",
      "Iteration 52, loss = 0.00421088\n",
      "Iteration 53, loss = 0.00378884\n",
      "Iteration 54, loss = 0.00361438\n",
      "Iteration 55, loss = 0.00337435\n",
      "Iteration 56, loss = 0.00329569\n",
      "Iteration 57, loss = 0.00322770\n",
      "Iteration 58, loss = 0.00276263\n",
      "Iteration 59, loss = 0.00256981\n",
      "Iteration 60, loss = 0.00234932\n",
      "Iteration 61, loss = 0.00221928\n",
      "Iteration 62, loss = 0.00221566\n",
      "Iteration 63, loss = 0.00192819\n",
      "Iteration 64, loss = 0.00183159\n",
      "Iteration 65, loss = 0.00177153\n",
      "Iteration 66, loss = 0.00163457\n",
      "Iteration 67, loss = 0.00157053\n",
      "Iteration 68, loss = 0.00151544\n",
      "Iteration 69, loss = 0.00146015\n",
      "Iteration 70, loss = 0.00133092\n",
      "Iteration 71, loss = 0.00128482\n",
      "Iteration 72, loss = 0.00122994\n",
      "Iteration 73, loss = 0.00116844\n",
      "Iteration 74, loss = 0.00112169\n",
      "Iteration 75, loss = 0.00105306\n",
      "Iteration 76, loss = 0.00101047\n",
      "Iteration 77, loss = 0.00100357\n",
      "Iteration 78, loss = 0.00092589\n",
      "Iteration 79, loss = 0.00087158\n",
      "Iteration 80, loss = 0.00083720\n",
      "Iteration 81, loss = 0.00086497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50733436\n",
      "Iteration 2, loss = 0.18001407\n",
      "Iteration 3, loss = 0.08808018\n",
      "Iteration 4, loss = 0.06439946\n",
      "Iteration 5, loss = 0.05397213\n",
      "Iteration 6, loss = 0.04705411\n",
      "Iteration 7, loss = 0.04226667\n",
      "Iteration 8, loss = 0.03856939\n",
      "Iteration 9, loss = 0.03528830\n",
      "Iteration 10, loss = 0.03270201\n",
      "Iteration 11, loss = 0.03040792\n",
      "Iteration 12, loss = 0.02859744\n",
      "Iteration 13, loss = 0.02637604\n",
      "Iteration 14, loss = 0.02505872\n",
      "Iteration 15, loss = 0.02357901\n",
      "Iteration 16, loss = 0.02252182\n",
      "Iteration 17, loss = 0.02124539\n",
      "Iteration 18, loss = 0.02057250\n",
      "Iteration 19, loss = 0.01884120\n",
      "Iteration 20, loss = 0.01781850\n",
      "Iteration 21, loss = 0.01680927\n",
      "Iteration 22, loss = 0.01609131\n",
      "Iteration 23, loss = 0.01544890\n",
      "Iteration 24, loss = 0.01612574\n",
      "Iteration 25, loss = 0.03116059\n",
      "Iteration 26, loss = 0.01882926\n",
      "Iteration 27, loss = 0.01688007\n",
      "Iteration 28, loss = 0.01575021\n",
      "Iteration 29, loss = 0.01495020\n",
      "Iteration 30, loss = 0.01424173\n",
      "Iteration 31, loss = 0.01371500\n",
      "Iteration 32, loss = 0.01312783\n",
      "Iteration 33, loss = 0.01297415\n",
      "Iteration 34, loss = 0.01233743\n",
      "Iteration 35, loss = 0.01198985\n",
      "Iteration 36, loss = 0.01190745\n",
      "Iteration 37, loss = 0.01130364\n",
      "Iteration 38, loss = 0.04843343\n",
      "Iteration 39, loss = 0.01611737\n",
      "Iteration 40, loss = 0.01414092\n",
      "Iteration 41, loss = 0.01297665\n",
      "Iteration 42, loss = 0.01229751\n",
      "Iteration 43, loss = 0.02037816\n",
      "Iteration 44, loss = 0.01305338\n",
      "Iteration 45, loss = 0.01195793\n",
      "Iteration 46, loss = 0.01152929\n",
      "Iteration 47, loss = 0.01095610\n",
      "Iteration 48, loss = 0.01062175\n",
      "Iteration 49, loss = 0.01031628\n",
      "Iteration 50, loss = 0.01000531\n",
      "Iteration 51, loss = 0.00983221\n",
      "Iteration 52, loss = 0.00944809\n",
      "Iteration 53, loss = 0.00923092\n",
      "Iteration 54, loss = 0.00893903\n",
      "Iteration 55, loss = 0.00883394\n",
      "Iteration 56, loss = 0.00848510\n",
      "Iteration 57, loss = 0.00841034\n",
      "Iteration 58, loss = 0.00804894\n",
      "Iteration 59, loss = 0.00792738\n",
      "Iteration 60, loss = 0.00766740\n",
      "Iteration 61, loss = 0.00759325\n",
      "Iteration 62, loss = 0.00727208\n",
      "Iteration 63, loss = 0.00705123\n",
      "Iteration 64, loss = 0.00683071\n",
      "Iteration 65, loss = 0.00678567\n",
      "Iteration 66, loss = 0.00655441\n",
      "Iteration 67, loss = 0.00623655\n",
      "Iteration 68, loss = 0.00603990\n",
      "Iteration 69, loss = 0.00576972\n",
      "Iteration 70, loss = 0.00556426\n",
      "Iteration 71, loss = 0.00540451\n",
      "Iteration 72, loss = 0.00526417\n",
      "Iteration 73, loss = 0.00489601\n",
      "Iteration 74, loss = 0.00474883\n",
      "Iteration 75, loss = 0.00451001\n",
      "Iteration 76, loss = 0.00434460\n",
      "Iteration 77, loss = 0.01235546\n",
      "Iteration 78, loss = 0.00589029\n",
      "Iteration 79, loss = 0.00472029\n",
      "Iteration 80, loss = 0.00415956\n",
      "Iteration 81, loss = 0.00387408\n",
      "Iteration 82, loss = 0.00365634\n",
      "Iteration 83, loss = 0.00345115\n",
      "Iteration 84, loss = 0.00328063\n",
      "Iteration 85, loss = 0.00311224\n",
      "Iteration 86, loss = 0.00296238\n",
      "Iteration 87, loss = 0.00280525\n",
      "Iteration 88, loss = 0.00273765\n",
      "Iteration 89, loss = 0.00261303\n",
      "Iteration 90, loss = 0.00245457\n",
      "Iteration 91, loss = 0.00230968\n",
      "Iteration 92, loss = 0.00229376\n",
      "Iteration 93, loss = 0.00206072\n",
      "Iteration 94, loss = 0.00205112\n",
      "Iteration 95, loss = 0.00197575\n",
      "Iteration 96, loss = 0.00187139\n",
      "Iteration 97, loss = 0.00180525\n",
      "Iteration 98, loss = 0.00175534\n",
      "Iteration 99, loss = 0.00165914\n",
      "Iteration 100, loss = 0.00169686\n",
      "Iteration 101, loss = 0.00155218\n",
      "Iteration 102, loss = 0.00150078\n",
      "Iteration 103, loss = 0.00143346\n",
      "Iteration 104, loss = 0.00141130\n",
      "Iteration 105, loss = 0.00134478\n",
      "Iteration 106, loss = 0.00132312\n",
      "Iteration 107, loss = 0.00124921\n",
      "Iteration 108, loss = 0.00120264\n",
      "Iteration 109, loss = 0.00116364\n",
      "Iteration 110, loss = 0.00111561\n",
      "Iteration 111, loss = 0.00108529\n",
      "Iteration 112, loss = 0.00105521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81665486\n",
      "Iteration 2, loss = 0.70862188\n",
      "Iteration 3, loss = 0.57281152\n",
      "Iteration 4, loss = 0.28162025\n",
      "Iteration 5, loss = 0.11899007\n",
      "Iteration 6, loss = 0.07651497\n",
      "Iteration 7, loss = 0.05926618\n",
      "Iteration 8, loss = 0.04989918\n",
      "Iteration 9, loss = 0.04409229\n",
      "Iteration 10, loss = 0.03951000\n",
      "Iteration 11, loss = 0.03718802\n",
      "Iteration 12, loss = 0.03427461\n",
      "Iteration 13, loss = 0.03251090\n",
      "Iteration 14, loss = 0.03002547\n",
      "Iteration 15, loss = 0.02818151\n",
      "Iteration 16, loss = 0.02594084\n",
      "Iteration 17, loss = 0.02428092\n",
      "Iteration 18, loss = 0.02222957\n",
      "Iteration 19, loss = 0.02089474\n",
      "Iteration 20, loss = 0.02002908\n",
      "Iteration 21, loss = 0.01809571\n",
      "Iteration 22, loss = 0.01687818\n",
      "Iteration 23, loss = 0.01597203\n",
      "Iteration 24, loss = 0.01523726\n",
      "Iteration 25, loss = 0.01432669\n",
      "Iteration 26, loss = 0.01347955\n",
      "Iteration 27, loss = 0.01275782\n",
      "Iteration 28, loss = 0.01199066\n",
      "Iteration 29, loss = 0.01227344\n",
      "Iteration 30, loss = 0.01065239\n",
      "Iteration 31, loss = 0.00992113\n",
      "Iteration 32, loss = 0.00966687\n",
      "Iteration 33, loss = 0.00900805\n",
      "Iteration 34, loss = 0.00848695\n",
      "Iteration 35, loss = 0.00815340\n",
      "Iteration 36, loss = 0.00750410\n",
      "Iteration 37, loss = 0.00713944\n",
      "Iteration 38, loss = 0.00668156\n",
      "Iteration 39, loss = 0.00622197\n",
      "Iteration 40, loss = 0.00599232\n",
      "Iteration 41, loss = 0.00553413\n",
      "Iteration 42, loss = 0.00514864\n",
      "Iteration 43, loss = 0.00467526\n",
      "Iteration 44, loss = 0.00438887\n",
      "Iteration 45, loss = 0.00396738\n",
      "Iteration 46, loss = 0.00379125\n",
      "Iteration 47, loss = 0.00349447\n",
      "Iteration 48, loss = 0.00317783\n",
      "Iteration 49, loss = 0.00293358\n",
      "Iteration 50, loss = 0.00282250\n",
      "Iteration 51, loss = 0.00280277\n",
      "Iteration 52, loss = 0.00259496\n",
      "Iteration 53, loss = 0.00244378\n",
      "Iteration 54, loss = 0.00229871\n",
      "Iteration 55, loss = 0.00219837\n",
      "Iteration 56, loss = 0.00211492\n",
      "Iteration 57, loss = 0.00203306\n",
      "Iteration 58, loss = 0.00199701\n",
      "Iteration 59, loss = 0.00191748\n",
      "Iteration 60, loss = 0.00181799\n",
      "Iteration 61, loss = 0.00173964\n",
      "Iteration 62, loss = 0.00163518\n",
      "Iteration 63, loss = 0.00140196\n",
      "Iteration 64, loss = 0.00129082\n",
      "Iteration 65, loss = 0.00126603\n",
      "Iteration 66, loss = 0.00120045\n",
      "Iteration 67, loss = 0.00115508\n",
      "Iteration 68, loss = 0.00111806\n",
      "Iteration 69, loss = 0.00109957\n",
      "Iteration 70, loss = 0.00105397\n",
      "Iteration 71, loss = 0.00101947\n",
      "Iteration 72, loss = 0.00098675\n",
      "Iteration 73, loss = 0.00098307\n",
      "Iteration 74, loss = 0.00095342\n",
      "Iteration 75, loss = 0.00090028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.96922932\n",
      "Iteration 2, loss = 0.83997791\n",
      "Iteration 3, loss = 0.74561692\n",
      "Iteration 4, loss = 0.44136550\n",
      "Iteration 5, loss = 0.14266080\n",
      "Iteration 6, loss = 0.07329015\n",
      "Iteration 7, loss = 0.05446499\n",
      "Iteration 8, loss = 0.04532800\n",
      "Iteration 9, loss = 0.04080994\n",
      "Iteration 10, loss = 0.03639884\n",
      "Iteration 11, loss = 0.03387256\n",
      "Iteration 12, loss = 0.03157991\n",
      "Iteration 13, loss = 0.03001587\n",
      "Iteration 14, loss = 0.02821071\n",
      "Iteration 15, loss = 0.02692192\n",
      "Iteration 16, loss = 0.02566205\n",
      "Iteration 17, loss = 0.02452022\n",
      "Iteration 18, loss = 0.02290938\n",
      "Iteration 19, loss = 0.02159253\n",
      "Iteration 20, loss = 0.02046360\n",
      "Iteration 21, loss = 0.01915354\n",
      "Iteration 22, loss = 0.01813429\n",
      "Iteration 23, loss = 0.01743663\n",
      "Iteration 24, loss = 0.01680954\n",
      "Iteration 25, loss = 0.01586907\n",
      "Iteration 26, loss = 0.01519924\n",
      "Iteration 27, loss = 0.01485298\n",
      "Iteration 28, loss = 0.01422324\n",
      "Iteration 29, loss = 0.01528035\n",
      "Iteration 30, loss = 0.01320694\n",
      "Iteration 31, loss = 0.01260585\n",
      "Iteration 32, loss = 0.01199443\n",
      "Iteration 33, loss = 0.01169613\n",
      "Iteration 34, loss = 0.01130548\n",
      "Iteration 35, loss = 0.01095321\n",
      "Iteration 36, loss = 0.01049302\n",
      "Iteration 37, loss = 0.01010862\n",
      "Iteration 38, loss = 0.00977527\n",
      "Iteration 39, loss = 0.00949840\n",
      "Iteration 40, loss = 0.00913441\n",
      "Iteration 41, loss = 0.00878113\n",
      "Iteration 42, loss = 0.00857750\n",
      "Iteration 43, loss = 0.00813656\n",
      "Iteration 44, loss = 0.00770892\n",
      "Iteration 45, loss = 0.00734458\n",
      "Iteration 46, loss = 0.00699078\n",
      "Iteration 47, loss = 0.00672173\n",
      "Iteration 48, loss = 0.00630341\n",
      "Iteration 49, loss = 0.00580823\n",
      "Iteration 50, loss = 0.00546753\n",
      "Iteration 51, loss = 0.00514634\n",
      "Iteration 52, loss = 0.00484781\n",
      "Iteration 53, loss = 0.00466666\n",
      "Iteration 54, loss = 0.00428090\n",
      "Iteration 55, loss = 0.00398086\n",
      "Iteration 56, loss = 0.00368737\n",
      "Iteration 57, loss = 0.00346813\n",
      "Iteration 58, loss = 0.00319949\n",
      "Iteration 59, loss = 0.00299646\n",
      "Iteration 60, loss = 0.00268732\n",
      "Iteration 61, loss = 0.00257692\n",
      "Iteration 62, loss = 0.00233515\n",
      "Iteration 63, loss = 0.00213231\n",
      "Iteration 64, loss = 0.00205137\n",
      "Iteration 65, loss = 0.00194945\n",
      "Iteration 66, loss = 0.00180823\n",
      "Iteration 67, loss = 0.00164963\n",
      "Iteration 68, loss = 0.00154656\n",
      "Iteration 69, loss = 0.00141628\n",
      "Iteration 70, loss = 0.00131502\n",
      "Iteration 71, loss = 0.00120457\n",
      "Iteration 72, loss = 0.00109957\n",
      "Iteration 73, loss = 0.00095386\n",
      "Iteration 74, loss = 0.00092180\n",
      "Iteration 75, loss = 0.00080930\n",
      "Iteration 76, loss = 0.00075966\n",
      "Iteration 77, loss = 0.00077131\n",
      "Iteration 78, loss = 0.00067257\n",
      "Iteration 79, loss = 0.00062312\n",
      "Iteration 80, loss = 0.00061428\n",
      "Iteration 81, loss = 0.00056952\n",
      "Iteration 82, loss = 0.00053940\n",
      "Iteration 83, loss = 0.00052585\n",
      "Iteration 84, loss = 0.00049027\n",
      "Iteration 85, loss = 0.00136507\n",
      "Iteration 86, loss = 0.00072466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71989085\n",
      "Iteration 2, loss = 0.51884457\n",
      "Iteration 3, loss = 0.28019574\n",
      "Iteration 4, loss = 0.13392281\n",
      "Iteration 5, loss = 0.08427949\n",
      "Iteration 6, loss = 0.06525847\n",
      "Iteration 7, loss = 0.05557328\n",
      "Iteration 8, loss = 0.04952940\n",
      "Iteration 9, loss = 0.04542642\n",
      "Iteration 10, loss = 0.04235847\n",
      "Iteration 11, loss = 0.03924806\n",
      "Iteration 12, loss = 0.03665630\n",
      "Iteration 13, loss = 0.03438286\n",
      "Iteration 14, loss = 0.03176439\n",
      "Iteration 15, loss = 0.02972584\n",
      "Iteration 16, loss = 0.02761728\n",
      "Iteration 17, loss = 0.02592482\n",
      "Iteration 18, loss = 0.02429501\n",
      "Iteration 19, loss = 0.02318111\n",
      "Iteration 20, loss = 0.02140565\n",
      "Iteration 21, loss = 0.02030742\n",
      "Iteration 22, loss = 0.01917035\n",
      "Iteration 23, loss = 0.01806534\n",
      "Iteration 24, loss = 0.01733883\n",
      "Iteration 25, loss = 0.01603191\n",
      "Iteration 26, loss = 0.01540035\n",
      "Iteration 27, loss = 0.01461339\n",
      "Iteration 28, loss = 0.01349100\n",
      "Iteration 29, loss = 0.01281757\n",
      "Iteration 30, loss = 0.01214746\n",
      "Iteration 31, loss = 0.01153367\n",
      "Iteration 32, loss = 0.01089641\n",
      "Iteration 33, loss = 0.01031497\n",
      "Iteration 34, loss = 0.00971773\n",
      "Iteration 35, loss = 0.00917575\n",
      "Iteration 36, loss = 0.00876083\n",
      "Iteration 37, loss = 0.00831323\n",
      "Iteration 38, loss = 0.00782684\n",
      "Iteration 39, loss = 0.00750609\n",
      "Iteration 40, loss = 0.00722352\n",
      "Iteration 41, loss = 0.00677386\n",
      "Iteration 42, loss = 0.00635372\n",
      "Iteration 43, loss = 0.00630396\n",
      "Iteration 44, loss = 0.00598911\n",
      "Iteration 45, loss = 0.00555217\n",
      "Iteration 46, loss = 0.00535027\n",
      "Iteration 47, loss = 0.00508353\n",
      "Iteration 48, loss = 0.00495417\n",
      "Iteration 49, loss = 0.00481288\n",
      "Iteration 50, loss = 0.00457903\n",
      "Iteration 51, loss = 0.00445887\n",
      "Iteration 52, loss = 0.00427196\n",
      "Iteration 53, loss = 0.00410156\n",
      "Iteration 54, loss = 0.00401016\n",
      "Iteration 55, loss = 0.00385761\n",
      "Iteration 56, loss = 0.00376483\n",
      "Iteration 57, loss = 0.00365324\n",
      "Iteration 58, loss = 0.00354222\n",
      "Iteration 59, loss = 0.00345903\n",
      "Iteration 60, loss = 0.00336859\n",
      "Iteration 61, loss = 0.00327952\n",
      "Iteration 62, loss = 0.00320941\n",
      "Iteration 63, loss = 0.00313966\n",
      "Iteration 64, loss = 0.00304769\n",
      "Iteration 65, loss = 0.00294759\n",
      "Iteration 66, loss = 0.00288619\n",
      "Iteration 67, loss = 0.00281151\n",
      "Iteration 68, loss = 0.00260939\n",
      "Iteration 69, loss = 0.00245306\n",
      "Iteration 70, loss = 0.00228408\n",
      "Iteration 71, loss = 0.00221759\n",
      "Iteration 72, loss = 0.00213693\n",
      "Iteration 73, loss = 0.00205473\n",
      "Iteration 74, loss = 0.00197488\n",
      "Iteration 75, loss = 0.00202992\n",
      "Iteration 76, loss = 0.00182358\n",
      "Iteration 77, loss = 0.00157141\n",
      "Iteration 78, loss = 0.00150988\n",
      "Iteration 79, loss = 0.00143302\n",
      "Iteration 80, loss = 0.00131080\n",
      "Iteration 81, loss = 0.00123774\n",
      "Iteration 82, loss = 0.00119692\n",
      "Iteration 83, loss = 0.00115894\n",
      "Iteration 84, loss = 0.00110870\n",
      "Iteration 85, loss = 0.00106928\n",
      "Iteration 86, loss = 0.00097140\n",
      "Iteration 87, loss = 0.00091151\n",
      "Iteration 88, loss = 0.00077274\n",
      "Iteration 89, loss = 0.00067481\n",
      "Iteration 90, loss = 0.00063225\n",
      "Iteration 91, loss = 0.00057433\n",
      "Iteration 92, loss = 0.00054655\n",
      "Iteration 93, loss = 0.00052019\n",
      "Iteration 94, loss = 0.00050240\n",
      "Iteration 95, loss = 0.00046418\n",
      "Iteration 96, loss = 0.00043035\n",
      "Iteration 97, loss = 0.00039820\n",
      "Iteration 98, loss = 0.00031195\n",
      "Iteration 99, loss = 0.00028068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73636580\n",
      "Iteration 2, loss = 0.52746143\n",
      "Iteration 3, loss = 0.38422314\n",
      "Iteration 4, loss = 0.31621254\n",
      "Iteration 5, loss = 0.27241963\n",
      "Iteration 6, loss = 0.24196341\n",
      "Iteration 7, loss = 0.21872626\n",
      "Iteration 8, loss = 0.20131965\n",
      "Iteration 9, loss = 0.18547173\n",
      "Iteration 10, loss = 0.17158293\n",
      "Iteration 11, loss = 0.15965595\n",
      "Iteration 12, loss = 0.14906312\n",
      "Iteration 13, loss = 0.14032062\n",
      "Iteration 14, loss = 0.13389568\n",
      "Iteration 15, loss = 0.12804926\n",
      "Iteration 16, loss = 0.12253735\n",
      "Iteration 17, loss = 0.11732556\n",
      "Iteration 18, loss = 0.11246441\n",
      "Iteration 19, loss = 0.10793061\n",
      "Iteration 20, loss = 0.10381312\n",
      "Iteration 21, loss = 0.09963610\n",
      "Iteration 22, loss = 0.09606204\n",
      "Iteration 23, loss = 0.09289780\n",
      "Iteration 24, loss = 0.09246799\n",
      "Iteration 25, loss = 0.08829806\n",
      "Iteration 26, loss = 0.08560436\n",
      "Iteration 27, loss = 0.08318204\n",
      "Iteration 28, loss = 0.08084906\n",
      "Iteration 29, loss = 0.07859530\n",
      "Iteration 30, loss = 0.07666899\n",
      "Iteration 31, loss = 0.07461540\n",
      "Iteration 32, loss = 0.07266312\n",
      "Iteration 33, loss = 0.07143601\n",
      "Iteration 34, loss = 0.06903346\n",
      "Iteration 35, loss = 0.06719302\n",
      "Iteration 36, loss = 0.06545804\n",
      "Iteration 37, loss = 0.06376040\n",
      "Iteration 38, loss = 0.06218498\n",
      "Iteration 39, loss = 0.06059467\n",
      "Iteration 40, loss = 0.05901907\n",
      "Iteration 41, loss = 0.05753512\n",
      "Iteration 42, loss = 0.05623541\n",
      "Iteration 43, loss = 0.05468561\n",
      "Iteration 44, loss = 0.05334872\n",
      "Iteration 45, loss = 0.05205451\n",
      "Iteration 46, loss = 0.05071382\n",
      "Iteration 47, loss = 0.04943295\n",
      "Iteration 48, loss = 0.04823719\n",
      "Iteration 49, loss = 0.04711912\n",
      "Iteration 50, loss = 0.04595788\n",
      "Iteration 51, loss = 0.04496047\n",
      "Iteration 52, loss = 0.04368177\n",
      "Iteration 53, loss = 0.04258586\n",
      "Iteration 54, loss = 0.04148980\n",
      "Iteration 55, loss = 0.04044152\n",
      "Iteration 56, loss = 0.03933387\n",
      "Iteration 57, loss = 0.03845964\n",
      "Iteration 58, loss = 0.03741085\n",
      "Iteration 59, loss = 0.03659152\n",
      "Iteration 60, loss = 0.03549707\n",
      "Iteration 61, loss = 0.03455209\n",
      "Iteration 62, loss = 0.03359385\n",
      "Iteration 63, loss = 0.03283071\n",
      "Iteration 64, loss = 0.03187642\n",
      "Iteration 65, loss = 0.03114262\n",
      "Iteration 66, loss = 0.03035918\n",
      "Iteration 67, loss = 0.02946249\n",
      "Iteration 68, loss = 0.02868510\n",
      "Iteration 69, loss = 0.02785972\n",
      "Iteration 70, loss = 0.02703241\n",
      "Iteration 71, loss = 0.02617612\n",
      "Iteration 72, loss = 0.02541551\n",
      "Iteration 73, loss = 0.02468681\n",
      "Iteration 74, loss = 0.02388877\n",
      "Iteration 75, loss = 0.02322965\n",
      "Iteration 76, loss = 0.02256851\n",
      "Iteration 77, loss = 0.02178929\n",
      "Iteration 78, loss = 0.02122850\n",
      "Iteration 79, loss = 0.02033220\n",
      "Iteration 80, loss = 0.01971570\n",
      "Iteration 81, loss = 0.01896416\n",
      "Iteration 82, loss = 0.01844515\n",
      "Iteration 83, loss = 0.01785142\n",
      "Iteration 84, loss = 0.01715799\n",
      "Iteration 85, loss = 0.01663504\n",
      "Iteration 86, loss = 0.01609895\n",
      "Iteration 87, loss = 0.01555071\n",
      "Iteration 88, loss = 0.01493487\n",
      "Iteration 89, loss = 0.01451134\n",
      "Iteration 90, loss = 0.01404873\n",
      "Iteration 91, loss = 0.01355906\n",
      "Iteration 92, loss = 0.01323378\n",
      "Iteration 93, loss = 0.01277094\n",
      "Iteration 94, loss = 0.01240359\n",
      "Iteration 95, loss = 0.01208294\n",
      "Iteration 96, loss = 0.01174567\n",
      "Iteration 97, loss = 0.01144516\n",
      "Iteration 98, loss = 0.01112521\n",
      "Iteration 99, loss = 0.01080385\n",
      "Iteration 100, loss = 0.01053451\n",
      "Iteration 101, loss = 0.01026527\n",
      "Iteration 102, loss = 0.01011753\n",
      "Iteration 103, loss = 0.00981700\n",
      "Iteration 104, loss = 0.00956003\n",
      "Iteration 105, loss = 0.00934951\n",
      "Iteration 106, loss = 0.00917927\n",
      "Iteration 107, loss = 0.00897808\n",
      "Iteration 108, loss = 0.00879060\n",
      "Iteration 109, loss = 0.00860181\n",
      "Iteration 110, loss = 0.00845333\n",
      "Iteration 111, loss = 0.00831780\n",
      "Iteration 112, loss = 0.00814973\n",
      "Iteration 113, loss = 0.00800601\n",
      "Iteration 114, loss = 0.00788212\n",
      "Iteration 115, loss = 0.00774041\n",
      "Iteration 116, loss = 0.00761611\n",
      "Iteration 117, loss = 0.00748697\n",
      "Iteration 118, loss = 0.00737721\n",
      "Iteration 119, loss = 0.00726624\n",
      "Iteration 120, loss = 0.00715756\n",
      "Iteration 121, loss = 0.00705598\n",
      "Iteration 122, loss = 0.00694991\n",
      "Iteration 123, loss = 0.00685704\n",
      "Iteration 124, loss = 0.00676020\n",
      "Iteration 125, loss = 0.00667941\n",
      "Iteration 126, loss = 0.00658637\n",
      "Iteration 127, loss = 0.00650977\n",
      "Iteration 128, loss = 0.00642785\n",
      "Iteration 129, loss = 0.00635172\n",
      "Iteration 130, loss = 0.00627697\n",
      "Iteration 131, loss = 0.00620639\n",
      "Iteration 132, loss = 0.00613720\n",
      "Iteration 133, loss = 0.00607142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59476676\n",
      "Iteration 2, loss = 0.43912199\n",
      "Iteration 3, loss = 0.32660955\n",
      "Iteration 4, loss = 0.26620062\n",
      "Iteration 5, loss = 0.23905491\n",
      "Iteration 6, loss = 0.21638234\n",
      "Iteration 7, loss = 0.18907182\n",
      "Iteration 8, loss = 0.15600158\n",
      "Iteration 9, loss = 0.12606547\n",
      "Iteration 10, loss = 0.10020341\n",
      "Iteration 11, loss = 0.08023223\n",
      "Iteration 12, loss = 0.06575450\n",
      "Iteration 13, loss = 0.05481705\n",
      "Iteration 14, loss = 0.04924748\n",
      "Iteration 15, loss = 0.04267086\n",
      "Iteration 16, loss = 0.03874793\n",
      "Iteration 17, loss = 0.03586933\n",
      "Iteration 18, loss = 0.03347400\n",
      "Iteration 19, loss = 0.03135619\n",
      "Iteration 20, loss = 0.02945862\n",
      "Iteration 21, loss = 0.02825081\n",
      "Iteration 22, loss = 0.02656137\n",
      "Iteration 23, loss = 0.02521760\n",
      "Iteration 24, loss = 0.02401072\n",
      "Iteration 25, loss = 0.02300344\n",
      "Iteration 26, loss = 0.02183380\n",
      "Iteration 27, loss = 0.02097524\n",
      "Iteration 28, loss = 0.02003499\n",
      "Iteration 29, loss = 0.01918766\n",
      "Iteration 30, loss = 0.01817007\n",
      "Iteration 31, loss = 0.01751718\n",
      "Iteration 32, loss = 0.01663839\n",
      "Iteration 33, loss = 0.01586332\n",
      "Iteration 34, loss = 0.01519969\n",
      "Iteration 35, loss = 0.01465396\n",
      "Iteration 36, loss = 0.01399877\n",
      "Iteration 37, loss = 0.01334654\n",
      "Iteration 38, loss = 0.01272233\n",
      "Iteration 39, loss = 0.01193415\n",
      "Iteration 40, loss = 0.01149598\n",
      "Iteration 41, loss = 0.01151373\n",
      "Iteration 42, loss = 0.01037966\n",
      "Iteration 43, loss = 0.01010105\n",
      "Iteration 44, loss = 0.00945995\n",
      "Iteration 45, loss = 0.00895495\n",
      "Iteration 46, loss = 0.00836163\n",
      "Iteration 47, loss = 0.00832024\n",
      "Iteration 48, loss = 0.00758030\n",
      "Iteration 49, loss = 0.00738841\n",
      "Iteration 50, loss = 0.00718828\n",
      "Iteration 51, loss = 0.00637688\n",
      "Iteration 52, loss = 0.00609693\n",
      "Iteration 53, loss = 0.00551375\n",
      "Iteration 54, loss = 0.00520299\n",
      "Iteration 55, loss = 0.00507378\n",
      "Iteration 56, loss = 0.00472590\n",
      "Iteration 57, loss = 0.00438039\n",
      "Iteration 58, loss = 0.00419582\n",
      "Iteration 59, loss = 0.00405631\n",
      "Iteration 60, loss = 0.00410489\n",
      "Iteration 61, loss = 0.00352365\n",
      "Iteration 62, loss = 0.00498957\n",
      "Iteration 63, loss = 0.00336079\n",
      "Iteration 64, loss = 0.00312872\n",
      "Iteration 65, loss = 0.00290648\n",
      "Iteration 66, loss = 0.00283073\n",
      "Iteration 67, loss = 0.00264069\n",
      "Iteration 68, loss = 0.00252563\n",
      "Iteration 69, loss = 0.00244474\n",
      "Iteration 70, loss = 0.00237155\n",
      "Iteration 71, loss = 0.00228835\n",
      "Iteration 72, loss = 0.00225625\n",
      "Iteration 73, loss = 0.00216575\n",
      "Iteration 74, loss = 0.00228069\n",
      "Iteration 75, loss = 0.00207805\n",
      "Iteration 76, loss = 0.00200525\n",
      "Iteration 77, loss = 0.00198465\n",
      "Iteration 78, loss = 0.00194969\n",
      "Iteration 79, loss = 0.00189792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64758488\n",
      "Iteration 2, loss = 0.43572729\n",
      "Iteration 3, loss = 0.26120854\n",
      "Iteration 4, loss = 0.15884233\n",
      "Iteration 5, loss = 0.10532748\n",
      "Iteration 6, loss = 0.07763969\n",
      "Iteration 7, loss = 0.06083045\n",
      "Iteration 8, loss = 0.05214878\n",
      "Iteration 9, loss = 0.04725581\n",
      "Iteration 10, loss = 0.04285910\n",
      "Iteration 11, loss = 0.03973776\n",
      "Iteration 12, loss = 0.03680563\n",
      "Iteration 13, loss = 0.03438648\n",
      "Iteration 14, loss = 0.03207911\n",
      "Iteration 15, loss = 0.03037861\n",
      "Iteration 16, loss = 0.02898482\n",
      "Iteration 17, loss = 0.02713050\n",
      "Iteration 18, loss = 0.02573269\n",
      "Iteration 19, loss = 0.02802919\n",
      "Iteration 20, loss = 0.02422078\n",
      "Iteration 21, loss = 0.02322662\n",
      "Iteration 22, loss = 0.02235208\n",
      "Iteration 23, loss = 0.02176482\n",
      "Iteration 24, loss = 0.02090140\n",
      "Iteration 25, loss = 0.02042621\n",
      "Iteration 26, loss = 0.01987833\n",
      "Iteration 27, loss = 0.01887893\n",
      "Iteration 28, loss = 0.01837266\n",
      "Iteration 29, loss = 0.01779887\n",
      "Iteration 30, loss = 0.01711434\n",
      "Iteration 31, loss = 0.01672937\n",
      "Iteration 32, loss = 0.01628091\n",
      "Iteration 33, loss = 0.01563736\n",
      "Iteration 34, loss = 0.01515174\n",
      "Iteration 35, loss = 0.01459333\n",
      "Iteration 36, loss = 0.01402656\n",
      "Iteration 37, loss = 0.01366553\n",
      "Iteration 38, loss = 0.01339545\n",
      "Iteration 39, loss = 0.01277587\n",
      "Iteration 40, loss = 0.01231559\n",
      "Iteration 41, loss = 0.01184792\n",
      "Iteration 42, loss = 0.01152287\n",
      "Iteration 43, loss = 0.01114956\n",
      "Iteration 44, loss = 0.01059549\n",
      "Iteration 45, loss = 0.01031002\n",
      "Iteration 46, loss = 0.00996323\n",
      "Iteration 47, loss = 0.00953702\n",
      "Iteration 48, loss = 0.00907569\n",
      "Iteration 49, loss = 0.00885515\n",
      "Iteration 50, loss = 0.00826878\n",
      "Iteration 51, loss = 0.00813711\n",
      "Iteration 52, loss = 0.00778370\n",
      "Iteration 53, loss = 0.00757999\n",
      "Iteration 54, loss = 0.00720896\n",
      "Iteration 55, loss = 0.00681626\n",
      "Iteration 56, loss = 0.00667585\n",
      "Iteration 57, loss = 0.00634846\n",
      "Iteration 58, loss = 0.00612705\n",
      "Iteration 59, loss = 0.00602670\n",
      "Iteration 60, loss = 0.00603876\n",
      "Iteration 61, loss = 0.00539149\n",
      "Iteration 62, loss = 0.00520029\n",
      "Iteration 63, loss = 0.00510338\n",
      "Iteration 64, loss = 0.00480837\n",
      "Iteration 65, loss = 0.00454049\n",
      "Iteration 66, loss = 0.00455696\n",
      "Iteration 67, loss = 0.00417905\n",
      "Iteration 68, loss = 0.00406413\n",
      "Iteration 69, loss = 0.00392252\n",
      "Iteration 70, loss = 0.00650354\n",
      "Iteration 71, loss = 0.00413989\n",
      "Iteration 72, loss = 0.00362077\n",
      "Iteration 73, loss = 0.00337958\n",
      "Iteration 74, loss = 0.00312585\n",
      "Iteration 75, loss = 0.00292588\n",
      "Iteration 76, loss = 0.00282503\n",
      "Iteration 77, loss = 0.00757516\n",
      "Iteration 78, loss = 0.00310890\n",
      "Iteration 79, loss = 0.00279640\n",
      "Iteration 80, loss = 0.00256901\n",
      "Iteration 81, loss = 0.00241123\n",
      "Iteration 82, loss = 0.00227978\n",
      "Iteration 83, loss = 0.00217746\n",
      "Iteration 84, loss = 0.00212454\n",
      "Iteration 85, loss = 0.00199062\n",
      "Iteration 86, loss = 0.00193836\n",
      "Iteration 87, loss = 0.00183777\n",
      "Iteration 88, loss = 0.00177799\n",
      "Iteration 89, loss = 0.00172704\n",
      "Iteration 90, loss = 0.00162776\n",
      "Iteration 91, loss = 0.00157916\n",
      "Iteration 92, loss = 0.00152506\n",
      "Iteration 93, loss = 0.00146194\n",
      "Iteration 94, loss = 0.00139258\n",
      "Iteration 95, loss = 0.00132098\n",
      "Iteration 96, loss = 0.00126355\n",
      "Iteration 97, loss = 0.00116589\n",
      "Iteration 98, loss = 0.00112145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61740244\n",
      "Iteration 2, loss = 0.51664306\n",
      "Iteration 3, loss = 0.38909244\n",
      "Iteration 4, loss = 0.23082917\n",
      "Iteration 5, loss = 0.12398564\n",
      "Iteration 6, loss = 0.07755798\n",
      "Iteration 7, loss = 0.05634394\n",
      "Iteration 8, loss = 0.04631999\n",
      "Iteration 9, loss = 0.04013483\n",
      "Iteration 10, loss = 0.03578674\n",
      "Iteration 11, loss = 0.03263928\n",
      "Iteration 12, loss = 0.03002346\n",
      "Iteration 13, loss = 0.02801432\n",
      "Iteration 14, loss = 0.02649088\n",
      "Iteration 15, loss = 0.02471671\n",
      "Iteration 16, loss = 0.02343077\n",
      "Iteration 17, loss = 0.02220676\n",
      "Iteration 18, loss = 0.02151851\n",
      "Iteration 19, loss = 0.02036893\n",
      "Iteration 20, loss = 0.01970010\n",
      "Iteration 21, loss = 0.01865279\n",
      "Iteration 22, loss = 0.01803145\n",
      "Iteration 23, loss = 0.01761879\n",
      "Iteration 24, loss = 0.01661751\n",
      "Iteration 25, loss = 0.01629585\n",
      "Iteration 26, loss = 0.01567948\n",
      "Iteration 27, loss = 0.01522177\n",
      "Iteration 28, loss = 0.01453897\n",
      "Iteration 29, loss = 0.01403145\n",
      "Iteration 30, loss = 0.01377475\n",
      "Iteration 31, loss = 0.01316206\n",
      "Iteration 32, loss = 0.01285885\n",
      "Iteration 33, loss = 0.01237423\n",
      "Iteration 34, loss = 0.01190945\n",
      "Iteration 35, loss = 0.01155476\n",
      "Iteration 36, loss = 0.01117436\n",
      "Iteration 37, loss = 0.01085491\n",
      "Iteration 38, loss = 0.01044965\n",
      "Iteration 39, loss = 0.01016846\n",
      "Iteration 40, loss = 0.00994905\n",
      "Iteration 41, loss = 0.00952866\n",
      "Iteration 42, loss = 0.00943464\n",
      "Iteration 43, loss = 0.00898930\n",
      "Iteration 44, loss = 0.00873065\n",
      "Iteration 45, loss = 0.00844007\n",
      "Iteration 46, loss = 0.00808069\n",
      "Iteration 47, loss = 0.00792184\n",
      "Iteration 48, loss = 0.00768804\n",
      "Iteration 49, loss = 0.00748862\n",
      "Iteration 50, loss = 0.00726880\n",
      "Iteration 51, loss = 0.00711139\n",
      "Iteration 52, loss = 0.00690008\n",
      "Iteration 53, loss = 0.00671159\n",
      "Iteration 54, loss = 0.00661514\n",
      "Iteration 55, loss = 0.00651530\n",
      "Iteration 56, loss = 0.00631159\n",
      "Iteration 57, loss = 0.00616911\n",
      "Iteration 58, loss = 0.00609822\n",
      "Iteration 59, loss = 0.00596743\n",
      "Iteration 60, loss = 0.00637127\n",
      "Iteration 61, loss = 0.00594510\n",
      "Iteration 62, loss = 0.00568752\n",
      "Iteration 63, loss = 0.00558749\n",
      "Iteration 64, loss = 0.00545915\n",
      "Iteration 65, loss = 0.00540110\n",
      "Iteration 66, loss = 0.00530726\n",
      "Iteration 67, loss = 0.00523881\n",
      "Iteration 68, loss = 0.00508430\n",
      "Iteration 69, loss = 0.00493208\n",
      "Iteration 70, loss = 0.00476663\n",
      "Iteration 71, loss = 0.00463557\n",
      "Iteration 72, loss = 0.00452419\n",
      "Iteration 73, loss = 0.00439948\n",
      "Iteration 74, loss = 0.00430939\n",
      "Iteration 75, loss = 0.00418687\n",
      "Iteration 76, loss = 0.00405740\n",
      "Iteration 77, loss = 0.00393991\n",
      "Iteration 78, loss = 0.00381634\n",
      "Iteration 79, loss = 0.00367299\n",
      "Iteration 80, loss = 0.00349267\n",
      "Iteration 81, loss = 0.00339459\n",
      "Iteration 82, loss = 0.00420890\n",
      "Iteration 83, loss = 0.00346686\n",
      "Iteration 84, loss = 0.00332342\n",
      "Iteration 85, loss = 0.00295087\n",
      "Iteration 86, loss = 0.00281447\n",
      "Iteration 87, loss = 0.00271922\n",
      "Iteration 88, loss = 0.00262916\n",
      "Iteration 89, loss = 0.00238014\n",
      "Iteration 90, loss = 0.00230041\n",
      "Iteration 91, loss = 0.00220553\n",
      "Iteration 92, loss = 0.00213909\n",
      "Iteration 93, loss = 0.00206295\n",
      "Iteration 94, loss = 0.00196207\n",
      "Iteration 95, loss = 0.00187064\n",
      "Iteration 96, loss = 0.00180467\n",
      "Iteration 97, loss = 0.00174888\n",
      "Iteration 98, loss = 0.00148179\n",
      "Iteration 99, loss = 0.00129565\n",
      "Iteration 100, loss = 0.00123226\n",
      "Iteration 101, loss = 0.00101143\n",
      "Iteration 102, loss = 0.00089853\n",
      "Iteration 103, loss = 0.00082429\n",
      "Iteration 104, loss = 0.00077821\n",
      "Iteration 105, loss = 0.00066054\n",
      "Iteration 106, loss = 0.00060330\n",
      "Iteration 107, loss = 0.00054866\n",
      "Iteration 108, loss = 0.00051453\n",
      "Iteration 109, loss = 0.00048638\n",
      "Iteration 110, loss = 0.00045324\n",
      "Iteration 111, loss = 0.00043066\n",
      "Iteration 112, loss = 0.00040653\n",
      "Iteration 113, loss = 0.00037255\n",
      "Iteration 114, loss = 0.00034784\n",
      "Iteration 115, loss = 0.00032839\n",
      "Iteration 116, loss = 0.00031517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.78965959\n",
      "Iteration 2, loss = 0.71309407\n",
      "Iteration 3, loss = 0.66724189\n",
      "Iteration 4, loss = 0.64179385\n",
      "Iteration 5, loss = 0.62830412\n",
      "Iteration 6, loss = 0.62051798\n",
      "Iteration 7, loss = 0.61441431\n",
      "Iteration 8, loss = 0.60809527\n",
      "Iteration 9, loss = 0.60073888\n",
      "Iteration 10, loss = 0.59175487\n",
      "Iteration 11, loss = 0.58064668\n",
      "Iteration 12, loss = 0.56680792\n",
      "Iteration 13, loss = 0.54964135\n",
      "Iteration 14, loss = 0.52842631\n",
      "Iteration 15, loss = 0.50268075\n",
      "Iteration 16, loss = 0.47237144\n",
      "Iteration 17, loss = 0.43792118\n",
      "Iteration 18, loss = 0.40056333\n",
      "Iteration 19, loss = 0.36195257\n",
      "Iteration 20, loss = 0.32375354\n",
      "Iteration 21, loss = 0.28750067\n",
      "Iteration 22, loss = 0.25437375\n",
      "Iteration 23, loss = 0.22481655\n",
      "Iteration 24, loss = 0.19903053\n",
      "Iteration 25, loss = 0.17681031\n",
      "Iteration 26, loss = 0.15791519\n",
      "Iteration 27, loss = 0.14170478\n",
      "Iteration 28, loss = 0.12795452\n",
      "Iteration 29, loss = 0.11630985\n",
      "Iteration 30, loss = 0.10633061\n",
      "Iteration 31, loss = 0.09786211\n",
      "Iteration 32, loss = 0.09059670\n",
      "Iteration 33, loss = 0.08423641\n",
      "Iteration 34, loss = 0.07876212\n",
      "Iteration 35, loss = 0.07398631\n",
      "Iteration 36, loss = 0.06978383\n",
      "Iteration 37, loss = 0.06608257\n",
      "Iteration 38, loss = 0.06280337\n",
      "Iteration 39, loss = 0.05986169\n",
      "Iteration 40, loss = 0.05734529\n",
      "Iteration 41, loss = 0.05495107\n",
      "Iteration 42, loss = 0.05282852\n",
      "Iteration 43, loss = 0.05095509\n",
      "Iteration 44, loss = 0.04932561\n",
      "Iteration 45, loss = 0.04774125\n",
      "Iteration 46, loss = 0.04632232\n",
      "Iteration 47, loss = 0.04505967\n",
      "Iteration 48, loss = 0.04387502\n",
      "Iteration 49, loss = 0.04286217\n",
      "Iteration 50, loss = 0.04190050\n",
      "Iteration 51, loss = 0.04100442\n",
      "Iteration 52, loss = 0.04019676\n",
      "Iteration 53, loss = 0.03948065\n",
      "Iteration 54, loss = 0.03880762\n",
      "Iteration 55, loss = 0.03817498\n",
      "Iteration 56, loss = 0.03761818\n",
      "Iteration 57, loss = 0.03706396\n",
      "Iteration 58, loss = 0.03660389\n",
      "Iteration 59, loss = 0.03611299\n",
      "Iteration 60, loss = 0.03573169\n",
      "Iteration 61, loss = 0.03528149\n",
      "Iteration 62, loss = 0.03484086\n",
      "Iteration 63, loss = 0.03446990\n",
      "Iteration 64, loss = 0.03407899\n",
      "Iteration 65, loss = 0.03378859\n",
      "Iteration 66, loss = 0.03335083\n",
      "Iteration 67, loss = 0.03309428\n",
      "Iteration 68, loss = 0.03277275\n",
      "Iteration 69, loss = 0.03248156\n",
      "Iteration 70, loss = 0.03223853\n",
      "Iteration 71, loss = 0.03202794\n",
      "Iteration 72, loss = 0.03180077\n",
      "Iteration 73, loss = 0.03163082\n",
      "Iteration 74, loss = 0.03143743\n",
      "Iteration 75, loss = 0.03124053\n",
      "Iteration 76, loss = 0.03109514\n",
      "Iteration 77, loss = 0.03092297\n",
      "Iteration 78, loss = 0.03080880\n",
      "Iteration 79, loss = 0.03068787\n",
      "Iteration 80, loss = 0.03051849\n",
      "Iteration 81, loss = 0.03036552\n",
      "Iteration 82, loss = 0.03022130\n",
      "Iteration 83, loss = 0.03007642\n",
      "Iteration 84, loss = 0.02994697\n",
      "Iteration 85, loss = 0.02976370\n",
      "Iteration 86, loss = 0.02959011\n",
      "Iteration 87, loss = 0.02935398\n",
      "Iteration 88, loss = 0.02914663\n",
      "Iteration 89, loss = 0.02894564\n",
      "Iteration 90, loss = 0.02874642\n",
      "Iteration 91, loss = 0.02851512\n",
      "Iteration 92, loss = 0.02842336\n",
      "Iteration 93, loss = 0.02822330\n",
      "Iteration 94, loss = 0.02808655\n",
      "Iteration 95, loss = 0.02791790\n",
      "Iteration 96, loss = 0.02781219\n",
      "Iteration 97, loss = 0.02770952\n",
      "Iteration 98, loss = 0.02761562\n",
      "Iteration 99, loss = 0.02752172\n",
      "Iteration 100, loss = 0.02742883\n",
      "Iteration 101, loss = 0.02734654\n",
      "Iteration 102, loss = 0.02723532\n",
      "Iteration 103, loss = 0.02712206\n",
      "Iteration 104, loss = 0.02700949\n",
      "Iteration 105, loss = 0.02690493\n",
      "Iteration 106, loss = 0.02679886\n",
      "Iteration 107, loss = 0.02666415\n",
      "Iteration 108, loss = 0.02659259\n",
      "Iteration 109, loss = 0.02639130\n",
      "Iteration 110, loss = 0.02628368\n",
      "Iteration 111, loss = 0.02614950\n",
      "Iteration 112, loss = 0.02605652\n",
      "Iteration 113, loss = 0.02595502\n",
      "Iteration 114, loss = 0.02585773\n",
      "Iteration 115, loss = 0.02578168\n",
      "Iteration 116, loss = 0.02567488\n",
      "Iteration 117, loss = 0.02556395\n",
      "Iteration 118, loss = 0.02545074\n",
      "Iteration 119, loss = 0.02537681\n",
      "Iteration 120, loss = 0.02528922\n",
      "Iteration 121, loss = 0.02521585\n",
      "Iteration 122, loss = 0.02511643\n",
      "Iteration 123, loss = 0.02503839\n",
      "Iteration 124, loss = 0.02496076\n",
      "Iteration 125, loss = 0.02489716\n",
      "Iteration 126, loss = 0.02476995\n",
      "Iteration 127, loss = 0.02466069\n",
      "Iteration 128, loss = 0.02458822\n",
      "Iteration 129, loss = 0.02454968\n",
      "Iteration 130, loss = 0.02447312\n",
      "Iteration 131, loss = 0.02441834\n",
      "Iteration 132, loss = 0.02438760\n",
      "Iteration 133, loss = 0.02432815\n",
      "Iteration 134, loss = 0.02426995\n",
      "Iteration 135, loss = 0.02421166\n",
      "Iteration 136, loss = 0.02419348\n",
      "Iteration 137, loss = 0.02411328\n",
      "Iteration 138, loss = 0.02401643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63693106\n",
      "Iteration 2, loss = 0.63515783\n",
      "Iteration 3, loss = 0.63180617\n",
      "Iteration 4, loss = 0.62349344\n",
      "Iteration 5, loss = 0.60669918\n",
      "Iteration 6, loss = 0.57775026\n",
      "Iteration 7, loss = 0.53392671\n",
      "Iteration 8, loss = 0.47467913\n",
      "Iteration 9, loss = 0.40582812\n",
      "Iteration 10, loss = 0.33814979\n",
      "Iteration 11, loss = 0.27962726\n",
      "Iteration 12, loss = 0.23383930\n",
      "Iteration 13, loss = 0.19823118\n",
      "Iteration 14, loss = 0.17057299\n",
      "Iteration 15, loss = 0.14954348\n",
      "Iteration 16, loss = 0.13278730\n",
      "Iteration 17, loss = 0.11936866\n",
      "Iteration 18, loss = 0.10849413\n",
      "Iteration 19, loss = 0.09948125\n",
      "Iteration 20, loss = 0.09182750\n",
      "Iteration 21, loss = 0.08549879\n",
      "Iteration 22, loss = 0.07989704\n",
      "Iteration 23, loss = 0.07508281\n",
      "Iteration 24, loss = 0.07089261\n",
      "Iteration 25, loss = 0.06710031\n",
      "Iteration 26, loss = 0.06389374\n",
      "Iteration 27, loss = 0.06086953\n",
      "Iteration 28, loss = 0.05824920\n",
      "Iteration 29, loss = 0.05588073\n",
      "Iteration 30, loss = 0.05363361\n",
      "Iteration 31, loss = 0.05153895\n",
      "Iteration 32, loss = 0.04972809\n",
      "Iteration 33, loss = 0.04793544\n",
      "Iteration 34, loss = 0.04627558\n",
      "Iteration 35, loss = 0.04502806\n",
      "Iteration 36, loss = 0.04354347\n",
      "Iteration 37, loss = 0.04222983\n",
      "Iteration 38, loss = 0.04105432\n",
      "Iteration 39, loss = 0.03996136\n",
      "Iteration 40, loss = 0.03899337\n",
      "Iteration 41, loss = 0.03801454\n",
      "Iteration 42, loss = 0.03722152\n",
      "Iteration 43, loss = 0.03631229\n",
      "Iteration 44, loss = 0.03561121\n",
      "Iteration 45, loss = 0.03486520\n",
      "Iteration 46, loss = 0.03413132\n",
      "Iteration 47, loss = 0.03349730\n",
      "Iteration 48, loss = 0.03295577\n",
      "Iteration 49, loss = 0.03222572\n",
      "Iteration 50, loss = 0.03162928\n",
      "Iteration 51, loss = 0.03115309\n",
      "Iteration 52, loss = 0.03076948\n",
      "Iteration 53, loss = 0.03022796\n",
      "Iteration 54, loss = 0.02964267\n",
      "Iteration 55, loss = 0.02923291\n",
      "Iteration 56, loss = 0.02874617\n",
      "Iteration 57, loss = 0.02827342\n",
      "Iteration 58, loss = 0.02781406\n",
      "Iteration 59, loss = 0.02740946\n",
      "Iteration 60, loss = 0.02684228\n",
      "Iteration 61, loss = 0.02640598\n",
      "Iteration 62, loss = 0.02591736\n",
      "Iteration 63, loss = 0.02541036\n",
      "Iteration 64, loss = 0.02500104\n",
      "Iteration 65, loss = 0.02444433\n",
      "Iteration 66, loss = 0.02392261\n",
      "Iteration 67, loss = 0.02352216\n",
      "Iteration 68, loss = 0.02304567\n",
      "Iteration 69, loss = 0.02262089\n",
      "Iteration 70, loss = 0.02229529\n",
      "Iteration 71, loss = 0.02193019\n",
      "Iteration 72, loss = 0.02156021\n",
      "Iteration 73, loss = 0.02122109\n",
      "Iteration 74, loss = 0.02091193\n",
      "Iteration 75, loss = 0.02064440\n",
      "Iteration 76, loss = 0.02025003\n",
      "Iteration 77, loss = 0.01992455\n",
      "Iteration 78, loss = 0.01970262\n",
      "Iteration 79, loss = 0.01943727\n",
      "Iteration 80, loss = 0.01926410\n",
      "Iteration 81, loss = 0.01899848\n",
      "Iteration 82, loss = 0.01880358\n",
      "Iteration 83, loss = 0.01868023\n",
      "Iteration 84, loss = 0.01846373\n",
      "Iteration 85, loss = 0.01832588\n",
      "Iteration 86, loss = 0.01809316\n",
      "Iteration 87, loss = 0.01798260\n",
      "Iteration 88, loss = 0.01777239\n",
      "Iteration 89, loss = 0.01764411\n",
      "Iteration 90, loss = 0.01750708\n",
      "Iteration 91, loss = 0.01734909\n",
      "Iteration 92, loss = 0.01723272\n",
      "Iteration 93, loss = 0.01710729\n",
      "Iteration 94, loss = 0.01691952\n",
      "Iteration 95, loss = 0.01680147\n",
      "Iteration 96, loss = 0.01659855\n",
      "Iteration 97, loss = 0.01640996\n",
      "Iteration 98, loss = 0.01632562\n",
      "Iteration 99, loss = 0.01606003\n",
      "Iteration 100, loss = 0.01584225\n",
      "Iteration 101, loss = 0.01568855\n",
      "Iteration 102, loss = 0.01549048\n",
      "Iteration 103, loss = 0.01538900\n",
      "Iteration 104, loss = 0.01525283\n",
      "Iteration 105, loss = 0.01509470\n",
      "Iteration 106, loss = 0.01491009\n",
      "Iteration 107, loss = 0.01472759\n",
      "Iteration 108, loss = 0.01460808\n",
      "Iteration 109, loss = 0.01448123\n",
      "Iteration 110, loss = 0.01433498\n",
      "Iteration 111, loss = 0.01423435\n",
      "Iteration 112, loss = 0.01409700\n",
      "Iteration 113, loss = 0.01393505\n",
      "Iteration 114, loss = 0.01379636\n",
      "Iteration 115, loss = 0.01373204\n",
      "Iteration 116, loss = 0.01358474\n",
      "Iteration 117, loss = 0.01352114\n",
      "Iteration 118, loss = 0.01344507\n",
      "Iteration 119, loss = 0.01331619\n",
      "Iteration 120, loss = 0.01327503\n",
      "Iteration 121, loss = 0.01315990\n",
      "Iteration 122, loss = 0.01312626\n",
      "Iteration 123, loss = 0.01309243\n",
      "Iteration 124, loss = 0.01302354\n",
      "Iteration 125, loss = 0.01295772\n",
      "Iteration 126, loss = 0.01289014\n",
      "Iteration 127, loss = 0.01282337\n",
      "Iteration 128, loss = 0.02074954\n",
      "Iteration 129, loss = 0.01386247\n",
      "Iteration 130, loss = 0.01326351\n",
      "Iteration 131, loss = 0.01312412\n",
      "Iteration 132, loss = 0.01302587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.88883815\n",
      "Iteration 2, loss = 0.78935368\n",
      "Iteration 3, loss = 0.72233072\n",
      "Iteration 4, loss = 0.67659362\n",
      "Iteration 5, loss = 0.64895723\n",
      "Iteration 6, loss = 0.63429928\n",
      "Iteration 7, loss = 0.62479704\n",
      "Iteration 8, loss = 0.61647603\n",
      "Iteration 9, loss = 0.60832349\n",
      "Iteration 10, loss = 0.59860717\n",
      "Iteration 11, loss = 0.58534370\n",
      "Iteration 12, loss = 0.56726576\n",
      "Iteration 13, loss = 0.54310311\n",
      "Iteration 14, loss = 0.51246096\n",
      "Iteration 15, loss = 0.47563621\n",
      "Iteration 16, loss = 0.43365081\n",
      "Iteration 17, loss = 0.38903283\n",
      "Iteration 18, loss = 0.34435427\n",
      "Iteration 19, loss = 0.30137222\n",
      "Iteration 20, loss = 0.26292062\n",
      "Iteration 21, loss = 0.23029721\n",
      "Iteration 22, loss = 0.20390898\n",
      "Iteration 23, loss = 0.18167533\n",
      "Iteration 24, loss = 0.16366203\n",
      "Iteration 25, loss = 0.14874073\n",
      "Iteration 26, loss = 0.13603987\n",
      "Iteration 27, loss = 0.12557486\n",
      "Iteration 28, loss = 0.11656157\n",
      "Iteration 29, loss = 0.10902638\n",
      "Iteration 30, loss = 0.10243508\n",
      "Iteration 31, loss = 0.09669594\n",
      "Iteration 32, loss = 0.09165281\n",
      "Iteration 33, loss = 0.08719935\n",
      "Iteration 34, loss = 0.08314088\n",
      "Iteration 35, loss = 0.07954223\n",
      "Iteration 36, loss = 0.07631369\n",
      "Iteration 37, loss = 0.07327005\n",
      "Iteration 38, loss = 0.07053188\n",
      "Iteration 39, loss = 0.06809942\n",
      "Iteration 40, loss = 0.06575123\n",
      "Iteration 41, loss = 0.06350902\n",
      "Iteration 42, loss = 0.06151196\n",
      "Iteration 43, loss = 0.05961138\n",
      "Iteration 44, loss = 0.05786417\n",
      "Iteration 45, loss = 0.05616076\n",
      "Iteration 46, loss = 0.05470300\n",
      "Iteration 47, loss = 0.05314307\n",
      "Iteration 48, loss = 0.05178099\n",
      "Iteration 49, loss = 0.05063073\n",
      "Iteration 50, loss = 0.04930637\n",
      "Iteration 51, loss = 0.04810649\n",
      "Iteration 52, loss = 0.04709036\n",
      "Iteration 53, loss = 0.04606072\n",
      "Iteration 54, loss = 0.04512961\n",
      "Iteration 55, loss = 0.04413586\n",
      "Iteration 56, loss = 0.04336432\n",
      "Iteration 57, loss = 0.04254436\n",
      "Iteration 58, loss = 0.04177317\n",
      "Iteration 59, loss = 0.04109004\n",
      "Iteration 60, loss = 0.04031655\n",
      "Iteration 61, loss = 0.03967668\n",
      "Iteration 62, loss = 0.03906164\n",
      "Iteration 63, loss = 0.03881029\n",
      "Iteration 64, loss = 0.03801535\n",
      "Iteration 65, loss = 0.03748049\n",
      "Iteration 66, loss = 0.03694463\n",
      "Iteration 67, loss = 0.03643825\n",
      "Iteration 68, loss = 0.03612799\n",
      "Iteration 69, loss = 0.03558247\n",
      "Iteration 70, loss = 0.03521251\n",
      "Iteration 71, loss = 0.03476379\n",
      "Iteration 72, loss = 0.03446263\n",
      "Iteration 73, loss = 0.03401134\n",
      "Iteration 74, loss = 0.03682194\n",
      "Iteration 75, loss = 0.03408405\n",
      "Iteration 76, loss = 0.03350146\n",
      "Iteration 77, loss = 0.03323847\n",
      "Iteration 78, loss = 0.03300188\n",
      "Iteration 79, loss = 0.03274324\n",
      "Iteration 80, loss = 0.03250425\n",
      "Iteration 81, loss = 0.03229286\n",
      "Iteration 82, loss = 0.03207590\n",
      "Iteration 83, loss = 0.03184327\n",
      "Iteration 84, loss = 0.03166466\n",
      "Iteration 85, loss = 0.03143593\n",
      "Iteration 86, loss = 0.03124955\n",
      "Iteration 87, loss = 0.03103775\n",
      "Iteration 88, loss = 0.03085395\n",
      "Iteration 89, loss = 0.03064352\n",
      "Iteration 90, loss = 0.03046106\n",
      "Iteration 91, loss = 0.03029067\n",
      "Iteration 92, loss = 0.03007009\n",
      "Iteration 93, loss = 0.02986271\n",
      "Iteration 94, loss = 0.02965610\n",
      "Iteration 95, loss = 0.02949133\n",
      "Iteration 96, loss = 0.02930062\n",
      "Iteration 97, loss = 0.02915793\n",
      "Iteration 98, loss = 0.02897626\n",
      "Iteration 99, loss = 0.02878079\n",
      "Iteration 100, loss = 0.02855744\n",
      "Iteration 101, loss = 0.02838422\n",
      "Iteration 102, loss = 0.02823988\n",
      "Iteration 103, loss = 0.02807949\n",
      "Iteration 104, loss = 0.02790420\n",
      "Iteration 105, loss = 0.02776202\n",
      "Iteration 106, loss = 0.02755242\n",
      "Iteration 107, loss = 0.02739932\n",
      "Iteration 108, loss = 0.02720616\n",
      "Iteration 109, loss = 0.02709764\n",
      "Iteration 110, loss = 0.02696563\n",
      "Iteration 111, loss = 0.02673385\n",
      "Iteration 112, loss = 0.02657926\n",
      "Iteration 113, loss = 0.02640673\n",
      "Iteration 114, loss = 0.02624897\n",
      "Iteration 115, loss = 0.02608933\n",
      "Iteration 116, loss = 0.02599906\n",
      "Iteration 117, loss = 0.02573791\n",
      "Iteration 118, loss = 0.02557288\n",
      "Iteration 119, loss = 0.02540483\n",
      "Iteration 120, loss = 0.02527938\n",
      "Iteration 121, loss = 0.02514210\n",
      "Iteration 122, loss = 0.02501100\n",
      "Iteration 123, loss = 0.02493646\n",
      "Iteration 124, loss = 0.02474685\n",
      "Iteration 125, loss = 0.02467283\n",
      "Iteration 126, loss = 0.02446474\n",
      "Iteration 127, loss = 0.02437116\n",
      "Iteration 128, loss = 0.02942551\n",
      "Iteration 129, loss = 0.02490889\n",
      "Iteration 130, loss = 0.02440069\n",
      "Iteration 131, loss = 0.02421672\n",
      "Iteration 132, loss = 0.02409150\n",
      "Iteration 133, loss = 0.02397715\n",
      "Iteration 134, loss = 0.02386243\n",
      "Iteration 135, loss = 0.02375354\n",
      "Iteration 136, loss = 0.02366342\n",
      "Iteration 137, loss = 0.02355269\n",
      "Iteration 138, loss = 0.02345336\n",
      "Iteration 139, loss = 0.02331954\n",
      "Iteration 140, loss = 0.02320572\n",
      "Iteration 141, loss = 0.02307701\n",
      "Iteration 142, loss = 0.02293822\n",
      "Iteration 143, loss = 0.02279937\n",
      "Iteration 144, loss = 0.02263374\n",
      "Iteration 145, loss = 0.02250881\n",
      "Iteration 146, loss = 0.02228205\n",
      "Iteration 147, loss = 0.02211897\n",
      "Iteration 148, loss = 0.02386297\n",
      "Iteration 149, loss = 0.02216966\n",
      "Iteration 150, loss = 0.02185986\n",
      "Iteration 151, loss = 0.02169081\n",
      "Iteration 152, loss = 0.02153177\n",
      "Iteration 153, loss = 0.02138193\n",
      "Iteration 154, loss = 0.02128886\n",
      "Iteration 155, loss = 0.02114499\n",
      "Iteration 156, loss = 0.02100913\n",
      "Iteration 157, loss = 0.02088139\n",
      "Iteration 158, loss = 0.02080963\n",
      "Iteration 159, loss = 0.02065498\n",
      "Iteration 160, loss = 0.02055854\n",
      "Iteration 161, loss = 0.02043372\n",
      "Iteration 162, loss = 0.02035687\n",
      "Iteration 163, loss = 0.02027799\n",
      "Iteration 164, loss = 0.02018283\n",
      "Iteration 165, loss = 0.02005137\n",
      "Iteration 166, loss = 0.01996253\n",
      "Iteration 167, loss = 0.01989063\n",
      "Iteration 168, loss = 0.01977353\n",
      "Iteration 169, loss = 0.01973744\n",
      "Iteration 170, loss = 0.01963252\n",
      "Iteration 171, loss = 0.01954609\n",
      "Iteration 172, loss = 0.01949651\n",
      "Iteration 173, loss = 0.01939901\n",
      "Iteration 174, loss = 0.01931463\n",
      "Iteration 175, loss = 0.01920701\n",
      "Iteration 176, loss = 0.01913760\n",
      "Iteration 177, loss = 0.01903207\n",
      "Iteration 178, loss = 0.01890683\n",
      "Iteration 179, loss = 0.01881574\n",
      "Iteration 180, loss = 0.01870662\n",
      "Iteration 181, loss = 0.01862000\n",
      "Iteration 182, loss = 0.01849424\n",
      "Iteration 183, loss = 0.01835369\n",
      "Iteration 184, loss = 0.01829112\n",
      "Iteration 185, loss = 0.01813040\n",
      "Iteration 186, loss = 0.01800821\n",
      "Iteration 187, loss = 0.01795582\n",
      "Iteration 188, loss = 0.01783600\n",
      "Iteration 189, loss = 0.01772840\n",
      "Iteration 190, loss = 0.01762775\n",
      "Iteration 191, loss = 0.01752690\n",
      "Iteration 192, loss = 0.01743680\n",
      "Iteration 193, loss = 0.01736566\n",
      "Iteration 194, loss = 0.01723390\n",
      "Iteration 195, loss = 0.01717571\n",
      "Iteration 196, loss = 0.01708786\n",
      "Iteration 197, loss = 0.01702516\n",
      "Iteration 198, loss = 0.01690196\n",
      "Iteration 199, loss = 0.01680279\n",
      "Iteration 200, loss = 0.01674075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66214145\n",
      "Iteration 2, loss = 0.64210039\n",
      "Iteration 3, loss = 0.63296520\n",
      "Iteration 4, loss = 0.62586329\n",
      "Iteration 5, loss = 0.61602659\n",
      "Iteration 6, loss = 0.59922619\n",
      "Iteration 7, loss = 0.57016017\n",
      "Iteration 8, loss = 0.52472163\n",
      "Iteration 9, loss = 0.46795553\n",
      "Iteration 10, loss = 0.40460280\n",
      "Iteration 11, loss = 0.34175785\n",
      "Iteration 12, loss = 0.28659395\n",
      "Iteration 13, loss = 0.24160626\n",
      "Iteration 14, loss = 0.20621781\n",
      "Iteration 15, loss = 0.17830804\n",
      "Iteration 16, loss = 0.15648587\n",
      "Iteration 17, loss = 0.13919700\n",
      "Iteration 18, loss = 0.12561092\n",
      "Iteration 19, loss = 0.11420053\n",
      "Iteration 20, loss = 0.10481467\n",
      "Iteration 21, loss = 0.09696056\n",
      "Iteration 22, loss = 0.09033070\n",
      "Iteration 23, loss = 0.08460261\n",
      "Iteration 24, loss = 0.07971788\n",
      "Iteration 25, loss = 0.07549821\n",
      "Iteration 26, loss = 0.07170508\n",
      "Iteration 27, loss = 0.06828231\n",
      "Iteration 28, loss = 0.06531352\n",
      "Iteration 29, loss = 0.06251253\n",
      "Iteration 30, loss = 0.06001455\n",
      "Iteration 31, loss = 0.05768640\n",
      "Iteration 32, loss = 0.05569686\n",
      "Iteration 33, loss = 0.05354937\n",
      "Iteration 34, loss = 0.05180309\n",
      "Iteration 35, loss = 0.04994240\n",
      "Iteration 36, loss = 0.04836173\n",
      "Iteration 37, loss = 0.04676976\n",
      "Iteration 38, loss = 0.04537243\n",
      "Iteration 39, loss = 0.04402186\n",
      "Iteration 40, loss = 0.04275082\n",
      "Iteration 41, loss = 0.04155369\n",
      "Iteration 42, loss = 0.04045082\n",
      "Iteration 43, loss = 0.03938596\n",
      "Iteration 44, loss = 0.03838381\n",
      "Iteration 45, loss = 0.03749641\n",
      "Iteration 46, loss = 0.03665425\n",
      "Iteration 47, loss = 0.03566423\n",
      "Iteration 48, loss = 0.03500154\n",
      "Iteration 49, loss = 0.03421647\n",
      "Iteration 50, loss = 0.03351178\n",
      "Iteration 51, loss = 0.03282391\n",
      "Iteration 52, loss = 0.03218324\n",
      "Iteration 53, loss = 0.03160352\n",
      "Iteration 54, loss = 0.03117469\n",
      "Iteration 55, loss = 0.03043913\n",
      "Iteration 56, loss = 0.02993811\n",
      "Iteration 57, loss = 0.02940520\n",
      "Iteration 58, loss = 0.02903513\n",
      "Iteration 59, loss = 0.02848916\n",
      "Iteration 60, loss = 0.02779088\n",
      "Iteration 61, loss = 0.02727462\n",
      "Iteration 62, loss = 0.02665934\n",
      "Iteration 63, loss = 0.02618174\n",
      "Iteration 64, loss = 0.02569632\n",
      "Iteration 65, loss = 0.02525188\n",
      "Iteration 66, loss = 0.02472120\n",
      "Iteration 67, loss = 0.02425746\n",
      "Iteration 68, loss = 0.02374716\n",
      "Iteration 69, loss = 0.02329770\n",
      "Iteration 70, loss = 0.02280080\n",
      "Iteration 71, loss = 0.02240950\n",
      "Iteration 72, loss = 0.02186018\n",
      "Iteration 73, loss = 0.02144410\n",
      "Iteration 74, loss = 0.02096716\n",
      "Iteration 75, loss = 0.02057236\n",
      "Iteration 76, loss = 0.02016764\n",
      "Iteration 77, loss = 0.01979942\n",
      "Iteration 78, loss = 0.01941113\n",
      "Iteration 79, loss = 0.01918950\n",
      "Iteration 80, loss = 0.01870626\n",
      "Iteration 81, loss = 0.01839925\n",
      "Iteration 82, loss = 0.01808178\n",
      "Iteration 83, loss = 0.01772908\n",
      "Iteration 84, loss = 0.01738866\n",
      "Iteration 85, loss = 0.01710542\n",
      "Iteration 86, loss = 0.01680458\n",
      "Iteration 87, loss = 0.01659923\n",
      "Iteration 88, loss = 0.01878590\n",
      "Iteration 89, loss = 0.01660492\n",
      "Iteration 90, loss = 0.01630263\n",
      "Iteration 91, loss = 0.01604922\n",
      "Iteration 92, loss = 0.01581277\n",
      "Iteration 93, loss = 0.01560245\n",
      "Iteration 94, loss = 0.01547371\n",
      "Iteration 95, loss = 0.01527087\n",
      "Iteration 96, loss = 0.01511067\n",
      "Iteration 97, loss = 0.01497124\n",
      "Iteration 98, loss = 0.01480157\n",
      "Iteration 99, loss = 0.01470128\n",
      "Iteration 100, loss = 0.01458634\n",
      "Iteration 101, loss = 0.01444913\n",
      "Iteration 102, loss = 0.01431381\n",
      "Iteration 103, loss = 0.01417646\n",
      "Iteration 104, loss = 0.01407029\n",
      "Iteration 105, loss = 0.01397101\n",
      "Iteration 106, loss = 0.01389026\n",
      "Iteration 107, loss = 0.01379254\n",
      "Iteration 108, loss = 0.01369958\n",
      "Iteration 109, loss = 0.01368471\n",
      "Iteration 110, loss = 0.01352983\n",
      "Iteration 111, loss = 0.01348511\n",
      "Iteration 112, loss = 0.01338909\n",
      "Iteration 113, loss = 0.01326332\n",
      "Iteration 114, loss = 0.01318404\n",
      "Iteration 115, loss = 0.01313202\n",
      "Iteration 116, loss = 0.01300469\n",
      "Iteration 117, loss = 0.01294009\n",
      "Iteration 118, loss = 0.01282970\n",
      "Iteration 119, loss = 0.01273354\n",
      "Iteration 120, loss = 0.01266476\n",
      "Iteration 121, loss = 0.01258310\n",
      "Iteration 122, loss = 0.01250697\n",
      "Iteration 123, loss = 0.01241769\n",
      "Iteration 124, loss = 0.01230606\n",
      "Iteration 125, loss = 0.01219025\n",
      "Iteration 126, loss = 0.01209669\n",
      "Iteration 127, loss = 0.01201255\n",
      "Iteration 128, loss = 0.01187197\n",
      "Iteration 129, loss = 0.01180159\n",
      "Iteration 130, loss = 0.01171368\n",
      "Iteration 131, loss = 0.01153748\n",
      "Iteration 132, loss = 0.01156929\n",
      "Iteration 133, loss = 0.01131511\n",
      "Iteration 134, loss = 0.01117296\n",
      "Iteration 135, loss = 0.01107134\n",
      "Iteration 136, loss = 0.01094930\n",
      "Iteration 137, loss = 0.01085771\n",
      "Iteration 138, loss = 0.01077656\n",
      "Iteration 139, loss = 0.01063937\n",
      "Iteration 140, loss = 0.01056603\n",
      "Iteration 141, loss = 0.01047104\n",
      "Iteration 142, loss = 0.01039774\n",
      "Iteration 143, loss = 0.01030867\n",
      "Iteration 144, loss = 0.01024884\n",
      "Iteration 145, loss = 0.01018670\n",
      "Iteration 146, loss = 0.01015807\n",
      "Iteration 147, loss = 0.01009252\n",
      "Iteration 148, loss = 0.01004908\n",
      "Iteration 149, loss = 0.01004775\n",
      "Iteration 150, loss = 0.00999248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68180904\n",
      "Iteration 2, loss = 0.64734228\n",
      "Iteration 3, loss = 0.63398324\n",
      "Iteration 4, loss = 0.62652217\n",
      "Iteration 5, loss = 0.61673737\n",
      "Iteration 6, loss = 0.60292520\n",
      "Iteration 7, loss = 0.58283343\n",
      "Iteration 8, loss = 0.55475380\n",
      "Iteration 9, loss = 0.51758793\n",
      "Iteration 10, loss = 0.47217398\n",
      "Iteration 11, loss = 0.42012840\n",
      "Iteration 12, loss = 0.36604942\n",
      "Iteration 13, loss = 0.31447031\n",
      "Iteration 14, loss = 0.26889305\n",
      "Iteration 15, loss = 0.23071059\n",
      "Iteration 16, loss = 0.19940035\n",
      "Iteration 17, loss = 0.17364696\n",
      "Iteration 18, loss = 0.15322391\n",
      "Iteration 19, loss = 0.13629077\n",
      "Iteration 20, loss = 0.12270683\n",
      "Iteration 21, loss = 0.11139886\n",
      "Iteration 22, loss = 0.10212817\n",
      "Iteration 23, loss = 0.09399836\n",
      "Iteration 24, loss = 0.08721968\n",
      "Iteration 25, loss = 0.08124304\n",
      "Iteration 26, loss = 0.07607191\n",
      "Iteration 27, loss = 0.07152719\n",
      "Iteration 28, loss = 0.06739384\n",
      "Iteration 29, loss = 0.06385311\n",
      "Iteration 30, loss = 0.06060213\n",
      "Iteration 31, loss = 0.05772786\n",
      "Iteration 32, loss = 0.05511327\n",
      "Iteration 33, loss = 0.05528230\n",
      "Iteration 34, loss = 0.05214112\n",
      "Iteration 35, loss = 0.05033272\n",
      "Iteration 36, loss = 0.04879903\n",
      "Iteration 37, loss = 0.04738233\n",
      "Iteration 38, loss = 0.04609132\n",
      "Iteration 39, loss = 0.04485548\n",
      "Iteration 40, loss = 0.04368728\n",
      "Iteration 41, loss = 0.04260155\n",
      "Iteration 42, loss = 0.04155808\n",
      "Iteration 43, loss = 0.04058521\n",
      "Iteration 44, loss = 0.03966472\n",
      "Iteration 45, loss = 0.03878209\n",
      "Iteration 46, loss = 0.03791479\n",
      "Iteration 47, loss = 0.03713165\n",
      "Iteration 48, loss = 0.03636064\n",
      "Iteration 49, loss = 0.03566245\n",
      "Iteration 50, loss = 0.03501959\n",
      "Iteration 51, loss = 0.03431642\n",
      "Iteration 52, loss = 0.03369162\n",
      "Iteration 53, loss = 0.03310660\n",
      "Iteration 54, loss = 0.03254088\n",
      "Iteration 55, loss = 0.03198654\n",
      "Iteration 56, loss = 0.03150178\n",
      "Iteration 57, loss = 0.03097828\n",
      "Iteration 58, loss = 0.03051441\n",
      "Iteration 59, loss = 0.03007000\n",
      "Iteration 60, loss = 0.02963687\n",
      "Iteration 61, loss = 0.02921806\n",
      "Iteration 62, loss = 0.02884064\n",
      "Iteration 63, loss = 0.02844657\n",
      "Iteration 64, loss = 0.02808160\n",
      "Iteration 65, loss = 0.02775793\n",
      "Iteration 66, loss = 0.02739395\n",
      "Iteration 67, loss = 0.02712710\n",
      "Iteration 68, loss = 0.02677395\n",
      "Iteration 69, loss = 0.02645220\n",
      "Iteration 70, loss = 0.02616257\n",
      "Iteration 71, loss = 0.02588011\n",
      "Iteration 72, loss = 0.02552201\n",
      "Iteration 73, loss = 0.02517704\n",
      "Iteration 74, loss = 0.02488489\n",
      "Iteration 75, loss = 0.02456216\n",
      "Iteration 76, loss = 0.02425632\n",
      "Iteration 77, loss = 0.02399001\n",
      "Iteration 78, loss = 0.02374323\n",
      "Iteration 79, loss = 0.02351778\n",
      "Iteration 80, loss = 0.02323582\n",
      "Iteration 81, loss = 0.02303574\n",
      "Iteration 82, loss = 0.02280631\n",
      "Iteration 83, loss = 0.02245275\n",
      "Iteration 84, loss = 0.02211057\n",
      "Iteration 85, loss = 0.02181697\n",
      "Iteration 86, loss = 0.02152846\n",
      "Iteration 87, loss = 0.02125741\n",
      "Iteration 88, loss = 0.02102039\n",
      "Iteration 89, loss = 0.02066836\n",
      "Iteration 90, loss = 0.02030331\n",
      "Iteration 91, loss = 0.02003383\n",
      "Iteration 92, loss = 0.01966906\n",
      "Iteration 93, loss = 0.01918499\n",
      "Iteration 94, loss = 0.01863338\n",
      "Iteration 95, loss = 0.01825012\n",
      "Iteration 96, loss = 0.01775480\n",
      "Iteration 97, loss = 0.01757563\n",
      "Iteration 98, loss = 0.01718267\n",
      "Iteration 99, loss = 0.01694712\n",
      "Iteration 100, loss = 0.01670891\n",
      "Iteration 101, loss = 0.01649198\n",
      "Iteration 102, loss = 0.01630884\n",
      "Iteration 103, loss = 0.01611904\n",
      "Iteration 104, loss = 0.01587856\n",
      "Iteration 105, loss = 0.01571051\n",
      "Iteration 106, loss = 0.01544924\n",
      "Iteration 107, loss = 0.01527674\n",
      "Iteration 108, loss = 0.01508174\n",
      "Iteration 109, loss = 0.01495652\n",
      "Iteration 110, loss = 0.01481995\n",
      "Iteration 111, loss = 0.01475744\n",
      "Iteration 112, loss = 0.01458279\n",
      "Iteration 113, loss = 0.01447459\n",
      "Iteration 114, loss = 0.01442410\n",
      "Iteration 115, loss = 0.01427512\n",
      "Iteration 116, loss = 0.01416086\n",
      "Iteration 117, loss = 0.01400100\n",
      "Iteration 118, loss = 0.01392487\n",
      "Iteration 119, loss = 0.01378434\n",
      "Iteration 120, loss = 0.01359360\n",
      "Iteration 121, loss = 0.01342318\n",
      "Iteration 122, loss = 0.01331391\n",
      "Iteration 123, loss = 0.01315526\n",
      "Iteration 124, loss = 0.01331342\n",
      "Iteration 125, loss = 0.01299798\n",
      "Iteration 126, loss = 0.01288745\n",
      "Iteration 127, loss = 0.01277299\n",
      "Iteration 128, loss = 0.01265613\n",
      "Iteration 129, loss = 0.01258571\n",
      "Iteration 130, loss = 0.01250345\n",
      "Iteration 131, loss = 0.01241079\n",
      "Iteration 132, loss = 0.01236376\n",
      "Iteration 133, loss = 0.01226715\n",
      "Iteration 134, loss = 0.01217084\n",
      "Iteration 135, loss = 0.01208604\n",
      "Iteration 136, loss = 0.01197121\n",
      "Iteration 137, loss = 0.01187243\n",
      "Iteration 138, loss = 0.01176163\n",
      "Iteration 139, loss = 0.01172017\n",
      "Iteration 140, loss = 0.01150724\n",
      "Iteration 141, loss = 0.01140286\n",
      "Iteration 142, loss = 0.01123773\n",
      "Iteration 143, loss = 0.01110460\n",
      "Iteration 144, loss = 0.01098403\n",
      "Iteration 145, loss = 0.01241877\n",
      "Iteration 146, loss = 0.01096703\n",
      "Iteration 147, loss = 0.01084773\n",
      "Iteration 148, loss = 0.01076980\n",
      "Iteration 149, loss = 0.01072114\n",
      "Iteration 150, loss = 0.01064576\n",
      "Iteration 151, loss = 0.01058593\n",
      "Iteration 152, loss = 0.01054868\n",
      "Iteration 153, loss = 0.01053870\n",
      "Iteration 154, loss = 0.01046205\n",
      "Iteration 155, loss = 0.01044806\n",
      "Iteration 156, loss = 0.01039867\n",
      "Iteration 157, loss = 0.01040978\n",
      "Iteration 158, loss = 0.01031612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70288253\n",
      "Iteration 2, loss = 0.65181452\n",
      "Iteration 3, loss = 0.62820871\n",
      "Iteration 4, loss = 0.61164623\n",
      "Iteration 5, loss = 0.59310521\n",
      "Iteration 6, loss = 0.56694456\n",
      "Iteration 7, loss = 0.52983721\n",
      "Iteration 8, loss = 0.48064054\n",
      "Iteration 9, loss = 0.42139597\n",
      "Iteration 10, loss = 0.35834412\n",
      "Iteration 11, loss = 0.29866736\n",
      "Iteration 12, loss = 0.24758305\n",
      "Iteration 13, loss = 0.20645385\n",
      "Iteration 14, loss = 0.17453869\n",
      "Iteration 15, loss = 0.14996617\n",
      "Iteration 16, loss = 0.13098547\n",
      "Iteration 17, loss = 0.11624861\n",
      "Iteration 18, loss = 0.10448083\n",
      "Iteration 19, loss = 0.09509455\n",
      "Iteration 20, loss = 0.08737461\n",
      "Iteration 21, loss = 0.08098734\n",
      "Iteration 22, loss = 0.07547960\n",
      "Iteration 23, loss = 0.07081988\n",
      "Iteration 24, loss = 0.06681768\n",
      "Iteration 25, loss = 0.06330313\n",
      "Iteration 26, loss = 0.06016208\n",
      "Iteration 27, loss = 0.05727117\n",
      "Iteration 28, loss = 0.05493646\n",
      "Iteration 29, loss = 0.05266083\n",
      "Iteration 30, loss = 0.05061483\n",
      "Iteration 31, loss = 0.04882325\n",
      "Iteration 32, loss = 0.04714595\n",
      "Iteration 33, loss = 0.04569606\n",
      "Iteration 34, loss = 0.04435418\n",
      "Iteration 35, loss = 0.04305044\n",
      "Iteration 36, loss = 0.04195142\n",
      "Iteration 37, loss = 0.04094934\n",
      "Iteration 38, loss = 0.04004208\n",
      "Iteration 39, loss = 0.03918352\n",
      "Iteration 40, loss = 0.03828076\n",
      "Iteration 41, loss = 0.03749227\n",
      "Iteration 42, loss = 0.03680307\n",
      "Iteration 43, loss = 0.03605843\n",
      "Iteration 44, loss = 0.03538804\n",
      "Iteration 45, loss = 0.03474929\n",
      "Iteration 46, loss = 0.03416245\n",
      "Iteration 47, loss = 0.03368156\n",
      "Iteration 48, loss = 0.03319366\n",
      "Iteration 49, loss = 0.03277630\n",
      "Iteration 50, loss = 0.03225067\n",
      "Iteration 51, loss = 0.03184316\n",
      "Iteration 52, loss = 0.03134277\n",
      "Iteration 53, loss = 0.03100887\n",
      "Iteration 54, loss = 0.03053659\n",
      "Iteration 55, loss = 0.03018282\n",
      "Iteration 56, loss = 0.02986418\n",
      "Iteration 57, loss = 0.02957245\n",
      "Iteration 58, loss = 0.02923402\n",
      "Iteration 59, loss = 0.02897709\n",
      "Iteration 60, loss = 0.02874714\n",
      "Iteration 61, loss = 0.02848265\n",
      "Iteration 62, loss = 0.02823840\n",
      "Iteration 63, loss = 0.02803121\n",
      "Iteration 64, loss = 0.02771512\n",
      "Iteration 65, loss = 0.02747671\n",
      "Iteration 66, loss = 0.02722339\n",
      "Iteration 67, loss = 0.02700876\n",
      "Iteration 68, loss = 0.02675028\n",
      "Iteration 69, loss = 0.02660321\n",
      "Iteration 70, loss = 0.02635864\n",
      "Iteration 71, loss = 0.02611802\n",
      "Iteration 72, loss = 0.02593676\n",
      "Iteration 73, loss = 0.02566136\n",
      "Iteration 74, loss = 0.02536409\n",
      "Iteration 75, loss = 0.02505815\n",
      "Iteration 76, loss = 0.02469791\n",
      "Iteration 77, loss = 0.02412774\n",
      "Iteration 78, loss = 0.02371538\n",
      "Iteration 79, loss = 0.02320130\n",
      "Iteration 80, loss = 0.02278888\n",
      "Iteration 81, loss = 0.02255302\n",
      "Iteration 82, loss = 0.02217761\n",
      "Iteration 83, loss = 0.02182162\n",
      "Iteration 84, loss = 0.02140832\n",
      "Iteration 85, loss = 0.02094281\n",
      "Iteration 86, loss = 0.02052782\n",
      "Iteration 87, loss = 0.02002697\n",
      "Iteration 88, loss = 0.01983345\n",
      "Iteration 89, loss = 0.01955439\n",
      "Iteration 90, loss = 0.01920121\n",
      "Iteration 91, loss = 0.01891059\n",
      "Iteration 92, loss = 0.01871847\n",
      "Iteration 93, loss = 0.01834077\n",
      "Iteration 94, loss = 0.01802986\n",
      "Iteration 95, loss = 0.01773375\n",
      "Iteration 96, loss = 0.01753036\n",
      "Iteration 97, loss = 0.01729247\n",
      "Iteration 98, loss = 0.01709286\n",
      "Iteration 99, loss = 0.01689053\n",
      "Iteration 100, loss = 0.01670969\n",
      "Iteration 101, loss = 0.01634809\n",
      "Iteration 102, loss = 0.01615656\n",
      "Iteration 103, loss = 0.01602774\n",
      "Iteration 104, loss = 0.01582814\n",
      "Iteration 105, loss = 0.01572291\n",
      "Iteration 106, loss = 0.01561200\n",
      "Iteration 107, loss = 0.01551319\n",
      "Iteration 108, loss = 0.01536538\n",
      "Iteration 109, loss = 0.01528735\n",
      "Iteration 110, loss = 0.01517770\n",
      "Iteration 111, loss = 0.01504092\n",
      "Iteration 112, loss = 0.01500171\n",
      "Iteration 113, loss = 0.01483900\n",
      "Iteration 114, loss = 0.01473132\n",
      "Iteration 115, loss = 0.01458845\n",
      "Iteration 116, loss = 0.01450124\n",
      "Iteration 117, loss = 0.01442331\n",
      "Iteration 118, loss = 0.01437567\n",
      "Iteration 119, loss = 0.01431234\n",
      "Iteration 120, loss = 0.01428487\n",
      "Iteration 121, loss = 0.01423132\n",
      "Iteration 122, loss = 0.01419263\n",
      "Iteration 123, loss = 0.01416374\n",
      "Iteration 124, loss = 0.01412145\n",
      "Iteration 125, loss = 0.01410151\n",
      "Iteration 126, loss = 0.01403829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65298367\n",
      "Iteration 2, loss = 0.63375645\n",
      "Iteration 3, loss = 0.62433939\n",
      "Iteration 4, loss = 0.60923448\n",
      "Iteration 5, loss = 0.58112343\n",
      "Iteration 6, loss = 0.53179756\n",
      "Iteration 7, loss = 0.46004040\n",
      "Iteration 8, loss = 0.37909184\n",
      "Iteration 9, loss = 0.30428264\n",
      "Iteration 10, loss = 0.24425021\n",
      "Iteration 11, loss = 0.20042166\n",
      "Iteration 12, loss = 0.16792662\n",
      "Iteration 13, loss = 0.14382305\n",
      "Iteration 14, loss = 0.12581623\n",
      "Iteration 15, loss = 0.11157334\n",
      "Iteration 16, loss = 0.10048678\n",
      "Iteration 17, loss = 0.09125333\n",
      "Iteration 18, loss = 0.08376543\n",
      "Iteration 19, loss = 0.07752797\n",
      "Iteration 20, loss = 0.07236115\n",
      "Iteration 21, loss = 0.06781112\n",
      "Iteration 22, loss = 0.06379735\n",
      "Iteration 23, loss = 0.06023053\n",
      "Iteration 24, loss = 0.05718234\n",
      "Iteration 25, loss = 0.05441461\n",
      "Iteration 26, loss = 0.05199018\n",
      "Iteration 27, loss = 0.04975580\n",
      "Iteration 28, loss = 0.04771361\n",
      "Iteration 29, loss = 0.04595358\n",
      "Iteration 30, loss = 0.04433899\n",
      "Iteration 31, loss = 0.04270884\n",
      "Iteration 32, loss = 0.04129292\n",
      "Iteration 33, loss = 0.04001651\n",
      "Iteration 34, loss = 0.03892784\n",
      "Iteration 35, loss = 0.03775386\n",
      "Iteration 36, loss = 0.03669966\n",
      "Iteration 37, loss = 0.03587023\n",
      "Iteration 38, loss = 0.03563636\n",
      "Iteration 39, loss = 0.03420508\n",
      "Iteration 40, loss = 0.03356335\n",
      "Iteration 41, loss = 0.03283016\n",
      "Iteration 42, loss = 0.03218721\n",
      "Iteration 43, loss = 0.03171089\n",
      "Iteration 44, loss = 0.03108081\n",
      "Iteration 45, loss = 0.03061394\n",
      "Iteration 46, loss = 0.03005162\n",
      "Iteration 47, loss = 0.02954895\n",
      "Iteration 48, loss = 0.02916567\n",
      "Iteration 49, loss = 0.02865653\n",
      "Iteration 50, loss = 0.02823256\n",
      "Iteration 51, loss = 0.02786232\n",
      "Iteration 52, loss = 0.02742290\n",
      "Iteration 53, loss = 0.02699337\n",
      "Iteration 54, loss = 0.02658072\n",
      "Iteration 55, loss = 0.02608644\n",
      "Iteration 56, loss = 0.02562748\n",
      "Iteration 57, loss = 0.02512917\n",
      "Iteration 58, loss = 0.02465375\n",
      "Iteration 59, loss = 0.02423975\n",
      "Iteration 60, loss = 0.02373853\n",
      "Iteration 61, loss = 0.02334753\n",
      "Iteration 62, loss = 0.02280681\n",
      "Iteration 63, loss = 0.02227199\n",
      "Iteration 64, loss = 0.02172270\n",
      "Iteration 65, loss = 0.02132044\n",
      "Iteration 66, loss = 0.02087737\n",
      "Iteration 67, loss = 0.02051421\n",
      "Iteration 68, loss = 0.02009962\n",
      "Iteration 69, loss = 0.01978044\n",
      "Iteration 70, loss = 0.01961895\n",
      "Iteration 71, loss = 0.01927919\n",
      "Iteration 72, loss = 0.01898884\n",
      "Iteration 73, loss = 0.01863175\n",
      "Iteration 74, loss = 0.01849339\n",
      "Iteration 75, loss = 0.01821263\n",
      "Iteration 76, loss = 0.01793991\n",
      "Iteration 77, loss = 0.01775196\n",
      "Iteration 78, loss = 0.01762531\n",
      "Iteration 79, loss = 0.01743590\n",
      "Iteration 80, loss = 0.01718675\n",
      "Iteration 81, loss = 0.01702433\n",
      "Iteration 82, loss = 0.01684069\n",
      "Iteration 83, loss = 0.01667515\n",
      "Iteration 84, loss = 0.01636923\n",
      "Iteration 85, loss = 0.01606339\n",
      "Iteration 86, loss = 0.01615039\n",
      "Iteration 87, loss = 0.01564832\n",
      "Iteration 88, loss = 0.01534830\n",
      "Iteration 89, loss = 0.01520765\n",
      "Iteration 90, loss = 0.01495813\n",
      "Iteration 91, loss = 0.01488074\n",
      "Iteration 92, loss = 0.01475644\n",
      "Iteration 93, loss = 0.01454958\n",
      "Iteration 94, loss = 0.01441304\n",
      "Iteration 95, loss = 0.01432241\n",
      "Iteration 96, loss = 0.01416626\n",
      "Iteration 97, loss = 0.01406756\n",
      "Iteration 98, loss = 0.01394349\n",
      "Iteration 99, loss = 0.01381175\n",
      "Iteration 100, loss = 0.01371387\n",
      "Iteration 101, loss = 0.01355975\n",
      "Iteration 102, loss = 0.01344829\n",
      "Iteration 103, loss = 0.01335033\n",
      "Iteration 104, loss = 0.01323800\n",
      "Iteration 105, loss = 0.01308868\n",
      "Iteration 106, loss = 0.01302880\n",
      "Iteration 107, loss = 0.01284975\n",
      "Iteration 108, loss = 0.01272390\n",
      "Iteration 109, loss = 0.01277940\n",
      "Iteration 110, loss = 0.01264237\n",
      "Iteration 111, loss = 0.01257647\n",
      "Iteration 112, loss = 0.01249937\n",
      "Iteration 113, loss = 0.01247805\n",
      "Iteration 114, loss = 0.01235983\n",
      "Iteration 115, loss = 0.01227583\n",
      "Iteration 116, loss = 0.01224441\n",
      "Iteration 117, loss = 0.01214338\n",
      "Iteration 118, loss = 0.01202807\n",
      "Iteration 119, loss = 0.01198837\n",
      "Iteration 120, loss = 0.01186704\n",
      "Iteration 121, loss = 0.01184722\n",
      "Iteration 122, loss = 0.01178589\n",
      "Iteration 123, loss = 0.01174378\n",
      "Iteration 124, loss = 0.01168971\n",
      "Iteration 125, loss = 0.01162772\n",
      "Iteration 126, loss = 0.01154401\n",
      "Iteration 127, loss = 0.01149903\n",
      "Iteration 128, loss = 0.01132606\n",
      "Iteration 129, loss = 0.01129795\n",
      "Iteration 130, loss = 0.01115115\n",
      "Iteration 131, loss = 0.01107994\n",
      "Iteration 132, loss = 0.01098173\n",
      "Iteration 133, loss = 0.01090517\n",
      "Iteration 134, loss = 0.01078165\n",
      "Iteration 135, loss = 0.01073542\n",
      "Iteration 136, loss = 0.01070322\n",
      "Iteration 137, loss = 0.01060712\n",
      "Iteration 138, loss = 0.01059411\n",
      "Iteration 139, loss = 0.01056692\n",
      "Iteration 140, loss = 0.01052947\n",
      "Iteration 141, loss = 0.01049490\n",
      "Iteration 142, loss = 0.01046285\n",
      "Iteration 143, loss = 0.01047487\n",
      "Iteration 144, loss = 0.01044600\n",
      "Iteration 145, loss = 0.01043535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64017672\n",
      "Iteration 2, loss = 0.63195422\n",
      "Iteration 3, loss = 0.62417964\n",
      "Iteration 4, loss = 0.61076353\n",
      "Iteration 5, loss = 0.58880534\n",
      "Iteration 6, loss = 0.55744227\n",
      "Iteration 7, loss = 0.51801858\n",
      "Iteration 8, loss = 0.47306992\n",
      "Iteration 9, loss = 0.42493706\n",
      "Iteration 10, loss = 0.37610122\n",
      "Iteration 11, loss = 0.32979934\n",
      "Iteration 12, loss = 0.28878866\n",
      "Iteration 13, loss = 0.25292497\n",
      "Iteration 14, loss = 0.22205250\n",
      "Iteration 15, loss = 0.19565141\n",
      "Iteration 16, loss = 0.17361932\n",
      "Iteration 17, loss = 0.15552045\n",
      "Iteration 18, loss = 0.14051930\n",
      "Iteration 19, loss = 0.12779746\n",
      "Iteration 20, loss = 0.11686303\n",
      "Iteration 21, loss = 0.10768831\n",
      "Iteration 22, loss = 0.09987297\n",
      "Iteration 23, loss = 0.09311885\n",
      "Iteration 24, loss = 0.08695153\n",
      "Iteration 25, loss = 0.08184720\n",
      "Iteration 26, loss = 0.07717408\n",
      "Iteration 27, loss = 0.07315158\n",
      "Iteration 28, loss = 0.07049650\n",
      "Iteration 29, loss = 0.06747519\n",
      "Iteration 30, loss = 0.06474405\n",
      "Iteration 31, loss = 0.06217920\n",
      "Iteration 32, loss = 0.05969337\n",
      "Iteration 33, loss = 0.05752299\n",
      "Iteration 34, loss = 0.05547022\n",
      "Iteration 35, loss = 0.05351644\n",
      "Iteration 36, loss = 0.05191563\n",
      "Iteration 37, loss = 0.05021353\n",
      "Iteration 38, loss = 0.04878821\n",
      "Iteration 39, loss = 0.04736369\n",
      "Iteration 40, loss = 0.04607390\n",
      "Iteration 41, loss = 0.04477773\n",
      "Iteration 42, loss = 0.04362047\n",
      "Iteration 43, loss = 0.04263153\n",
      "Iteration 44, loss = 0.04165253\n",
      "Iteration 45, loss = 0.04063505\n",
      "Iteration 46, loss = 0.03968804\n",
      "Iteration 47, loss = 0.03873764\n",
      "Iteration 48, loss = 0.03812010\n",
      "Iteration 49, loss = 0.03724784\n",
      "Iteration 50, loss = 0.03649371\n",
      "Iteration 51, loss = 0.03575631\n",
      "Iteration 52, loss = 0.03509531\n",
      "Iteration 53, loss = 0.03458867\n",
      "Iteration 54, loss = 0.03390483\n",
      "Iteration 55, loss = 0.03325710\n",
      "Iteration 56, loss = 0.03274109\n",
      "Iteration 57, loss = 0.03222174\n",
      "Iteration 58, loss = 0.03169790\n",
      "Iteration 59, loss = 0.03128508\n",
      "Iteration 60, loss = 0.03067612\n",
      "Iteration 61, loss = 0.03031568\n",
      "Iteration 62, loss = 0.02971533\n",
      "Iteration 63, loss = 0.02934723\n",
      "Iteration 64, loss = 0.02876617\n",
      "Iteration 65, loss = 0.02824090\n",
      "Iteration 66, loss = 0.02789829\n",
      "Iteration 67, loss = 0.02721042\n",
      "Iteration 68, loss = 0.02680999\n",
      "Iteration 69, loss = 0.02619086\n",
      "Iteration 70, loss = 0.02565283\n",
      "Iteration 71, loss = 0.02513210\n",
      "Iteration 72, loss = 0.02447336\n",
      "Iteration 73, loss = 0.02399960\n",
      "Iteration 74, loss = 0.02370604\n",
      "Iteration 75, loss = 0.02315756\n",
      "Iteration 76, loss = 0.02288850\n",
      "Iteration 77, loss = 0.02229585\n",
      "Iteration 78, loss = 0.02199498\n",
      "Iteration 79, loss = 0.02154787\n",
      "Iteration 80, loss = 0.02141795\n",
      "Iteration 81, loss = 0.02097030\n",
      "Iteration 82, loss = 0.02090825\n",
      "Iteration 83, loss = 0.02058527\n",
      "Iteration 84, loss = 0.02036441\n",
      "Iteration 85, loss = 0.02011982\n",
      "Iteration 86, loss = 0.01979585\n",
      "Iteration 87, loss = 0.01965558\n",
      "Iteration 88, loss = 0.01945173\n",
      "Iteration 89, loss = 0.01921007\n",
      "Iteration 90, loss = 0.01902433\n",
      "Iteration 91, loss = 0.01880274\n",
      "Iteration 92, loss = 0.01869063\n",
      "Iteration 93, loss = 0.01841909\n",
      "Iteration 94, loss = 0.01817542\n",
      "Iteration 95, loss = 0.01800622\n",
      "Iteration 96, loss = 0.01779771\n",
      "Iteration 97, loss = 0.01759037\n",
      "Iteration 98, loss = 0.01734826\n",
      "Iteration 99, loss = 0.01727814\n",
      "Iteration 100, loss = 0.01705922\n",
      "Iteration 101, loss = 0.01687479\n",
      "Iteration 102, loss = 0.01676448\n",
      "Iteration 103, loss = 0.01667252\n",
      "Iteration 104, loss = 0.01654762\n",
      "Iteration 105, loss = 0.01639734\n",
      "Iteration 106, loss = 0.01625324\n",
      "Iteration 107, loss = 0.01620480\n",
      "Iteration 108, loss = 0.01603455\n",
      "Iteration 109, loss = 0.01590779\n",
      "Iteration 110, loss = 0.01578153\n",
      "Iteration 111, loss = 0.01570272\n",
      "Iteration 112, loss = 0.01558883\n",
      "Iteration 113, loss = 0.01552591\n",
      "Iteration 114, loss = 0.01547164\n",
      "Iteration 115, loss = 0.01537723\n",
      "Iteration 116, loss = 0.01531531\n",
      "Iteration 117, loss = 0.01524851\n",
      "Iteration 118, loss = 0.01519138\n",
      "Iteration 119, loss = 0.01513271\n",
      "Iteration 120, loss = 0.01510635\n",
      "Iteration 121, loss = 0.01504434\n",
      "Iteration 122, loss = 0.01501967\n",
      "Iteration 123, loss = 0.01496445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63607108\n",
      "Iteration 2, loss = 0.63265315\n",
      "Iteration 3, loss = 0.62280049\n",
      "Iteration 4, loss = 0.59835999\n",
      "Iteration 5, loss = 0.55418179\n",
      "Iteration 6, loss = 0.49171213\n",
      "Iteration 7, loss = 0.42073802\n",
      "Iteration 8, loss = 0.35501339\n",
      "Iteration 9, loss = 0.29968885\n",
      "Iteration 10, loss = 0.25599104\n",
      "Iteration 11, loss = 0.22202130\n",
      "Iteration 12, loss = 0.19554187\n",
      "Iteration 13, loss = 0.17441442\n",
      "Iteration 14, loss = 0.15721734\n",
      "Iteration 15, loss = 0.14310932\n",
      "Iteration 16, loss = 0.13136450\n",
      "Iteration 17, loss = 0.12172615\n",
      "Iteration 18, loss = 0.11367992\n",
      "Iteration 19, loss = 0.10652246\n",
      "Iteration 20, loss = 0.10026803\n",
      "Iteration 21, loss = 0.09459459\n",
      "Iteration 22, loss = 0.08961234\n",
      "Iteration 23, loss = 0.08521262\n",
      "Iteration 24, loss = 0.08132565\n",
      "Iteration 25, loss = 0.07752988\n",
      "Iteration 26, loss = 0.07420000\n",
      "Iteration 27, loss = 0.07101138\n",
      "Iteration 28, loss = 0.06821168\n",
      "Iteration 29, loss = 0.06552397\n",
      "Iteration 30, loss = 0.06313309\n",
      "Iteration 31, loss = 0.06078829\n",
      "Iteration 32, loss = 0.05860816\n",
      "Iteration 33, loss = 0.05658753\n",
      "Iteration 34, loss = 0.05477917\n",
      "Iteration 35, loss = 0.05292495\n",
      "Iteration 36, loss = 0.05129234\n",
      "Iteration 37, loss = 0.04958051\n",
      "Iteration 38, loss = 0.04803300\n",
      "Iteration 39, loss = 0.04650111\n",
      "Iteration 40, loss = 0.04512730\n",
      "Iteration 41, loss = 0.04384827\n",
      "Iteration 42, loss = 0.04254695\n",
      "Iteration 43, loss = 0.04140338\n",
      "Iteration 44, loss = 0.04008664\n",
      "Iteration 45, loss = 0.03887601\n",
      "Iteration 46, loss = 0.03783400\n",
      "Iteration 47, loss = 0.03682309\n",
      "Iteration 48, loss = 0.03569108\n",
      "Iteration 49, loss = 0.03465992\n",
      "Iteration 50, loss = 0.03352509\n",
      "Iteration 51, loss = 0.03245014\n",
      "Iteration 52, loss = 0.03149500\n",
      "Iteration 53, loss = 0.03060969\n",
      "Iteration 54, loss = 0.03004630\n",
      "Iteration 55, loss = 0.02919354\n",
      "Iteration 56, loss = 0.02842797\n",
      "Iteration 57, loss = 0.02775269\n",
      "Iteration 58, loss = 0.02713823\n",
      "Iteration 59, loss = 0.02646030\n",
      "Iteration 60, loss = 0.02589753\n",
      "Iteration 61, loss = 0.02538048\n",
      "Iteration 62, loss = 0.02474860\n",
      "Iteration 63, loss = 0.02419543\n",
      "Iteration 64, loss = 0.02360388\n",
      "Iteration 65, loss = 0.02311570\n",
      "Iteration 66, loss = 0.02267816\n",
      "Iteration 67, loss = 0.02237799\n",
      "Iteration 68, loss = 0.02197572\n",
      "Iteration 69, loss = 0.02141948\n",
      "Iteration 70, loss = 0.02110095\n",
      "Iteration 71, loss = 0.02079601\n",
      "Iteration 72, loss = 0.02031013\n",
      "Iteration 73, loss = 0.01997584\n",
      "Iteration 74, loss = 0.01946436\n",
      "Iteration 75, loss = 0.01911584\n",
      "Iteration 76, loss = 0.01877480\n",
      "Iteration 77, loss = 0.01842916\n",
      "Iteration 78, loss = 0.01807298\n",
      "Iteration 79, loss = 0.01786098\n",
      "Iteration 80, loss = 0.01746085\n",
      "Iteration 81, loss = 0.01716062\n",
      "Iteration 82, loss = 0.01689355\n",
      "Iteration 83, loss = 0.01832557\n",
      "Iteration 84, loss = 0.01672765\n",
      "Iteration 85, loss = 0.01640519\n",
      "Iteration 86, loss = 0.01617059\n",
      "Iteration 87, loss = 0.01590971\n",
      "Iteration 88, loss = 0.01572703\n",
      "Iteration 89, loss = 0.01551245\n",
      "Iteration 90, loss = 0.01535929\n",
      "Iteration 91, loss = 0.01513460\n",
      "Iteration 92, loss = 0.01500725\n",
      "Iteration 93, loss = 0.01479035\n",
      "Iteration 94, loss = 0.01463084\n",
      "Iteration 95, loss = 0.01450984\n",
      "Iteration 96, loss = 0.01433485\n",
      "Iteration 97, loss = 0.01418008\n",
      "Iteration 98, loss = 0.01407102\n",
      "Iteration 99, loss = 0.01389510\n",
      "Iteration 100, loss = 0.01380206\n",
      "Iteration 101, loss = 0.01360445\n",
      "Iteration 102, loss = 0.01344877\n",
      "Iteration 103, loss = 0.01329395\n",
      "Iteration 104, loss = 0.01314587\n",
      "Iteration 105, loss = 0.01311442\n",
      "Iteration 106, loss = 0.01287982\n",
      "Iteration 107, loss = 0.01281904\n",
      "Iteration 108, loss = 0.01265405\n",
      "Iteration 109, loss = 0.01253095\n",
      "Iteration 110, loss = 0.01241289\n",
      "Iteration 111, loss = 0.01231989\n",
      "Iteration 112, loss = 0.01211720\n",
      "Iteration 113, loss = 0.01203876\n",
      "Iteration 114, loss = 0.01185809\n",
      "Iteration 115, loss = 0.01172248\n",
      "Iteration 116, loss = 0.01151571\n",
      "Iteration 117, loss = 0.01137955\n",
      "Iteration 118, loss = 0.01122116\n",
      "Iteration 119, loss = 0.01110296\n",
      "Iteration 120, loss = 0.01108011\n",
      "Iteration 121, loss = 0.01090506\n",
      "Iteration 122, loss = 0.01081381\n",
      "Iteration 123, loss = 0.01072387\n",
      "Iteration 124, loss = 0.01065051\n",
      "Iteration 125, loss = 0.01055604\n",
      "Iteration 126, loss = 0.01125606\n",
      "Iteration 127, loss = 0.01054693\n",
      "Iteration 128, loss = 0.01046181\n",
      "Iteration 129, loss = 0.01039558\n",
      "Iteration 130, loss = 0.01034624\n",
      "Iteration 131, loss = 0.01030089\n",
      "Iteration 132, loss = 0.01028786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64057844\n",
      "Iteration 2, loss = 0.63440633\n",
      "Iteration 3, loss = 0.62736702\n",
      "Iteration 4, loss = 0.60693359\n",
      "Iteration 5, loss = 0.55922139\n",
      "Iteration 6, loss = 0.48126023\n",
      "Iteration 7, loss = 0.39210971\n",
      "Iteration 8, loss = 0.31410498\n",
      "Iteration 9, loss = 0.25422838\n",
      "Iteration 10, loss = 0.20954225\n",
      "Iteration 11, loss = 0.17713903\n",
      "Iteration 12, loss = 0.15233780\n",
      "Iteration 13, loss = 0.13328631\n",
      "Iteration 14, loss = 0.11889467\n",
      "Iteration 15, loss = 0.10710091\n",
      "Iteration 16, loss = 0.09752533\n",
      "Iteration 17, loss = 0.08955316\n",
      "Iteration 18, loss = 0.08279503\n",
      "Iteration 19, loss = 0.07683213\n",
      "Iteration 20, loss = 0.07175542\n",
      "Iteration 21, loss = 0.06739209\n",
      "Iteration 22, loss = 0.06351205\n",
      "Iteration 23, loss = 0.05993878\n",
      "Iteration 24, loss = 0.05671018\n",
      "Iteration 25, loss = 0.05388570\n",
      "Iteration 26, loss = 0.05133841\n",
      "Iteration 27, loss = 0.04900581\n",
      "Iteration 28, loss = 0.04706876\n",
      "Iteration 29, loss = 0.04498580\n",
      "Iteration 30, loss = 0.04314749\n",
      "Iteration 31, loss = 0.04155608\n",
      "Iteration 32, loss = 0.04011455\n",
      "Iteration 33, loss = 0.03880799\n",
      "Iteration 34, loss = 0.03749501\n",
      "Iteration 35, loss = 0.03630904\n",
      "Iteration 36, loss = 0.03516139\n",
      "Iteration 37, loss = 0.03404514\n",
      "Iteration 38, loss = 0.03300969\n",
      "Iteration 39, loss = 0.03207192\n",
      "Iteration 40, loss = 0.03104935\n",
      "Iteration 41, loss = 0.03032359\n",
      "Iteration 42, loss = 0.02940343\n",
      "Iteration 43, loss = 0.02853509\n",
      "Iteration 44, loss = 0.02804532\n",
      "Iteration 45, loss = 0.02719940\n",
      "Iteration 46, loss = 0.02666252\n",
      "Iteration 47, loss = 0.02609080\n",
      "Iteration 48, loss = 0.02548858\n",
      "Iteration 49, loss = 0.02498833\n",
      "Iteration 50, loss = 0.02446109\n",
      "Iteration 51, loss = 0.02399095\n",
      "Iteration 52, loss = 0.02340858\n",
      "Iteration 53, loss = 0.02282786\n",
      "Iteration 54, loss = 0.02222416\n",
      "Iteration 55, loss = 0.02165074\n",
      "Iteration 56, loss = 0.02120206\n",
      "Iteration 57, loss = 0.02079830\n",
      "Iteration 58, loss = 0.02037183\n",
      "Iteration 59, loss = 0.02006789\n",
      "Iteration 60, loss = 0.01960300\n",
      "Iteration 61, loss = 0.01922976\n",
      "Iteration 62, loss = 0.01876108\n",
      "Iteration 63, loss = 0.01849933\n",
      "Iteration 64, loss = 0.01805810\n",
      "Iteration 65, loss = 0.01770684\n",
      "Iteration 66, loss = 0.01738390\n",
      "Iteration 67, loss = 0.01694527\n",
      "Iteration 68, loss = 0.01672465\n",
      "Iteration 69, loss = 0.01640926\n",
      "Iteration 70, loss = 0.01615350\n",
      "Iteration 71, loss = 0.01595110\n",
      "Iteration 72, loss = 0.01569750\n",
      "Iteration 73, loss = 0.01555840\n",
      "Iteration 74, loss = 0.01529740\n",
      "Iteration 75, loss = 0.01515446\n",
      "Iteration 76, loss = 0.01496484\n",
      "Iteration 77, loss = 0.01456502\n",
      "Iteration 78, loss = 0.01432240\n",
      "Iteration 79, loss = 0.01406917\n",
      "Iteration 80, loss = 0.01383665\n",
      "Iteration 81, loss = 0.01356346\n",
      "Iteration 82, loss = 0.01339921\n",
      "Iteration 83, loss = 0.01317439\n",
      "Iteration 84, loss = 0.01306087\n",
      "Iteration 85, loss = 0.01291543\n",
      "Iteration 86, loss = 0.01278561\n",
      "Iteration 87, loss = 0.01266275\n",
      "Iteration 88, loss = 0.01261249\n",
      "Iteration 89, loss = 0.01244684\n",
      "Iteration 90, loss = 0.01235832\n",
      "Iteration 91, loss = 0.01225693\n",
      "Iteration 92, loss = 0.01220080\n",
      "Iteration 93, loss = 0.01208120\n",
      "Iteration 94, loss = 0.01198441\n",
      "Iteration 95, loss = 0.01183735\n",
      "Iteration 96, loss = 0.01169152\n",
      "Iteration 97, loss = 0.01155504\n",
      "Iteration 98, loss = 0.01140319\n",
      "Iteration 99, loss = 0.01126457\n",
      "Iteration 100, loss = 0.01114435\n",
      "Iteration 101, loss = 0.01106130\n",
      "Iteration 102, loss = 0.01100367\n",
      "Iteration 103, loss = 0.01087114\n",
      "Iteration 104, loss = 0.01076851\n",
      "Iteration 105, loss = 0.01064757\n",
      "Iteration 106, loss = 0.01051171\n",
      "Iteration 107, loss = 0.01041162\n",
      "Iteration 108, loss = 0.01034204\n",
      "Iteration 109, loss = 0.01029751\n",
      "Iteration 110, loss = 0.01023016\n",
      "Iteration 111, loss = 0.01021953\n",
      "Iteration 112, loss = 0.01013555\n",
      "Iteration 113, loss = 0.01008906\n",
      "Iteration 114, loss = 0.01002783\n",
      "Iteration 115, loss = 0.01007240\n",
      "Iteration 116, loss = 0.00995417\n",
      "Iteration 117, loss = 0.00986284\n",
      "Iteration 118, loss = 0.00983993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82834424\n",
      "Iteration 2, loss = 0.54818533\n",
      "Iteration 3, loss = 0.19576224\n",
      "Iteration 4, loss = 0.07968467\n",
      "Iteration 5, loss = 0.05564286\n",
      "Iteration 6, loss = 0.04582652\n",
      "Iteration 7, loss = 0.04048700\n",
      "Iteration 8, loss = 0.03638713\n",
      "Iteration 9, loss = 0.03315204\n",
      "Iteration 10, loss = 0.03068311\n",
      "Iteration 11, loss = 0.02965148\n",
      "Iteration 12, loss = 0.02698291\n",
      "Iteration 13, loss = 0.02513850\n",
      "Iteration 14, loss = 0.02354570\n",
      "Iteration 15, loss = 0.02242623\n",
      "Iteration 16, loss = 0.02071067\n",
      "Iteration 17, loss = 0.01940260\n",
      "Iteration 18, loss = 0.01833007\n",
      "Iteration 19, loss = 0.01709925\n",
      "Iteration 20, loss = 0.01556967\n",
      "Iteration 21, loss = 0.01473088\n",
      "Iteration 22, loss = 0.01400452\n",
      "Iteration 23, loss = 0.01318295\n",
      "Iteration 24, loss = 0.01236229\n",
      "Iteration 25, loss = 0.01217792\n",
      "Iteration 26, loss = 0.01110046\n",
      "Iteration 27, loss = 0.01069463\n",
      "Iteration 28, loss = 0.01018427\n",
      "Iteration 29, loss = 0.00971436\n",
      "Iteration 30, loss = 0.00935254\n",
      "Iteration 31, loss = 0.00911398\n",
      "Iteration 32, loss = 0.00865222\n",
      "Iteration 33, loss = 0.00834812\n",
      "Iteration 34, loss = 0.00800678\n",
      "Iteration 35, loss = 0.00762705\n",
      "Iteration 36, loss = 0.00739711\n",
      "Iteration 37, loss = 0.00704404\n",
      "Iteration 38, loss = 0.00677181\n",
      "Iteration 39, loss = 0.00650826\n",
      "Iteration 40, loss = 0.00636976\n",
      "Iteration 41, loss = 0.00622636\n",
      "Iteration 42, loss = 0.00608849\n",
      "Iteration 43, loss = 0.00593050\n",
      "Iteration 44, loss = 0.00580879\n",
      "Iteration 45, loss = 0.00562479\n",
      "Iteration 46, loss = 0.00545521\n",
      "Iteration 47, loss = 0.00532574\n",
      "Iteration 48, loss = 0.00520091\n",
      "Iteration 49, loss = 0.00509312\n",
      "Iteration 50, loss = 0.00488854\n",
      "Iteration 51, loss = 0.00464093\n",
      "Iteration 52, loss = 0.00417893\n",
      "Iteration 53, loss = 0.00385280\n",
      "Iteration 54, loss = 0.00370085\n",
      "Iteration 55, loss = 0.00318600\n",
      "Iteration 56, loss = 0.00295616\n",
      "Iteration 57, loss = 0.00265448\n",
      "Iteration 58, loss = 0.00252995\n",
      "Iteration 59, loss = 0.00233517\n",
      "Iteration 60, loss = 0.00270247\n",
      "Iteration 61, loss = 0.00222553\n",
      "Iteration 62, loss = 0.00160486\n",
      "Iteration 63, loss = 0.00133123\n",
      "Iteration 64, loss = 0.00129130\n",
      "Iteration 65, loss = 0.00109482\n",
      "Iteration 66, loss = 0.00097782\n",
      "Iteration 67, loss = 0.00086135\n",
      "Iteration 68, loss = 0.00081014\n",
      "Iteration 69, loss = 0.00075391\n",
      "Iteration 70, loss = 0.00070263\n",
      "Iteration 71, loss = 0.00063402\n",
      "Iteration 72, loss = 0.00049491\n",
      "Iteration 73, loss = 0.00047173\n",
      "Iteration 74, loss = 0.00044517\n",
      "Iteration 75, loss = 0.00042472\n",
      "Iteration 76, loss = 0.00040915\n",
      "Iteration 77, loss = 0.00038863\n",
      "Iteration 78, loss = 0.00038231\n",
      "Iteration 79, loss = 0.00028812\n",
      "Iteration 80, loss = 0.00020386\n",
      "Iteration 81, loss = 0.00018087\n",
      "Iteration 82, loss = 0.00017235\n",
      "Iteration 83, loss = 0.00016354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=MLPClassifier(activation=&#x27;logistic&#x27;,\n",
       "                                           hidden_layer_sizes=(7, 2, 4),\n",
       "                                           verbose=True),\n",
       "                   param_distributions={&#x27;activation&#x27;: (&#x27;logistic&#x27;, &#x27;relu&#x27;),\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(2, 2, 2),\n",
       "                                                               (2, 2, 3),\n",
       "                                                               (2, 2, 4),\n",
       "                                                               (2, 2, 5),\n",
       "                                                               (2, 2, 6),\n",
       "                                                               (2, 2, 7),\n",
       "                                                               (2, 2, 8),\n",
       "                                                               (2, 2, 9),\n",
       "                                                               (2, 3, 2),\n",
       "                                                               (2, 3, 3),\n",
       "                                                               (2, 3, 4),\n",
       "                                                               (2, 3, 5),\n",
       "                                                               (2, 3, 6),\n",
       "                                                               (2, 3, 7),\n",
       "                                                               (2, 3, 8),\n",
       "                                                               (2, 3, 9),\n",
       "                                                               (2, 4, 2),\n",
       "                                                               (2, 4, 3),\n",
       "                                                               (2, 4, 4),\n",
       "                                                               (2, 4, 5),\n",
       "                                                               (2, 4, 6),\n",
       "                                                               (2, 4, 7),\n",
       "                                                               (2, 4, 8),\n",
       "                                                               (2, 4, 9),\n",
       "                                                               (2, 5, 2),\n",
       "                                                               (2, 5, 3),\n",
       "                                                               (2, 5, 4),\n",
       "                                                               (2, 5, 5),\n",
       "                                                               (2, 5, 6),\n",
       "                                                               (2, 5, 7), ...]},\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=MLPClassifier(activation=&#x27;logistic&#x27;,\n",
       "                                           hidden_layer_sizes=(7, 2, 4),\n",
       "                                           verbose=True),\n",
       "                   param_distributions={&#x27;activation&#x27;: (&#x27;logistic&#x27;, &#x27;relu&#x27;),\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(2, 2, 2),\n",
       "                                                               (2, 2, 3),\n",
       "                                                               (2, 2, 4),\n",
       "                                                               (2, 2, 5),\n",
       "                                                               (2, 2, 6),\n",
       "                                                               (2, 2, 7),\n",
       "                                                               (2, 2, 8),\n",
       "                                                               (2, 2, 9),\n",
       "                                                               (2, 3, 2),\n",
       "                                                               (2, 3, 3),\n",
       "                                                               (2, 3, 4),\n",
       "                                                               (2, 3, 5),\n",
       "                                                               (2, 3, 6),\n",
       "                                                               (2, 3, 7),\n",
       "                                                               (2, 3, 8),\n",
       "                                                               (2, 3, 9),\n",
       "                                                               (2, 4, 2),\n",
       "                                                               (2, 4, 3),\n",
       "                                                               (2, 4, 4),\n",
       "                                                               (2, 4, 5),\n",
       "                                                               (2, 4, 6),\n",
       "                                                               (2, 4, 7),\n",
       "                                                               (2, 4, 8),\n",
       "                                                               (2, 4, 9),\n",
       "                                                               (2, 5, 2),\n",
       "                                                               (2, 5, 3),\n",
       "                                                               (2, 5, 4),\n",
       "                                                               (2, 5, 5),\n",
       "                                                               (2, 5, 6),\n",
       "                                                               (2, 5, 7), ...]},\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(7, 2, 4), verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(7, 2, 4), verbose=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=MLPClassifier(activation='logistic',\n",
       "                                           hidden_layer_sizes=(7, 2, 4),\n",
       "                                           verbose=True),\n",
       "                   param_distributions={'activation': ('logistic', 'relu'),\n",
       "                                        'hidden_layer_sizes': [(2, 2, 2),\n",
       "                                                               (2, 2, 3),\n",
       "                                                               (2, 2, 4),\n",
       "                                                               (2, 2, 5),\n",
       "                                                               (2, 2, 6),\n",
       "                                                               (2, 2, 7),\n",
       "                                                               (2, 2, 8),\n",
       "                                                               (2, 2, 9),\n",
       "                                                               (2, 3, 2),\n",
       "                                                               (2, 3, 3),\n",
       "                                                               (2, 3, 4),\n",
       "                                                               (2, 3, 5),\n",
       "                                                               (2, 3, 6),\n",
       "                                                               (2, 3, 7),\n",
       "                                                               (2, 3, 8),\n",
       "                                                               (2, 3, 9),\n",
       "                                                               (2, 4, 2),\n",
       "                                                               (2, 4, 3),\n",
       "                                                               (2, 4, 4),\n",
       "                                                               (2, 4, 5),\n",
       "                                                               (2, 4, 6),\n",
       "                                                               (2, 4, 7),\n",
       "                                                               (2, 4, 8),\n",
       "                                                               (2, 4, 9),\n",
       "                                                               (2, 5, 2),\n",
       "                                                               (2, 5, 3),\n",
       "                                                               (2, 5, 4),\n",
       "                                                               (2, 5, 5),\n",
       "                                                               (2, 5, 6),\n",
       "                                                               (2, 5, 7), ...]},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048967398363424\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             3783         2251\n",
      "Cazyme                   21         1644\n",
      "negative accuracy=  0.745111037454425\n",
      "positive accuracy=  0.9735735735735735\n"
     ]
    }
   ],
   "source": [
    "y_pred_opt = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred_opt))\n",
    "mat_opt = confusion_matrix(y_test, y_pred_opt)\n",
    "cfmat_df_opt = pd.DataFrame(np.array(mat_opt))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df_opt.index, cfmat_df_opt.columns = index_, columns_\n",
    "print(cfmat_df_opt)\n",
    "\n",
    "print('negative accuracy= ', cfmat_df.iloc[0,0]/sum(cfmat_df.iloc[0,:]))\n",
    "print('positive accuracy= ', cfmat_df.iloc[1,1]/sum(cfmat_df.iloc[1,:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"optimal\" 3-layer mlp sometimes gives worse overall accuracy but similar or better AUC score [why?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (8, 9, 6), 'activation': 'relu'}\n",
      "{'mean_fit_time': array([15.25779605, 15.88552618, 20.55743203, 20.26331162, 10.14728656,\n",
      "       17.81804085, 10.57677169, 11.48586574, 18.31916995, 16.91030555]), 'std_fit_time': array([4.49488095, 3.59790777, 3.14122891, 2.37620479, 2.62328691,\n",
      "       2.05942586, 1.52928489, 1.95812432, 2.79479782, 1.19921401]), 'mean_score_time': array([0.02553639, 0.02201395, 0.02324996, 0.02537608, 0.0202888 ,\n",
      "       0.02220945, 0.03037386, 0.02310562, 0.02113423, 0.02177558]), 'std_score_time': array([0.00916084, 0.00215645, 0.00366868, 0.00309977, 0.00365238,\n",
      "       0.00192138, 0.01311827, 0.00084643, 0.00357781, 0.00204612]), 'param_hidden_layer_sizes': masked_array(data=[(8, 5, 4), (2, 4, 2), (8, 6, 8), (4, 9, 7), (8, 9, 6),\n",
      "                   (2, 3, 8), (8, 7, 6), (3, 6, 7), (3, 7, 6), (7, 7, 7)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_activation': masked_array(data=['relu', 'relu', 'logistic', 'logistic', 'relu',\n",
      "                   'logistic', 'relu', 'relu', 'logistic', 'logistic'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (8, 5, 4), 'activation': 'relu'}, {'hidden_layer_sizes': (2, 4, 2), 'activation': 'relu'}, {'hidden_layer_sizes': (8, 6, 8), 'activation': 'logistic'}, {'hidden_layer_sizes': (4, 9, 7), 'activation': 'logistic'}, {'hidden_layer_sizes': (8, 9, 6), 'activation': 'relu'}, {'hidden_layer_sizes': (2, 3, 8), 'activation': 'logistic'}, {'hidden_layer_sizes': (8, 7, 6), 'activation': 'relu'}, {'hidden_layer_sizes': (3, 6, 7), 'activation': 'relu'}, {'hidden_layer_sizes': (3, 7, 6), 'activation': 'logistic'}, {'hidden_layer_sizes': (7, 7, 7), 'activation': 'logistic'}], 'split0_test_score': array([0.98567144, 0.98833722, 0.988004  , 0.98867044, 0.98667111,\n",
      "       0.98867044, 0.98767078, 0.988004  , 0.98967011, 0.98867044]), 'split1_test_score': array([0.98333333, 0.98533333, 0.985     , 0.98233333, 0.985     ,\n",
      "       0.98333333, 0.984     , 0.98266667, 0.98433333, 0.98433333]), 'split2_test_score': array([0.992     , 0.99333333, 0.99366667, 0.99333333, 0.99433333,\n",
      "       0.993     , 0.99266667, 0.99266667, 0.99133333, 0.993     ]), 'split3_test_score': array([0.99166667, 0.66666667, 0.98933333, 0.98833333, 0.989     ,\n",
      "       0.989     , 0.98966667, 0.98833333, 0.99      , 0.99066667]), 'split4_test_score': array([0.96833333, 0.967     , 0.96833333, 0.967     , 0.96933333,\n",
      "       0.97033333, 0.966     , 0.967     , 0.96766667, 0.966     ]), 'mean_test_score': array([0.98420096, 0.92013411, 0.98486747, 0.98393409, 0.98486756,\n",
      "       0.98486742, 0.98400082, 0.98373413, 0.98460069, 0.98453409]), 'std_test_score': array([0.00861694, 0.12704557, 0.00872471, 0.00915924, 0.00838165,\n",
      "       0.00789016, 0.00943077, 0.00894813, 0.00879787, 0.00969476]), 'rank_test_score': array([ 6, 10,  2,  8,  1,  3,  7,  9,  4,  5])}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivial Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trivial classifier =  [0.78373815]\n"
     ]
    }
   ],
   "source": [
    "s, p = sum(y_test), len(y_test)\n",
    "#predict all zeros:\n",
    "trivial_score = (p-s)/p\n",
    "print('Accuracy of trivial classifier = ',trivial_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction probabilities for `mlp` and `clf` (the optimal 3-layer mlp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = mlp.predict_proba(X_test)\n",
    "y_prob_opt = clf.predict_proba(X_test) #optimal hyperparams using randomisedsearchCV above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_opt, tpr_opt, thresholds_opt = roc_curve(y_test, y_prob_opt[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr_triv, tpr_triv, thresholds_triv = roc_curve(y_test, y_prob_triv[:,0], pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting ROC curves and calculating AUC scores for the above: (for positive prediction)\n",
    "\n",
    "ROC curve of \"optimal\" mlp shown in orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr_opt, tpr_opt)\n",
    "\n",
    "# plt.plot(fpr_triv,tpr_triv)\n",
    "\n",
    "# plt.xlabel('false negative rate')\n",
    "# plt.ylabel('true negative rate')\n",
    "\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate auc (10,10,10) =  0.9390693477700438\n",
      "Positive rate auc (optimal)=  0.9267438469294618\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(y_test, y_prob[:,1])\n",
    "print('Positive rate auc (10,10,10) = ', auc_score)\n",
    "auc_score = roc_auc_score(y_test, y_prob_opt[:,1])\n",
    "print('Positive rate auc (optimal)= ', auc_score)\n",
    "\n",
    "# auc_score_neg = roc_auc_score((np.ones(np.shape(y_test))-y_test), y_prob[:,0])\n",
    "# print('negative auc score = ', auc_score_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: The AUC score for the optimal mlp is usually lower than that of a randomly specified mlp (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr_opt_n, tpr_opt_n, thresholds_opt_n = roc_curve(y_test, y_prob_opt[:,0], pos_label=0)\n",
    "# fpr_n, tpr_n, thresholds_n = roc_curve(y_test, y_prob[:,0], pos_label=0)\n",
    "# plt.plot(fpr_n, tpr_n)\n",
    "# plt.plot(fpr_opt_n, tpr_opt_n)\n",
    "# plt.xlabel('false negative rate')\n",
    "# plt.ylabel('true negative rate')\n",
    "# plt.show()\n",
    "\n",
    "# auc_score = roc_auc_score(1-y_test, y_prob_opt[:,0])\n",
    "# print('Negative rate auc (opt)= ', auc_score)\n",
    "# auc_score = roc_auc_score(1-y_test, y_prob[:,0])\n",
    "# print('Negative rate auc (10,10,10)= ', auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive rate ROC: High variance in accuracy and AUC score; eg for (10,10,10), AUC score varied between 0.68 - 0.93, accuracy: 70-76%\n",
    "\n",
    "    high AUC score + low accuracy due to good positive prediction + poor negative prediction\n",
    "\n",
    "<!-- - Negative rate ROC: AUC consistently high for optimal 3-layer mlp (> 92.5%) -->\n",
    "\n",
    "- Optimal hyperparameters give poor accuracy (~70%) but consistently high AUC score, between 0.93-0.95.\n",
    "\n",
    "- Changing size of training set (within reasonable bounds ie <20,000) does not seem to affect this\n",
    "\n",
    "- Overall accuracy (& negative accuracy) highest at around 2,500/12,500 pos/neg. (Is this because it is closest to the proportion in the test set?)\n",
    "\n",
    "- AUC score highest at around 5,000/10,000; this makes sense since AUC has some bias towards positive accuracy(?)\n",
    "\n",
    "- Increasing the proportion of positive training data decreased accuracy of negative prediction as expected\n",
    "\n",
    "Table below displays accuracy and AUC scores for different proportions of negative/positive training data (size of training dataset fixed)\n",
    "<!-- & since training size of only <20,000 could run in reasonable time)  -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Positive training size | Negative training size | Accuracy (overall) | Accuracy (positive) | Accuracy (negative) | AUC   |\n",
    "|------------------------|------------------------|--------------------|---------------------|---------------------|-------|\n",
    "| 500                    | 14,500                 | 0.773              | 0.952               | 0.724               | 0.923 |\n",
    "| 2,500                  | 12,500                 | 0.795              | 0.974               | 0.745               | 0.935 |\n",
    "| 5,000                  | 10,000                 | 0.747              | 0.987               | 0.681               | 0.940 |\n",
    "| 7,500                  | 7,500                  | 0.675              | 0.989               | 0.588               | 0.918 |\n",
    "| 10,000                 | 5,000                  | 0.689              | 0.990               | 0.606               | 0.938 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal positive to negative training ratio was around 2500:12500, or 1:5. This produced by the highest (negative) accuracy, and a very good AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters for a 3-layer MLP: AUC score = 0.94, good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = np.zeros(PUL_df.shape[0])\n",
    "train_ind[:int(np.floor(PUL_df.shape[0]*0.6))] = 1 #prop. train\n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "X_PUL, y_PUL = PUL_df.to_numpy()[:,:-1], PUL_df.to_numpy()[:,-1:]\n",
    "\n",
    "X_PUL_train, y_PUL_train = X_PUL[train_ind==1,:], y_PUL[train_ind==1]\n",
    "X_PUL_test, y_PUL_test = X_PUL[train_ind==0,:], y_PUL[train_ind==0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Only using PUL file for training and testing: (60/40)\n",
    "\n",
    "[to see if accuracy improves so, for example, something may be wrong with the training dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, y2_train = X_PUL_train, y_PUL_train\n",
    "X2_test, y2_test = X_PUL_test, y_PUL_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70889669\n",
      "Iteration 2, loss = 0.56608791\n",
      "Iteration 3, loss = 0.51478544\n",
      "Iteration 4, loss = 0.47753064\n",
      "Iteration 5, loss = 0.43486368\n",
      "Iteration 6, loss = 0.39492189\n",
      "Iteration 7, loss = 0.36396285\n",
      "Iteration 8, loss = 0.33926983\n",
      "Iteration 9, loss = 0.31816372\n",
      "Iteration 10, loss = 0.29941161\n",
      "Iteration 11, loss = 0.28301677\n",
      "Iteration 12, loss = 0.26899128\n",
      "Iteration 13, loss = 0.25644812\n",
      "Iteration 14, loss = 0.24540742\n",
      "Iteration 15, loss = 0.23574234\n",
      "Iteration 16, loss = 0.22753962\n",
      "Iteration 17, loss = 0.22058173\n",
      "Iteration 18, loss = 0.21464311\n",
      "Iteration 19, loss = 0.20973611\n",
      "Iteration 20, loss = 0.20512615\n",
      "Iteration 21, loss = 0.20086267\n",
      "Iteration 22, loss = 0.19808688\n",
      "Iteration 23, loss = 0.19369995\n",
      "Iteration 24, loss = 0.18944078\n",
      "Iteration 25, loss = 0.18620458\n",
      "Iteration 26, loss = 0.18197621\n",
      "Iteration 27, loss = 0.17849670\n",
      "Iteration 28, loss = 0.17634801\n",
      "Iteration 29, loss = 0.17367596\n",
      "Iteration 30, loss = 0.17193148\n",
      "Iteration 31, loss = 0.16879395\n",
      "Iteration 32, loss = 0.16524757\n",
      "Iteration 33, loss = 0.16374568\n",
      "Iteration 34, loss = 0.16122821\n",
      "Iteration 35, loss = 0.15837364\n",
      "Iteration 36, loss = 0.15570678\n",
      "Iteration 37, loss = 0.15356479\n",
      "Iteration 38, loss = 0.15126276\n",
      "Iteration 39, loss = 0.14869398\n",
      "Iteration 40, loss = 0.14756015\n",
      "Iteration 41, loss = 0.14507144\n",
      "Iteration 42, loss = 0.14269409\n",
      "Iteration 43, loss = 0.14060846\n",
      "Iteration 44, loss = 0.13839281\n",
      "Iteration 45, loss = 0.13638510\n",
      "Iteration 46, loss = 0.13440611\n",
      "Iteration 47, loss = 0.13397302\n",
      "Iteration 48, loss = 0.13087271\n",
      "Iteration 49, loss = 0.12849392\n",
      "Iteration 50, loss = 0.12789641\n",
      "Iteration 51, loss = 0.12595654\n",
      "Iteration 52, loss = 0.12404419\n",
      "Iteration 53, loss = 0.12656241\n",
      "Iteration 54, loss = 0.12216848\n",
      "Iteration 55, loss = 0.11910931\n",
      "Iteration 56, loss = 0.11769473\n",
      "Iteration 57, loss = 0.12017017\n",
      "Iteration 58, loss = 0.11529939\n",
      "Iteration 59, loss = 0.11373329\n",
      "Iteration 60, loss = 0.11267320\n",
      "Iteration 61, loss = 0.11354270\n",
      "Iteration 62, loss = 0.10918936\n",
      "Iteration 63, loss = 0.10762473\n",
      "Iteration 64, loss = 0.10617159\n",
      "Iteration 65, loss = 0.10526937\n",
      "Iteration 66, loss = 0.10489711\n",
      "Iteration 67, loss = 0.10377726\n",
      "Iteration 68, loss = 0.10200547\n",
      "Iteration 69, loss = 0.10102183\n",
      "Iteration 70, loss = 0.09956060\n",
      "Iteration 71, loss = 0.09929414\n",
      "Iteration 72, loss = 0.10194458\n",
      "Iteration 73, loss = 0.09749561\n",
      "Iteration 74, loss = 0.09701169\n",
      "Iteration 75, loss = 0.09578443\n",
      "Iteration 76, loss = 0.09382181\n",
      "Iteration 77, loss = 0.09281459\n",
      "Iteration 78, loss = 0.09123800\n",
      "Iteration 79, loss = 0.09118507\n",
      "Iteration 80, loss = 0.09120906\n",
      "Iteration 81, loss = 0.08841655\n",
      "Iteration 82, loss = 0.08797770\n",
      "Iteration 83, loss = 0.08705989\n",
      "Iteration 84, loss = 0.08719036\n",
      "Iteration 85, loss = 0.08568019\n",
      "Iteration 86, loss = 0.08481121\n",
      "Iteration 87, loss = 0.08589307\n",
      "Iteration 88, loss = 0.08293686\n",
      "Iteration 89, loss = 0.08294328\n",
      "Iteration 90, loss = 0.08373125\n",
      "Iteration 91, loss = 0.08118547\n",
      "Iteration 92, loss = 0.08066064\n",
      "Iteration 93, loss = 0.07990783\n",
      "Iteration 94, loss = 0.08012134\n",
      "Iteration 95, loss = 0.07749737\n",
      "Iteration 96, loss = 0.07670387\n",
      "Iteration 97, loss = 0.07537905\n",
      "Iteration 98, loss = 0.07639744\n",
      "Iteration 99, loss = 0.07704697\n",
      "Iteration 100, loss = 0.07393103\n",
      "Iteration 101, loss = 0.07357255\n",
      "Iteration 102, loss = 0.07229500\n",
      "Iteration 103, loss = 0.07315952\n",
      "Iteration 104, loss = 0.07175178\n",
      "Iteration 105, loss = 0.07091114\n",
      "Iteration 106, loss = 0.07334016\n",
      "Iteration 107, loss = 0.06987289\n",
      "Iteration 108, loss = 0.06889501\n",
      "Iteration 109, loss = 0.06991172\n",
      "Iteration 110, loss = 0.06738711\n",
      "Iteration 111, loss = 0.06728628\n",
      "Iteration 112, loss = 0.06657740\n",
      "Iteration 113, loss = 0.06861115\n",
      "Iteration 114, loss = 0.06524204\n",
      "Iteration 115, loss = 0.06410205\n",
      "Iteration 116, loss = 0.06331575\n",
      "Iteration 117, loss = 0.06416840\n",
      "Iteration 118, loss = 0.06468410\n",
      "Iteration 119, loss = 0.06222173\n",
      "Iteration 120, loss = 0.06212429\n",
      "Iteration 121, loss = 0.06042268\n",
      "Iteration 122, loss = 0.06160247\n",
      "Iteration 123, loss = 0.06150945\n",
      "Iteration 124, loss = 0.06213226\n",
      "Iteration 125, loss = 0.05930781\n",
      "Iteration 126, loss = 0.05871614\n",
      "Iteration 127, loss = 0.05942439\n",
      "Iteration 128, loss = 0.05633261\n",
      "Iteration 129, loss = 0.05676130\n",
      "Iteration 130, loss = 0.05755273\n",
      "Iteration 131, loss = 0.05510697\n",
      "Iteration 132, loss = 0.05446965\n",
      "Iteration 133, loss = 0.05342799\n",
      "Iteration 134, loss = 0.05322607\n",
      "Iteration 135, loss = 0.05253165\n",
      "Iteration 136, loss = 0.05178271\n",
      "Iteration 137, loss = 0.05117355\n",
      "Iteration 138, loss = 0.05163395\n",
      "Iteration 139, loss = 0.05164955\n",
      "Iteration 140, loss = 0.05270721\n",
      "Iteration 141, loss = 0.04907108\n",
      "Iteration 142, loss = 0.05462490\n",
      "Iteration 143, loss = 0.05015685\n",
      "Iteration 144, loss = 0.05012530\n",
      "Iteration 145, loss = 0.04821887\n",
      "Iteration 146, loss = 0.04842633\n",
      "Iteration 147, loss = 0.04804154\n",
      "Iteration 148, loss = 0.04581299\n",
      "Iteration 149, loss = 0.04531137\n",
      "Iteration 150, loss = 0.04763164\n",
      "Iteration 151, loss = 0.04651677\n",
      "Iteration 152, loss = 0.04669777\n",
      "Iteration 153, loss = 0.04972130\n",
      "Iteration 154, loss = 0.04382924\n",
      "Iteration 155, loss = 0.04249591\n",
      "Iteration 156, loss = 0.04295127\n",
      "Iteration 157, loss = 0.04241329\n",
      "Iteration 158, loss = 0.04142334\n",
      "Iteration 159, loss = 0.04083759\n",
      "Iteration 160, loss = 0.04104575\n",
      "Iteration 161, loss = 0.04044943\n",
      "Iteration 162, loss = 0.04083346\n",
      "Iteration 163, loss = 0.04021387\n",
      "Iteration 164, loss = 0.04329350\n",
      "Iteration 165, loss = 0.03913608\n",
      "Iteration 166, loss = 0.04127806\n",
      "Iteration 167, loss = 0.04034328\n",
      "Iteration 168, loss = 0.04049943\n",
      "Iteration 169, loss = 0.03721854\n",
      "Iteration 170, loss = 0.04028488\n",
      "Iteration 171, loss = 0.03665375\n",
      "Iteration 172, loss = 0.03601767\n",
      "Iteration 173, loss = 0.03478830\n",
      "Iteration 174, loss = 0.03545552\n",
      "Iteration 175, loss = 0.03586862\n",
      "Iteration 176, loss = 0.03711609\n",
      "Iteration 177, loss = 0.03477230\n",
      "Iteration 178, loss = 0.03350241\n",
      "Iteration 179, loss = 0.03529103\n",
      "Iteration 180, loss = 0.03471925\n",
      "Iteration 181, loss = 0.03484565\n",
      "Iteration 182, loss = 0.03236278\n",
      "Iteration 183, loss = 0.03170963\n",
      "Iteration 184, loss = 0.03144797\n",
      "Iteration 185, loss = 0.03163103\n",
      "Iteration 186, loss = 0.03032962\n",
      "Iteration 187, loss = 0.03769863\n",
      "Iteration 188, loss = 0.03151463\n",
      "Iteration 189, loss = 0.03143284\n",
      "Iteration 190, loss = 0.03003210\n",
      "Iteration 191, loss = 0.02965970\n",
      "Iteration 192, loss = 0.03061567\n",
      "Iteration 193, loss = 0.02884697\n",
      "Iteration 194, loss = 0.03516716\n",
      "Iteration 195, loss = 0.03704631\n",
      "Iteration 196, loss = 0.02933546\n",
      "Iteration 197, loss = 0.03237733\n",
      "Iteration 198, loss = 0.03154615\n",
      "Iteration 199, loss = 0.02948296\n",
      "Iteration 200, loss = 0.02768915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(10,10),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    verbose = True).fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = mlp2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298701298701298\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             2321          130\n",
      "Cazyme                   86          543\n",
      "negative accuracy=  0.9469604243166054\n",
      "positive accuracy=  0.863275039745628\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y2_test,y2_pred))\n",
    "mat = confusion_matrix(y2_test, y2_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)\n",
    "\n",
    "print('negative accuracy= ', cfmat_df.iloc[0,0]/sum(cfmat_df.iloc[0,:]))\n",
    "print('positive accuracy= ', cfmat_df.iloc[1,1]/sum(cfmat_df.iloc[1,:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- - 1 layer (10 neurons): 93.2% accuracy,  -->\n",
    "\n",
    "| Number of layers | Accuracy |\n",
    "|------------------|----------|\n",
    "| 2                | 0.929    |\n",
    "| 3                | 0.931    |\n",
    "| 4                | 0.932    |\n",
    "\n",
    "Positive prediction is still quite poor (~86%) but much better than in (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fewer neurons led to poorer positive prediction but not on negative prediction (why?), for example:\n",
    "- (10,10): 86.0%\n",
    "- (3,3): 0% (!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y2_prob = mlp.predict_proba(X2_test)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y2_test, y2_prob[:,0], pos_label=0)\n",
    "plt.plot(fpr2, tpr2)\n",
    "plt.xlabel('false negative rate')\n",
    "plt.ylabel('true negative rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.9369787744400747\n"
     ]
    }
   ],
   "source": [
    "auc2_score = roc_auc_score(y2_test, y2_prob[:,1])\n",
    "print('auc = ', auc2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score (10,10): 0.93\n",
    "\n",
    "No change in AUC score compared to (a), but large increase in accuracy (in particular, positive prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Aside:] Histogram of no. cazymes in each PUL:\n",
    "- `caz_res` = no. cazymes in each PUL\n",
    "- `caz_res_frac` = proportion cazymes in each PUL\n",
    "- `caz_res_len` = total no. proteins in each PUL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PUL_keys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# grouping proteins in the same PUL:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#y-axis: count\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#x-axis: frequency of cazymes in one PUL\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m test_list\u001b[39m=\u001b[39mPUL_keys\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mutil_func\u001b[39m(x, y): \u001b[39mreturn\u001b[39;00m x[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m y[\u001b[39m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m res \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PUL_keys' is not defined"
     ]
    }
   ],
   "source": [
    "# grouping proteins in the same PUL:\n",
    "#y-axis: count\n",
    "#x-axis: frequency of cazymes in one PUL\n",
    "test_list=PUL_keys\n",
    "def util_func(x, y): return x[0] == y[0]\n",
    "\n",
    "res = []\n",
    "for sub in test_list:\n",
    "    ele = sub.split('_')\n",
    "    ele_cat = next((x for x in res if util_func(ele,str(x[0]).split('_'))), [])\n",
    "    if ele_cat == []:\n",
    "        res.append(ele_cat)\n",
    "    ele_cat.append(sub)\n",
    "\n",
    "print(\"list after categorisation : \" + str(res))\n",
    "len(res) # PUL0076 is missing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0\n",
    "caz_res=[]\n",
    "\n",
    "for i in range(len(res)):\n",
    "    indtemp=ind\n",
    "    caz_res.append(sum(np.array(PUL_df.iloc[indtemp:indtemp+len(res[i]),-1])))\n",
    "    ind=indtemp+len(res[i])\n",
    "    # if caz_res[i]== 0:\n",
    "    #     print(res[i])\n",
    "# print(caz_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(caz_res, bins = range(min(caz_res),max(caz_res)+1))\n",
    "plt.xlabel('no. cazymes in PUL'), plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of proportion cazymes in each PUL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind=0\n",
    "caz_res_frac=[]\n",
    "for i in range(len(res)):\n",
    "    indtemp=ind\n",
    "    caz_res_frac.append((sum(np.array(PUL_df.iloc[indtemp:indtemp+len(res[i]),-1])))/len(res[i]))\n",
    "    ind=indtemp+len(res[i])\n",
    "# print(caz_res_frac)\n",
    "\n",
    "plt.hist(caz_res_frac, bins = 100)\n",
    "plt.xlabel('proportion cazymes in PUL'), plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Largest spike at 0 - this is unexpected, since we expect each PUL to contain at least one cazyme!\n",
    "- Spikes at ~0.33, 0.5 and 1 - could be due to PULs containing only few (1-3) proteins? (A histogram of PUL length, shown below confirms this; the marjority of PULs are dimers and trimers.)\n",
    "- Most PULs lie below 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m PUL_len\u001b[39m=\u001b[39m[]\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m res:\n\u001b[0;32m      3\u001b[0m     PUL_len\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(x))\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mhist(PUL_len, bins \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "PUL_len=[]\n",
    "for x in res:\n",
    "    PUL_len.append(len(x))\n",
    "\n",
    "plt.hist(PUL_len, bins = 100)\n",
    "plt.xlabel('length of PUL'), plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "It was found that the best training ratio was 1:5 (positive:negative). This may be due to the majority occurence of non-cazymes, so a large proportion of negative training data was required. In addition, it was found that negative accuracy was always far poorer (~0.7) than positive accuracy (>0.9).\n",
    "\n",
    "Using RandomizedSearchCV to find optimal hyperparameters resulted in a wide range of results upon iteration, but the majority produced an accuracy of ~0.79 and and an AUC score of ~0.93.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
