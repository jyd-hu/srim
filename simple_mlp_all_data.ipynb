{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple MLP using all the data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re as re\n",
    "import math\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load all files & put into dataframes\n",
    "\n",
    "Negative training data:\n",
    "- `pfam_training_data_augment.h5`\n",
    "- `non_cazy_kegg.h5`\n",
    "\n",
    "Positive training data:\n",
    "- `vicreg_train_val_embeddings_noCAZOME_noLargeSeqs_combined.h5` (+ use `cazy_family_by_taxa_60.json` to pick a representative sample across all enzyme classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUL_embeddings = []\n",
    "PUL_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\PUL.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        PUL_embeddings.append(np.array(f[key][()]))\n",
    "        PUL_keys.append(key)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PFAM_embeddings = []\n",
    "PFAM_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\pfam_training_data_augment.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        PFAM_embeddings.append(f[key][()])\n",
    "        PFAM_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_embeddings = []\n",
    "kegg_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\non_cazy_kegg.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        kegg_embeddings.append(f[key][()])\n",
    "        kegg_keys.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the PULs are stored in `PUL_df`, which also indicates if each protein is a CAZyme (1) or not (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUL file:\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "f_path = 'C:\\\\Users\\\\alpha\\\\OneDrive - University of Cambridge\\\\BACKUP 14-04-22\\\\docs\\\\Maths\\\\SRIM\\\\code\\\\PUL.faa'\n",
    "PUL_array, PUL_keys2 = [], []\n",
    "\n",
    "with open(f_path, mode='r') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        identifier, description = record.id, record.description\n",
    "        PUL_keys2.append(identifier)\n",
    "        if 'CAZyme' in description:\n",
    "            PUL_array.append(1)\n",
    "        else:\n",
    "            PUL_array.append(0)\n",
    "\n",
    "PUL_array = np.array(PUL_array)\n",
    "PUL_array = PUL_array.reshape(np.shape(PUL_array)[0],-1)\n",
    "PUL_array_df = pd.DataFrame(PUL_array, index=PUL_keys2,columns=['cazy'])\n",
    "\n",
    "col_label=['emb'+str(i) for i in range(len(list(PUL_embeddings)[0]))]\n",
    "# col_label.append('cazy')\n",
    "\n",
    "PUL_embeddings_list=list(PUL_embeddings)\n",
    "temp_df = pd.DataFrame(PUL_embeddings_list, index=PUL_keys, columns=['emb'+str(i) for i in range(len(list(PUL_embeddings)[0]))])\n",
    "\n",
    "# PUL_df = pd.DataFrame(data=np.concatenate([PUL_embeddings,PUL_array], axis=1), index=PUL_keys2, columns=col_label)\n",
    "PUL_df = temp_df.join(PUL_array_df)\n",
    "\n",
    "#indexing issue fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>emb9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1015</th>\n",
       "      <th>emb1016</th>\n",
       "      <th>emb1017</th>\n",
       "      <th>emb1018</th>\n",
       "      <th>emb1019</th>\n",
       "      <th>emb1020</th>\n",
       "      <th>emb1021</th>\n",
       "      <th>emb1022</th>\n",
       "      <th>emb1023</th>\n",
       "      <th>cazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PUL0001_1</th>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>-0.032013</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>-0.044128</td>\n",
       "      <td>-0.086609</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049255</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>-0.111633</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>0.030334</td>\n",
       "      <td>-0.028244</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>0.025970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_10</th>\n",
       "      <td>-0.014236</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.034546</td>\n",
       "      <td>-0.026245</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>-0.001535</td>\n",
       "      <td>-0.026840</td>\n",
       "      <td>-0.044830</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>-0.120483</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>-0.005569</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_11</th>\n",
       "      <td>-0.019104</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>0.051361</td>\n",
       "      <td>0.030670</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.076233</td>\n",
       "      <td>0.047943</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042297</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.089661</td>\n",
       "      <td>0.064331</td>\n",
       "      <td>-0.042755</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_12</th>\n",
       "      <td>0.047211</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>-0.008369</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>-0.002466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.036804</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>-0.035187</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>0.069641</td>\n",
       "      <td>-0.001657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0001_13</th>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.021515</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>0.024506</td>\n",
       "      <td>-0.014908</td>\n",
       "      <td>-0.058167</td>\n",
       "      <td>-0.014183</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>-0.064758</td>\n",
       "      <td>0.049774</td>\n",
       "      <td>-0.009315</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>0.060211</td>\n",
       "      <td>0.017761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_5</th>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.035370</td>\n",
       "      <td>-0.035553</td>\n",
       "      <td>-0.049744</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006264</td>\n",
       "      <td>-0.016479</td>\n",
       "      <td>-0.024261</td>\n",
       "      <td>0.067261</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>-0.026321</td>\n",
       "      <td>0.032532</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>0.066528</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_6</th>\n",
       "      <td>0.009483</td>\n",
       "      <td>0.044464</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>-0.036957</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>-0.014572</td>\n",
       "      <td>-0.093689</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.013908</td>\n",
       "      <td>-0.104553</td>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>-0.053558</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_7</th>\n",
       "      <td>0.051819</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>-0.028381</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>-0.026932</td>\n",
       "      <td>-0.030212</td>\n",
       "      <td>0.031433</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>-0.069275</td>\n",
       "      <td>0.019424</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>-0.048309</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>-0.010933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_8</th>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.047760</td>\n",
       "      <td>0.041046</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.082153</td>\n",
       "      <td>-0.036713</td>\n",
       "      <td>-0.099487</td>\n",
       "      <td>-0.023865</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015564</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>-0.069214</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.011467</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>0.034729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUL0602_9</th>\n",
       "      <td>0.024521</td>\n",
       "      <td>0.037872</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.020950</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>-0.080750</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>-0.006287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019974</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>0.042999</td>\n",
       "      <td>0.024536</td>\n",
       "      <td>-0.007603</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>0.062805</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                emb0      emb1      emb2      emb3      emb4      emb5  \\\n",
       "PUL0001_1   0.017563  0.059692  0.030075  0.007584 -0.032013  0.063904   \n",
       "PUL0001_10 -0.014236  0.032715  0.034546 -0.026245  0.007820 -0.001535   \n",
       "PUL0001_11 -0.019104  0.034027  0.051361  0.030670 -0.013756  0.012695   \n",
       "PUL0001_12  0.047211  0.002373  0.000755  0.020142 -0.008369  0.028976   \n",
       "PUL0001_13  0.024719  0.031494  0.027359  0.021515 -0.007629  0.024506   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "PUL0602_5   0.017685  0.051422  0.006428 -0.001532  0.006233  0.035370   \n",
       "PUL0602_6   0.009483  0.044464  0.050842 -0.036957  0.004826 -0.014664   \n",
       "PUL0602_7   0.051819  0.009346  0.015388 -0.028381  0.013084  0.042877   \n",
       "PUL0602_8   0.032349  0.047760  0.041046 -0.033142  0.004124  0.082153   \n",
       "PUL0602_9   0.024521  0.037872  0.005112 -0.020950  0.020767  0.042908   \n",
       "\n",
       "                emb6      emb7      emb8      emb9  ...   emb1015   emb1016  \\\n",
       "PUL0001_1  -0.044128 -0.086609  0.033752 -0.031281  ... -0.049255 -0.006824   \n",
       "PUL0001_10 -0.026840 -0.044830 -0.022736  0.003244  ... -0.020203  0.010963   \n",
       "PUL0001_11 -0.019608 -0.076233  0.047943  0.000046  ... -0.042297 -0.000536   \n",
       "PUL0001_12 -0.000050 -0.083252  0.017487 -0.002466  ... -0.031281 -0.036804   \n",
       "PUL0001_13 -0.014908 -0.058167 -0.014183 -0.020538  ...  0.004028  0.015884   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "PUL0602_5  -0.035553 -0.049744  0.028076  0.002031  ... -0.006264 -0.016479   \n",
       "PUL0602_6  -0.014572 -0.093689 -0.026871  0.017822  ... -0.001588  0.013908   \n",
       "PUL0602_7  -0.026932 -0.030212  0.031433  0.006268  ... -0.001817  0.013542   \n",
       "PUL0602_8  -0.036713 -0.099487 -0.023865  0.007652  ... -0.015564  0.004181   \n",
       "PUL0602_9  -0.052765 -0.080750 -0.012650 -0.006287  ... -0.019974  0.022400   \n",
       "\n",
       "             emb1017   emb1018   emb1019   emb1020   emb1021   emb1022  \\\n",
       "PUL0001_1  -0.111633  0.083191  0.030334 -0.028244 -0.008194  0.031616   \n",
       "PUL0001_10 -0.120483  0.041931  0.013710  0.003368 -0.005569  0.021667   \n",
       "PUL0001_11 -0.089661  0.064331 -0.042755 -0.018158 -0.025391  0.001602   \n",
       "PUL0001_12 -0.075195  0.022964  0.014938 -0.035187 -0.028992  0.069641   \n",
       "PUL0001_13 -0.064758  0.049774 -0.009315  0.007256  0.006168  0.060211   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "PUL0602_5  -0.024261  0.067261  0.056061 -0.026321  0.032532 -0.009926   \n",
       "PUL0602_6  -0.104553  0.027496  0.050049 -0.053558  0.024475  0.020248   \n",
       "PUL0602_7  -0.069275  0.019424 -0.009377 -0.048309  0.009567  0.007690   \n",
       "PUL0602_8  -0.069214  0.032410  0.000823  0.000344 -0.011467  0.043060   \n",
       "PUL0602_9  -0.098633  0.042999  0.024536 -0.007603 -0.011642  0.062805   \n",
       "\n",
       "             emb1023  cazy  \n",
       "PUL0001_1   0.025970     1  \n",
       "PUL0001_10 -0.024216     0  \n",
       "PUL0001_11  0.011215     0  \n",
       "PUL0001_12 -0.001657     0  \n",
       "PUL0001_13  0.017761     1  \n",
       "...              ...   ...  \n",
       "PUL0602_5   0.066528     1  \n",
       "PUL0602_6   0.038025     0  \n",
       "PUL0602_7  -0.010933     0  \n",
       "PUL0602_8   0.034729     0  \n",
       "PUL0602_9   0.114685     0  \n",
       "\n",
       "[7699 rows x 1025 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUL_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-PUL negative (non-CAZyme) data is stored in `non_cazy_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-cazymes:\n",
    "non_cazy_embeddings = np.concatenate([PFAM_embeddings,kegg_embeddings], axis=0)\n",
    "non_cazy_keys = np.concatenate([PFAM_keys, kegg_keys], axis=0)\n",
    "non_cazy_df = pd.DataFrame(data=non_cazy_embeddings, index=non_cazy_keys, columns=['emb'+str(i) for i in range(len(list(non_cazy_embeddings)[0]))])\n",
    "non_cazy_df = non_cazy_df.join(pd.DataFrame(data=np.zeros(np.array(non_cazy_keys).shape[0]), index=non_cazy_keys, columns=['cazy']), how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_embeddings = []\n",
    "cazy_keys = []\n",
    "with h5py.File('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\vicreg_train_val_embeddings_noCAZOME_noLargeSeqs_combined.h5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        cazy_embeddings.append(f[key][()])\n",
    "        cazy_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7699, 1024)\n",
      "(59673, 1024)\n",
      "(244592, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(PUL_embeddings))\n",
    "print(np.shape(non_cazy_embeddings))\n",
    "print(np.shape(cazy_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create lookup using .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_ids = []\n",
    "family_keys = []\n",
    "file = open('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\cazy_family_by_taxa_60.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_ids = []\n",
    "cazy_families = []\n",
    "file = open('C:\\\\Users\\\\alpha\\\\Documents\\\\jennifer\\\\maths\\\\SRIM\\\\code\\\\cazy_family_by_taxa_60.json')\n",
    "data = json.loads(file.read())\n",
    "\n",
    "for key in data.keys(): #keys are IDs, values are classes\n",
    "        cazy_ids.append(key)\n",
    "        cazy_families.append(data.get(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cazy_df = pd.DataFrame(data=concat , columns=['id','class'])\n",
    "cazy_df_2 = pd.DataFrame(data=cazy_families, index=cazy_ids , columns=['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe of cazymes with extra columnns containing embeddings, then inner join with cazy_df_2 (intersect dataframes at indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_df_1 = pd.DataFrame(data=cazy_embeddings, index=cazy_keys, columns=['emb'+str(i) for i in range(len(list(cazy_embeddings)[0]))])\n",
    "\n",
    "# long (~12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             class      emb0      emb1      emb2      emb3      emb4  \\\n",
      "AZS17016.1  GH13_2  0.049500  0.037415 -0.038025  0.031235  0.017197   \n",
      "AVO05213.1    GH32 -0.014870  0.036865  0.009872 -0.011703  0.015808   \n",
      "QPG94344.1    GT61  0.016739 -0.028412  0.026016  0.017227  0.013565   \n",
      "APD47233.1     GT4  0.015976 -0.009315 -0.032532  0.033173  0.011223   \n",
      "ANU32067.1     GT4  0.010872 -0.021881 -0.008972 -0.013237  0.000924   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "QMI07227.1    GT83 -0.008034 -0.009453  0.029785  0.033478  0.024063   \n",
      "UMA63307.1     GT2  0.019272 -0.044830  0.024490  0.026031  0.026672   \n",
      "UPW01296.1     GT4  0.055878 -0.059723 -0.039551 -0.001044  0.057983   \n",
      "ULL17023.1    GH43  0.013275  0.033997  0.013687  0.021133  0.018799   \n",
      "USF23742.1     GT9  0.022705 -0.043182 -0.023972  0.007046 -0.025818   \n",
      "\n",
      "                emb5      emb6      emb7      emb8  ...   emb1014   emb1015  \\\n",
      "AZS17016.1  0.031616 -0.019043 -0.046234  0.007179  ... -0.040833 -0.034882   \n",
      "AVO05213.1 -0.001546  0.005375 -0.102600 -0.002607  ... -0.015167 -0.013428   \n",
      "QPG94344.1  0.033905 -0.067627 -0.064880 -0.005409  ... -0.072754 -0.009453   \n",
      "APD47233.1  0.055298 -0.019485 -0.062164  0.036804  ... -0.027649 -0.028488   \n",
      "ANU32067.1  0.037537 -0.050995 -0.051697  0.014412  ... -0.032379 -0.040771   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "QMI07227.1  0.021744 -0.033081 -0.051086  0.059387  ... -0.012657 -0.030960   \n",
      "UMA63307.1  0.003311 -0.064575 -0.080627 -0.030411  ...  0.004433 -0.040588   \n",
      "UPW01296.1  0.034698 -0.031525 -0.088623  0.003918  ... -0.030212  0.000360   \n",
      "ULL17023.1  0.044891 -0.020218 -0.037231  0.026855  ... -0.012169 -0.036713   \n",
      "USF23742.1  0.057068 -0.012955 -0.104492  0.002665  ...  0.012947 -0.064270   \n",
      "\n",
      "             emb1016   emb1017   emb1018   emb1019   emb1020   emb1021  \\\n",
      "AZS17016.1 -0.048950 -0.054535  0.053986  0.058350 -0.017746  0.000415   \n",
      "AVO05213.1  0.028091 -0.086609  0.072632  0.039520 -0.009178  0.034973   \n",
      "QPG94344.1 -0.022400  0.017334  0.031250  0.055725  0.022354 -0.053772   \n",
      "APD47233.1 -0.006145  0.000610  0.007973  0.017776 -0.014000 -0.018326   \n",
      "ANU32067.1  0.010300 -0.068787  0.038727  0.052124 -0.036377  0.008217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "QMI07227.1 -0.010986 -0.069885  0.026779  0.051392  0.016006 -0.047211   \n",
      "UMA63307.1 -0.030167 -0.038300 -0.033752  0.017395 -0.015884 -0.026962   \n",
      "UPW01296.1 -0.020020 -0.026672  0.013168  0.062378 -0.035492 -0.030624   \n",
      "ULL17023.1  0.012344 -0.091064  0.013885  0.050232  0.000664  0.004559   \n",
      "USF23742.1 -0.004471 -0.082336  0.065735  0.049774 -0.016922 -0.023758   \n",
      "\n",
      "             emb1022   emb1023  \n",
      "AZS17016.1  0.031189  0.030411  \n",
      "AVO05213.1  0.057617  0.035248  \n",
      "QPG94344.1  0.024490  0.016556  \n",
      "APD47233.1  0.027618  0.011330  \n",
      "ANU32067.1  0.031891 -0.020660  \n",
      "...              ...       ...  \n",
      "QMI07227.1  0.022720  0.053162  \n",
      "UMA63307.1  0.026901 -0.016068  \n",
      "UPW01296.1  0.041962  0.034149  \n",
      "ULL17023.1  0.057648  0.028473  \n",
      "USF23742.1  0.039551  0.022232  \n",
      "\n",
      "[232736 rows x 1025 columns]\n"
     ]
    }
   ],
   "source": [
    "cazy_df = cazy_df_2.join(cazy_df_1, how='inner')\n",
    "print(cazy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_GH_df = cazy_df[cazy_df['class'].str.contains('GH')]\n",
    "cazy_GT_df = cazy_df[cazy_df['class'].str.contains('GT')]\n",
    "cazy_PL_df = cazy_df[cazy_df['class'].str.contains('PL')]\n",
    "cazy_CE_df = cazy_df[cazy_df['class'].str.contains('CE')]\n",
    "\n",
    "allclasses=['GH', 'GT', 'PL', 'CE']\n",
    "cazy_other_df = cazy_df[cazy_df['class'].str.contains('|'.join(allclasses))]\n",
    "\n",
    "#then use pd.DataFrame.sample to take random sample of items across axis 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size of first four dataframes add up to 232736; so in fact no other classes to account for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample proportionally to size of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=5000/232736\n",
    "new_cazy_GH_df = cazy_GH_df.sample(frac=x, axis=0)\n",
    "new_cazy_GT_df = cazy_GT_df.sample(frac=x, axis=0)\n",
    "new_cazy_PL_df = cazy_PL_df.sample(frac=x, axis=0)\n",
    "new_cazy_CE_df = cazy_CE_df.sample(frac=x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate into one DataFrame:\n",
    "\n",
    "Positive training data is stored in `cazy_train_df` (5001 embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cazy_train_df = pd.concat([new_cazy_GH_df, new_cazy_GT_df, new_cazy_PL_df, new_cazy_CE_df], axis=0)\n",
    "cazy_train_df = cazy_train_df.join(pd.DataFrame(data=np.ones(np.array(cazy_ids).shape[0]), index=cazy_ids, columns=['cazy']), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1015</th>\n",
       "      <th>emb1016</th>\n",
       "      <th>emb1017</th>\n",
       "      <th>emb1018</th>\n",
       "      <th>emb1019</th>\n",
       "      <th>emb1020</th>\n",
       "      <th>emb1021</th>\n",
       "      <th>emb1022</th>\n",
       "      <th>emb1023</th>\n",
       "      <th>cazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QPC84684.1</th>\n",
       "      <td>GH78</td>\n",
       "      <td>0.016434</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>-0.002573</td>\n",
       "      <td>0.022385</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>-0.087402</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004845</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>-0.052094</td>\n",
       "      <td>0.025772</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.041046</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATO55907.1</th>\n",
       "      <td>GH25</td>\n",
       "      <td>-0.015541</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.040192</td>\n",
       "      <td>-0.008339</td>\n",
       "      <td>-0.031921</td>\n",
       "      <td>-0.038879</td>\n",
       "      <td>-0.085327</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054749</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>-0.038757</td>\n",
       "      <td>0.045563</td>\n",
       "      <td>0.030258</td>\n",
       "      <td>-0.010086</td>\n",
       "      <td>0.047241</td>\n",
       "      <td>-0.052155</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UIP58386.1</th>\n",
       "      <td>GH78</td>\n",
       "      <td>-0.000752</td>\n",
       "      <td>-0.038025</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>-0.011497</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>-0.012955</td>\n",
       "      <td>-0.070496</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034729</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>-0.034576</td>\n",
       "      <td>0.007610</td>\n",
       "      <td>0.017227</td>\n",
       "      <td>-0.004364</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.060394</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH16690.1</th>\n",
       "      <td>GH13_31</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.037079</td>\n",
       "      <td>-0.001828</td>\n",
       "      <td>-0.008621</td>\n",
       "      <td>-0.001671</td>\n",
       "      <td>0.047028</td>\n",
       "      <td>-0.061798</td>\n",
       "      <td>-0.077820</td>\n",
       "      <td>-0.022659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023575</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>-0.059937</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.013550</td>\n",
       "      <td>-0.012604</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>0.042236</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBA32915.1</th>\n",
       "      <td>GH63</td>\n",
       "      <td>0.045410</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.050629</td>\n",
       "      <td>-0.041229</td>\n",
       "      <td>0.046570</td>\n",
       "      <td>-0.075317</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>-0.113525</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006058</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>-0.064148</td>\n",
       "      <td>0.151245</td>\n",
       "      <td>-0.022156</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>0.039734</td>\n",
       "      <td>0.057281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QGW65371.1</th>\n",
       "      <td>CE4</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>-0.023987</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.058319</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.042236</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-0.064392</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045868</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>-0.005524</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.079346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBB47795.1</th>\n",
       "      <td>CE9</td>\n",
       "      <td>0.022385</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.039673</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>-0.057648</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>-0.039856</td>\n",
       "      <td>-0.094421</td>\n",
       "      <td>-0.032898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066528</td>\n",
       "      <td>-0.004166</td>\n",
       "      <td>-0.098450</td>\n",
       "      <td>0.023987</td>\n",
       "      <td>-0.006149</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.036133</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UGQ12528.1</th>\n",
       "      <td>CE9</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>0.021515</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>-0.013252</td>\n",
       "      <td>-0.033569</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>-0.014488</td>\n",
       "      <td>-0.076721</td>\n",
       "      <td>0.024826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053619</td>\n",
       "      <td>-0.025909</td>\n",
       "      <td>-0.083740</td>\n",
       "      <td>-0.046753</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.035004</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>-0.024826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDT00392.1</th>\n",
       "      <td>CE6</td>\n",
       "      <td>0.038483</td>\n",
       "      <td>0.060425</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>-0.049988</td>\n",
       "      <td>-0.047943</td>\n",
       "      <td>0.061951</td>\n",
       "      <td>-0.073364</td>\n",
       "      <td>-0.090149</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.026443</td>\n",
       "      <td>-0.050110</td>\n",
       "      <td>-0.034607</td>\n",
       "      <td>0.047302</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.013847</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QZD55491.1</th>\n",
       "      <td>CE4</td>\n",
       "      <td>0.028839</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>-0.004890</td>\n",
       "      <td>0.012596</td>\n",
       "      <td>-0.007664</td>\n",
       "      <td>-0.021255</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>-0.058716</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052399</td>\n",
       "      <td>-0.059174</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>-0.005310</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>-0.023636</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.006275</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              class      emb0      emb1      emb2      emb3      emb4  \\\n",
       "QPC84684.1     GH78  0.016434  0.020996  0.010185 -0.004520 -0.002573   \n",
       "ATO55907.1     GH25 -0.015541  0.004791  0.003641  0.040192 -0.008339   \n",
       "UIP58386.1     GH78 -0.000752 -0.038025  0.023712 -0.011497  0.001517   \n",
       "BBH16690.1  GH13_31  0.034088  0.037079 -0.001828 -0.008621 -0.001671   \n",
       "CBA32915.1     GH63  0.045410  0.006805  0.050629 -0.041229  0.046570   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "QGW65371.1      CE4  0.012901 -0.023987  0.004219  0.058319  0.013100   \n",
       "BBB47795.1      CE9  0.022385  0.043793  0.039673  0.004883 -0.057648   \n",
       "UGQ12528.1      CE9  0.009865  0.021515  0.025146 -0.013252 -0.033569   \n",
       "QDT00392.1      CE6  0.038483  0.060425  0.015427 -0.049988 -0.047943   \n",
       "QZD55491.1      CE4  0.028839 -0.012772 -0.004890  0.012596 -0.007664   \n",
       "\n",
       "                emb5      emb6      emb7      emb8  ...   emb1015   emb1016  \\\n",
       "QPC84684.1  0.022385 -0.025391 -0.087402 -0.003696  ... -0.004845  0.007828   \n",
       "ATO55907.1 -0.031921 -0.038879 -0.085327 -0.020142  ... -0.054749 -0.040405   \n",
       "UIP58386.1  0.024445 -0.012955 -0.070496 -0.000738  ... -0.034729  0.011955   \n",
       "BBH16690.1  0.047028 -0.061798 -0.077820 -0.022659  ... -0.023575 -0.000435   \n",
       "CBA32915.1 -0.075317  0.030457 -0.113525  0.009491  ... -0.006058  0.002708   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "QGW65371.1  0.042236 -0.004894 -0.064392  0.000936  ... -0.045868  0.004707   \n",
       "BBB47795.1  0.053528 -0.039856 -0.094421 -0.032898  ... -0.066528 -0.004166   \n",
       "UGQ12528.1  0.008659 -0.014488 -0.076721  0.024826  ... -0.053619 -0.025909   \n",
       "QDT00392.1  0.061951 -0.073364 -0.090149  0.001916  ...  0.004028  0.026443   \n",
       "QZD55491.1 -0.021255  0.000913 -0.058716 -0.006329  ... -0.052399 -0.059174   \n",
       "\n",
       "             emb1017   emb1018   emb1019   emb1020   emb1021   emb1022  \\\n",
       "QPC84684.1 -0.052094  0.025772  0.033905  0.001213  0.017990  0.041046   \n",
       "ATO55907.1 -0.038757  0.045563  0.030258 -0.010086  0.047241 -0.052155   \n",
       "UIP58386.1 -0.034576  0.007610  0.017227 -0.004364  0.009430  0.060394   \n",
       "BBH16690.1 -0.059937  0.015625 -0.013550 -0.012604 -0.050293  0.051605   \n",
       "CBA32915.1 -0.064148  0.151245 -0.022156 -0.075684 -0.016235  0.039734   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "QGW65371.1 -0.005524 -0.003883  0.048767  0.016068  0.001952  0.022064   \n",
       "BBB47795.1 -0.098450  0.023987 -0.006149 -0.023438 -0.036133  0.039795   \n",
       "UGQ12528.1 -0.083740 -0.046753 -0.024216 -0.000406 -0.035004  0.002735   \n",
       "QDT00392.1 -0.050110 -0.034607  0.047302 -0.000613 -0.013847  0.015625   \n",
       "QZD55491.1 -0.050629 -0.005310  0.041107 -0.023636 -0.004990 -0.049408   \n",
       "\n",
       "             emb1023  cazy  \n",
       "QPC84684.1  0.002014   1.0  \n",
       "ATO55907.1  0.006237   1.0  \n",
       "UIP58386.1 -0.011192   1.0  \n",
       "BBH16690.1  0.042236   1.0  \n",
       "CBA32915.1  0.057281   1.0  \n",
       "...              ...   ...  \n",
       "QGW65371.1  0.079346   1.0  \n",
       "BBB47795.1  0.041504   1.0  \n",
       "UGQ12528.1 -0.024826   1.0  \n",
       "QDT00392.1  0.003159   1.0  \n",
       "QZD55491.1 -0.006275   1.0  \n",
       "\n",
       "[5001 rows x 1026 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cazy_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create training dataframe + train a simple MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative training data is stored in `non_cazy_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cazy_train_df = non_cazy_df.sample(n=10000, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Using PUL file entirely for inference; `train_df` for training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Concatenate non-cazy and cazy training dataframes (size = 10000 + 5001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([cazy_train_df, non_cazy_train_df], axis=0).iloc[:,1:] #removes 'class' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(train_df.iloc[:,:-1]), np.array(train_df.iloc[:,-1:])\n",
    "X_test, y_test = np.array(PUL_df.iloc[:,:-1]), np.array(PUL_df.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP: 2 layers, each with 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66621192\n",
      "Iteration 2, loss = 0.64645933\n",
      "Iteration 3, loss = 0.63729106\n",
      "Iteration 4, loss = 0.62853588\n",
      "Iteration 5, loss = 0.61743365\n",
      "Iteration 6, loss = 0.60316161\n",
      "Iteration 7, loss = 0.58436629\n",
      "Iteration 8, loss = 0.55947048\n",
      "Iteration 9, loss = 0.52769701\n",
      "Iteration 10, loss = 0.48985019\n",
      "Iteration 11, loss = 0.44816615\n",
      "Iteration 12, loss = 0.40544248\n",
      "Iteration 13, loss = 0.36490650\n",
      "Iteration 14, loss = 0.32761950\n",
      "Iteration 15, loss = 0.29417611\n",
      "Iteration 16, loss = 0.26458403\n",
      "Iteration 17, loss = 0.23867258\n",
      "Iteration 18, loss = 0.21600138\n",
      "Iteration 19, loss = 0.19615313\n",
      "Iteration 20, loss = 0.17873264\n",
      "Iteration 21, loss = 0.16371186\n",
      "Iteration 22, loss = 0.15071952\n",
      "Iteration 23, loss = 0.13933400\n",
      "Iteration 24, loss = 0.12922848\n",
      "Iteration 25, loss = 0.12032861\n",
      "Iteration 26, loss = 0.11237258\n",
      "Iteration 27, loss = 0.10517639\n",
      "Iteration 28, loss = 0.09884723\n",
      "Iteration 29, loss = 0.09307522\n",
      "Iteration 30, loss = 0.08792307\n",
      "Iteration 31, loss = 0.08328090\n",
      "Iteration 32, loss = 0.07909631\n",
      "Iteration 33, loss = 0.07523590\n",
      "Iteration 34, loss = 0.07176428\n",
      "Iteration 35, loss = 0.06848220\n",
      "Iteration 36, loss = 0.06545607\n",
      "Iteration 37, loss = 0.06273742\n",
      "Iteration 38, loss = 0.06030104\n",
      "Iteration 39, loss = 0.05792230\n",
      "Iteration 40, loss = 0.05570639\n",
      "Iteration 41, loss = 0.05375888\n",
      "Iteration 42, loss = 0.05174678\n",
      "Iteration 43, loss = 0.04982242\n",
      "Iteration 44, loss = 0.04785976\n",
      "Iteration 45, loss = 0.04606662\n",
      "Iteration 46, loss = 0.04454248\n",
      "Iteration 47, loss = 0.04307621\n",
      "Iteration 48, loss = 0.04170256\n",
      "Iteration 49, loss = 0.04043668\n",
      "Iteration 50, loss = 0.03912167\n",
      "Iteration 51, loss = 0.03804884\n",
      "Iteration 52, loss = 0.03703898\n",
      "Iteration 53, loss = 0.03598902\n",
      "Iteration 54, loss = 0.03504933\n",
      "Iteration 55, loss = 0.03412954\n",
      "Iteration 56, loss = 0.03325922\n",
      "Iteration 57, loss = 0.03242983\n",
      "Iteration 58, loss = 0.03168826\n",
      "Iteration 59, loss = 0.03095026\n",
      "Iteration 60, loss = 0.03023867\n",
      "Iteration 61, loss = 0.02966807\n",
      "Iteration 62, loss = 0.02892635\n",
      "Iteration 63, loss = 0.02846595\n",
      "Iteration 64, loss = 0.02790765\n",
      "Iteration 65, loss = 0.02743594\n",
      "Iteration 66, loss = 0.02699905\n",
      "Iteration 67, loss = 0.02658040\n",
      "Iteration 68, loss = 0.02612654\n",
      "Iteration 69, loss = 0.02577086\n",
      "Iteration 70, loss = 0.02537733\n",
      "Iteration 71, loss = 0.02499323\n",
      "Iteration 72, loss = 0.02465059\n",
      "Iteration 73, loss = 0.02436308\n",
      "Iteration 74, loss = 0.02402722\n",
      "Iteration 75, loss = 0.02371208\n",
      "Iteration 76, loss = 0.02348549\n",
      "Iteration 77, loss = 0.02321568\n",
      "Iteration 78, loss = 0.02293256\n",
      "Iteration 79, loss = 0.02256720\n",
      "Iteration 80, loss = 0.02235502\n",
      "Iteration 81, loss = 0.02213042\n",
      "Iteration 82, loss = 0.02171059\n",
      "Iteration 83, loss = 0.02139848\n",
      "Iteration 84, loss = 0.02087498\n",
      "Iteration 85, loss = 0.02029719\n",
      "Iteration 86, loss = 0.02014559\n",
      "Iteration 87, loss = 0.01971020\n",
      "Iteration 88, loss = 0.01932857\n",
      "Iteration 89, loss = 0.01895137\n",
      "Iteration 90, loss = 0.01874646\n",
      "Iteration 91, loss = 0.01853550\n",
      "Iteration 92, loss = 0.01823896\n",
      "Iteration 93, loss = 0.01799653\n",
      "Iteration 94, loss = 0.01772963\n",
      "Iteration 95, loss = 0.01745209\n",
      "Iteration 96, loss = 0.01724748\n",
      "Iteration 97, loss = 0.01732531\n",
      "Iteration 98, loss = 0.01697504\n",
      "Iteration 99, loss = 0.01678376\n",
      "Iteration 100, loss = 0.01672162\n",
      "Iteration 101, loss = 0.01653804\n",
      "Iteration 102, loss = 0.01645018\n",
      "Iteration 103, loss = 0.01627497\n",
      "Iteration 104, loss = 0.01621072\n",
      "Iteration 105, loss = 0.01605320\n",
      "Iteration 106, loss = 0.01594905\n",
      "Iteration 107, loss = 0.01583303\n",
      "Iteration 108, loss = 0.01567753\n",
      "Iteration 109, loss = 0.01554895\n",
      "Iteration 110, loss = 0.01530099\n",
      "Iteration 111, loss = 0.01533725\n",
      "Iteration 112, loss = 0.01505960\n",
      "Iteration 113, loss = 0.01488126\n",
      "Iteration 114, loss = 0.01476256\n",
      "Iteration 115, loss = 0.01464525\n",
      "Iteration 116, loss = 0.01452068\n",
      "Iteration 117, loss = 0.01442281\n",
      "Iteration 118, loss = 0.01430622\n",
      "Iteration 119, loss = 0.01426647\n",
      "Iteration 120, loss = 0.01422743\n",
      "Iteration 121, loss = 0.01415811\n",
      "Iteration 122, loss = 0.01412440\n",
      "Iteration 123, loss = 0.01406415\n",
      "Iteration 124, loss = 0.01404048\n",
      "Iteration 125, loss = 0.01398131\n",
      "Iteration 126, loss = 0.01394603\n",
      "Iteration 127, loss = 0.01391775\n",
      "Iteration 128, loss = 0.01384116\n",
      "Iteration 129, loss = 0.01399662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(7,2,4),\n",
    "                     activation = 'logistic',\n",
    "                     solver = 'adam',\n",
    "                     verbose = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7112612027536044\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             3847         2187\n",
      "Cazyme                   36         1629\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "#confusion matrix\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Using PUL file entirely for inference produces poor results, worse than trivial classifier (78.4%) - why??\n",
    "- positive prediction is much better than negative\n",
    "- Increasing number of layers gives better results (10 layers: 75.1%) for (a) but worse for (c) (10 layers: 80.8%) (why)\n",
    "\n",
    "**2 layers:**\n",
    "- 74.7% (100,100); 76.2% (10,10)\n",
    "\n",
    "**3 layers:**\n",
    "- 76.2% (100,100,100)\n",
    "\n",
    "**4 layers:**\n",
    " - relu: 72.6% (10,10,10,10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RandomizedSearchCV for hyperparameter tuning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = []\n",
    "for i in range(2,10):\n",
    "    for j in range(2,10):\n",
    "        for k in range(2,10):\n",
    "            templist.append((i,j,k))\n",
    "\n",
    "distributions = {'hidden_layer_sizes':templist,\n",
    "              'activation':('logistic','relu')\n",
    "              }\n",
    "\n",
    "#layer norm\n",
    "# regularise weights\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(estimator=mlp, param_distributions=distributions, cv=5, verbose=1) #, refit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: 'hidden_layer_sizes': (7, 2, 4), 'activation': 'logistic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65141205\n",
      "Iteration 2, loss = 0.64759013\n",
      "Iteration 3, loss = 0.64456793\n",
      "Iteration 4, loss = 0.64223068\n",
      "Iteration 5, loss = 0.64046713\n",
      "Iteration 6, loss = 0.63918862\n",
      "Iteration 7, loss = 0.63826718\n",
      "Iteration 8, loss = 0.63767913\n",
      "Iteration 9, loss = 0.63724264\n",
      "Iteration 10, loss = 0.63695860\n",
      "Iteration 11, loss = 0.63679267\n",
      "Iteration 12, loss = 0.63667419\n",
      "Iteration 13, loss = 0.63660843\n",
      "Iteration 14, loss = 0.63657262\n",
      "Iteration 15, loss = 0.63655618\n",
      "Iteration 16, loss = 0.63653473\n",
      "Iteration 17, loss = 0.63655096\n",
      "Iteration 18, loss = 0.63652591\n",
      "Iteration 19, loss = 0.63652641\n",
      "Iteration 20, loss = 0.63652360\n",
      "Iteration 21, loss = 0.63653672\n",
      "Iteration 22, loss = 0.63652034\n",
      "Iteration 23, loss = 0.63653035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64220063\n",
      "Iteration 2, loss = 0.51168219\n",
      "Iteration 3, loss = 0.41325286\n",
      "Iteration 4, loss = 0.33292991\n",
      "Iteration 5, loss = 0.27170699\n",
      "Iteration 6, loss = 0.22433698\n",
      "Iteration 7, loss = 0.18762370\n",
      "Iteration 8, loss = 0.15899564\n",
      "Iteration 9, loss = 0.13830963\n",
      "Iteration 10, loss = 0.12051187\n",
      "Iteration 11, loss = 0.10627104\n",
      "Iteration 12, loss = 0.09456215\n",
      "Iteration 13, loss = 0.08496506\n",
      "Iteration 14, loss = 0.07781330\n",
      "Iteration 15, loss = 0.07166806\n",
      "Iteration 16, loss = 0.06655945\n",
      "Iteration 17, loss = 0.06186687\n",
      "Iteration 18, loss = 0.05771017\n",
      "Iteration 19, loss = 0.05411061\n",
      "Iteration 20, loss = 0.05070997\n",
      "Iteration 21, loss = 0.04777720\n",
      "Iteration 22, loss = 0.04516428\n",
      "Iteration 23, loss = 0.04257904\n",
      "Iteration 24, loss = 0.04032902\n",
      "Iteration 25, loss = 0.03801184\n",
      "Iteration 26, loss = 0.03619472\n",
      "Iteration 27, loss = 0.03450025\n",
      "Iteration 28, loss = 0.03303206\n",
      "Iteration 29, loss = 0.03150475\n",
      "Iteration 30, loss = 0.03028433\n",
      "Iteration 31, loss = 0.02904482\n",
      "Iteration 32, loss = 0.02773270\n",
      "Iteration 33, loss = 0.02681842\n",
      "Iteration 34, loss = 0.02580102\n",
      "Iteration 35, loss = 0.02507480\n",
      "Iteration 36, loss = 0.02419334\n",
      "Iteration 37, loss = 0.02332130\n",
      "Iteration 38, loss = 0.02257127\n",
      "Iteration 39, loss = 0.02199708\n",
      "Iteration 40, loss = 0.02131857\n",
      "Iteration 41, loss = 0.02061587\n",
      "Iteration 42, loss = 0.01981952\n",
      "Iteration 43, loss = 0.01914109\n",
      "Iteration 44, loss = 0.01885411\n",
      "Iteration 45, loss = 0.01816322\n",
      "Iteration 46, loss = 0.01764275\n",
      "Iteration 47, loss = 0.01724546\n",
      "Iteration 48, loss = 0.01701404\n",
      "Iteration 49, loss = 0.01635313\n",
      "Iteration 50, loss = 0.01627101\n",
      "Iteration 51, loss = 0.01586415\n",
      "Iteration 52, loss = 0.01529387\n",
      "Iteration 53, loss = 0.01488988\n",
      "Iteration 54, loss = 0.01457749\n",
      "Iteration 55, loss = 0.01428931\n",
      "Iteration 56, loss = 0.01420342\n",
      "Iteration 57, loss = 0.01377710\n",
      "Iteration 58, loss = 0.01346026\n",
      "Iteration 59, loss = 0.01337603\n",
      "Iteration 60, loss = 0.01302265\n",
      "Iteration 61, loss = 0.01291177\n",
      "Iteration 62, loss = 0.01264159\n",
      "Iteration 63, loss = 0.01263879\n",
      "Iteration 64, loss = 0.01240277\n",
      "Iteration 65, loss = 0.01225430\n",
      "Iteration 66, loss = 0.01228160\n",
      "Iteration 67, loss = 0.01229675\n",
      "Iteration 68, loss = 0.01208044\n",
      "Iteration 69, loss = 0.01201918\n",
      "Iteration 70, loss = 0.01194339\n",
      "Iteration 71, loss = 0.01182857\n",
      "Iteration 72, loss = 0.01174655\n",
      "Iteration 73, loss = 0.01166917\n",
      "Iteration 74, loss = 0.01154516\n",
      "Iteration 75, loss = 0.01148431\n",
      "Iteration 76, loss = 0.01138206\n",
      "Iteration 77, loss = 0.01135203\n",
      "Iteration 78, loss = 0.01127484\n",
      "Iteration 79, loss = 0.01122660\n",
      "Iteration 80, loss = 0.01118174\n",
      "Iteration 81, loss = 0.01115863\n",
      "Iteration 82, loss = 0.01111232\n",
      "Iteration 83, loss = 0.01108108\n",
      "Iteration 84, loss = 0.01105724\n",
      "Iteration 85, loss = 0.01727222\n",
      "Iteration 86, loss = 0.01174667\n",
      "Iteration 87, loss = 0.01131771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.01842592\n",
      "Iteration 2, loss = 0.95115376\n",
      "Iteration 3, loss = 0.89996187\n",
      "Iteration 4, loss = 0.85880705\n",
      "Iteration 5, loss = 0.79134211\n",
      "Iteration 6, loss = 0.55738655\n",
      "Iteration 7, loss = 0.34344252\n",
      "Iteration 8, loss = 0.25377782\n",
      "Iteration 9, loss = 0.21059281\n",
      "Iteration 10, loss = 0.17675993\n",
      "Iteration 11, loss = 0.12884461\n",
      "Iteration 12, loss = 0.08119025\n",
      "Iteration 13, loss = 0.06418945\n",
      "Iteration 14, loss = 0.05641323\n",
      "Iteration 15, loss = 0.05151560\n",
      "Iteration 16, loss = 0.04746650\n",
      "Iteration 17, loss = 0.04433022\n",
      "Iteration 18, loss = 0.04168728\n",
      "Iteration 19, loss = 0.03908192\n",
      "Iteration 20, loss = 0.03682507\n",
      "Iteration 21, loss = 0.03503454\n",
      "Iteration 22, loss = 0.03323998\n",
      "Iteration 23, loss = 0.03162002\n",
      "Iteration 24, loss = 0.03015363\n",
      "Iteration 25, loss = 0.02895134\n",
      "Iteration 26, loss = 0.02768941\n",
      "Iteration 27, loss = 0.02659825\n",
      "Iteration 28, loss = 0.02555460\n",
      "Iteration 29, loss = 0.02566666\n",
      "Iteration 30, loss = 0.02403692\n",
      "Iteration 31, loss = 0.02307360\n",
      "Iteration 32, loss = 0.02256238\n",
      "Iteration 33, loss = 0.02184794\n",
      "Iteration 34, loss = 0.02109557\n",
      "Iteration 35, loss = 0.02051302\n",
      "Iteration 36, loss = 0.01999223\n",
      "Iteration 37, loss = 0.01946813\n",
      "Iteration 38, loss = 0.01888230\n",
      "Iteration 39, loss = 0.01847432\n",
      "Iteration 40, loss = 0.01781816\n",
      "Iteration 41, loss = 0.01751149\n",
      "Iteration 42, loss = 0.01706642\n",
      "Iteration 43, loss = 0.01659185\n",
      "Iteration 44, loss = 0.01603328\n",
      "Iteration 45, loss = 0.01559912\n",
      "Iteration 46, loss = 0.01527819\n",
      "Iteration 47, loss = 0.01489244\n",
      "Iteration 48, loss = 0.01441542\n",
      "Iteration 49, loss = 0.01419107\n",
      "Iteration 50, loss = 0.01367997\n",
      "Iteration 51, loss = 0.01338074\n",
      "Iteration 52, loss = 0.01304503\n",
      "Iteration 53, loss = 0.01265391\n",
      "Iteration 54, loss = 0.01257232\n",
      "Iteration 55, loss = 0.01189708\n",
      "Iteration 56, loss = 0.01170313\n",
      "Iteration 57, loss = 0.01142799\n",
      "Iteration 58, loss = 0.01107617\n",
      "Iteration 59, loss = 0.01073746\n",
      "Iteration 60, loss = 0.01060625\n",
      "Iteration 61, loss = 0.01021162\n",
      "Iteration 62, loss = 0.00980382\n",
      "Iteration 63, loss = 0.00963069\n",
      "Iteration 64, loss = 0.00928292\n",
      "Iteration 65, loss = 0.00911596\n",
      "Iteration 66, loss = 0.00885782\n",
      "Iteration 67, loss = 0.00856814\n",
      "Iteration 68, loss = 0.00837920\n",
      "Iteration 69, loss = 0.00813404\n",
      "Iteration 70, loss = 0.00779781\n",
      "Iteration 71, loss = 0.00762153\n",
      "Iteration 72, loss = 0.00744729\n",
      "Iteration 73, loss = 0.00720629\n",
      "Iteration 74, loss = 0.00695399\n",
      "Iteration 75, loss = 0.00675237\n",
      "Iteration 76, loss = 0.00659948\n",
      "Iteration 77, loss = 0.00651314\n",
      "Iteration 78, loss = 0.00625848\n",
      "Iteration 79, loss = 0.00604333\n",
      "Iteration 80, loss = 0.00594137\n",
      "Iteration 81, loss = 0.00576482\n",
      "Iteration 82, loss = 0.00563699\n",
      "Iteration 83, loss = 0.00544789\n",
      "Iteration 84, loss = 0.00528396\n",
      "Iteration 85, loss = 0.00508950\n",
      "Iteration 86, loss = 0.00503073\n",
      "Iteration 87, loss = 0.00485435\n",
      "Iteration 88, loss = 0.00477559\n",
      "Iteration 89, loss = 0.00460749\n",
      "Iteration 90, loss = 0.00447522\n",
      "Iteration 91, loss = 0.00433418\n",
      "Iteration 92, loss = 0.00425482\n",
      "Iteration 93, loss = 0.00411199\n",
      "Iteration 94, loss = 0.00400334\n",
      "Iteration 95, loss = 0.00392494\n",
      "Iteration 96, loss = 0.00383068\n",
      "Iteration 97, loss = 0.00364799\n",
      "Iteration 98, loss = 0.00356576\n",
      "Iteration 99, loss = 0.00344946\n",
      "Iteration 100, loss = 0.00335177\n",
      "Iteration 101, loss = 0.00327146\n",
      "Iteration 102, loss = 0.00315443\n",
      "Iteration 103, loss = 0.00306147\n",
      "Iteration 104, loss = 0.00297949\n",
      "Iteration 105, loss = 0.00287750\n",
      "Iteration 106, loss = 0.00276339\n",
      "Iteration 107, loss = 0.00267665\n",
      "Iteration 108, loss = 0.00258009\n",
      "Iteration 109, loss = 0.00252261\n",
      "Iteration 110, loss = 0.00242158\n",
      "Iteration 111, loss = 0.00231220\n",
      "Iteration 112, loss = 0.00221615\n",
      "Iteration 113, loss = 0.00212767\n",
      "Iteration 114, loss = 0.00205237\n",
      "Iteration 115, loss = 0.00195748\n",
      "Iteration 116, loss = 0.00192055\n",
      "Iteration 117, loss = 0.00179727\n",
      "Iteration 118, loss = 0.00172454\n",
      "Iteration 119, loss = 0.00166069\n",
      "Iteration 120, loss = 0.00153922\n",
      "Iteration 121, loss = 0.00148046\n",
      "Iteration 122, loss = 0.00139862\n",
      "Iteration 123, loss = 0.00134305\n",
      "Iteration 124, loss = 0.00127683\n",
      "Iteration 125, loss = 0.00118543\n",
      "Iteration 126, loss = 0.00119647\n",
      "Iteration 127, loss = 0.00106874\n",
      "Iteration 128, loss = 0.00099281\n",
      "Iteration 129, loss = 0.00096634\n",
      "Iteration 130, loss = 0.00088043\n",
      "Iteration 131, loss = 0.00082284\n",
      "Iteration 132, loss = 0.00076853\n",
      "Iteration 133, loss = 0.00073231\n",
      "Iteration 134, loss = 0.00069401\n",
      "Iteration 135, loss = 0.00063924\n",
      "Iteration 136, loss = 0.00059734\n",
      "Iteration 137, loss = 0.00056420\n",
      "Iteration 138, loss = 0.00052685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.92261489\n",
      "Iteration 2, loss = 0.90108939\n",
      "Iteration 3, loss = 0.88114728\n",
      "Iteration 4, loss = 0.86156104\n",
      "Iteration 5, loss = 0.84309219\n",
      "Iteration 6, loss = 0.82642762\n",
      "Iteration 7, loss = 0.81091565\n",
      "Iteration 8, loss = 0.79567695\n",
      "Iteration 9, loss = 0.78132088\n",
      "Iteration 10, loss = 0.76865404\n",
      "Iteration 11, loss = 0.75630395\n",
      "Iteration 12, loss = 0.74474687\n",
      "Iteration 13, loss = 0.73469816\n",
      "Iteration 14, loss = 0.72488166\n",
      "Iteration 15, loss = 0.71576051\n",
      "Iteration 16, loss = 0.70739399\n",
      "Iteration 17, loss = 0.70029672\n",
      "Iteration 18, loss = 0.69332613\n",
      "Iteration 19, loss = 0.68691324\n",
      "Iteration 20, loss = 0.68106805\n",
      "Iteration 21, loss = 0.67578882\n",
      "Iteration 22, loss = 0.67102546\n",
      "Iteration 23, loss = 0.66715851\n",
      "Iteration 24, loss = 0.66374386\n",
      "Iteration 25, loss = 0.66027972\n",
      "Iteration 26, loss = 0.65747403\n",
      "Iteration 27, loss = 0.65505536\n",
      "Iteration 28, loss = 0.65252014\n",
      "Iteration 29, loss = 0.65021629\n",
      "Iteration 30, loss = 0.64821542\n",
      "Iteration 31, loss = 0.64643376\n",
      "Iteration 32, loss = 0.64489685\n",
      "Iteration 33, loss = 0.64355816\n",
      "Iteration 34, loss = 0.64239831\n",
      "Iteration 35, loss = 0.64139857\n",
      "Iteration 36, loss = 0.64076317\n",
      "Iteration 37, loss = 0.64004226\n",
      "Iteration 38, loss = 0.63959173\n",
      "Iteration 39, loss = 0.63922170\n",
      "Iteration 40, loss = 0.63889829\n",
      "Iteration 41, loss = 0.63846146\n",
      "Iteration 42, loss = 0.63806290\n",
      "Iteration 43, loss = 0.63773918\n",
      "Iteration 44, loss = 0.63747771\n",
      "Iteration 45, loss = 0.63724926\n",
      "Iteration 46, loss = 0.63708603\n",
      "Iteration 47, loss = 0.63702602\n",
      "Iteration 48, loss = 0.63691437\n",
      "Iteration 49, loss = 0.63681495\n",
      "Iteration 50, loss = 0.63679714\n",
      "Iteration 51, loss = 0.63673714\n",
      "Iteration 52, loss = 0.63667226\n",
      "Iteration 53, loss = 0.63663123\n",
      "Iteration 54, loss = 0.63659437\n",
      "Iteration 55, loss = 0.63658645\n",
      "Iteration 56, loss = 0.63656615\n",
      "Iteration 57, loss = 0.63655695\n",
      "Iteration 58, loss = 0.63656626\n",
      "Iteration 59, loss = 0.63656445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64259515\n",
      "Iteration 2, loss = 0.63865855\n",
      "Iteration 3, loss = 0.63704445\n",
      "Iteration 4, loss = 0.63665087\n",
      "Iteration 5, loss = 0.63656212\n",
      "Iteration 6, loss = 0.63658195\n",
      "Iteration 7, loss = 0.63661663\n",
      "Iteration 8, loss = 0.63658232\n",
      "Iteration 9, loss = 0.63661262\n",
      "Iteration 10, loss = 0.63654256\n",
      "Iteration 11, loss = 0.63660916\n",
      "Iteration 12, loss = 0.63659821\n",
      "Iteration 13, loss = 0.63657988\n",
      "Iteration 14, loss = 0.63657719\n",
      "Iteration 15, loss = 0.63657201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08762343\n",
      "Iteration 2, loss = 1.02014983\n",
      "Iteration 3, loss = 0.99370100\n",
      "Iteration 4, loss = 0.96844067\n",
      "Iteration 5, loss = 0.94304983\n",
      "Iteration 6, loss = 0.91513547\n",
      "Iteration 7, loss = 0.87029633\n",
      "Iteration 8, loss = 0.80168508\n",
      "Iteration 9, loss = 0.75603726\n",
      "Iteration 10, loss = 0.72346342\n",
      "Iteration 11, loss = 0.69426991\n",
      "Iteration 12, loss = 0.66726029\n",
      "Iteration 13, loss = 0.64195324\n",
      "Iteration 14, loss = 0.61790186\n",
      "Iteration 15, loss = 0.59469674\n",
      "Iteration 16, loss = 0.57285885\n",
      "Iteration 17, loss = 0.55156697\n",
      "Iteration 18, loss = 0.53151945\n",
      "Iteration 19, loss = 0.51214581\n",
      "Iteration 20, loss = 0.49346182\n",
      "Iteration 21, loss = 0.47555909\n",
      "Iteration 22, loss = 0.45836108\n",
      "Iteration 23, loss = 0.44208013\n",
      "Iteration 24, loss = 0.42608113\n",
      "Iteration 25, loss = 0.41095802\n",
      "Iteration 26, loss = 0.39617568\n",
      "Iteration 27, loss = 0.38203149\n",
      "Iteration 28, loss = 0.36863715\n",
      "Iteration 29, loss = 0.35568108\n",
      "Iteration 30, loss = 0.34323979\n",
      "Iteration 31, loss = 0.33114488\n",
      "Iteration 32, loss = 0.31964030\n",
      "Iteration 33, loss = 0.30847386\n",
      "Iteration 34, loss = 0.29792296\n",
      "Iteration 35, loss = 0.28771263\n",
      "Iteration 36, loss = 0.27806139\n",
      "Iteration 37, loss = 0.26865356\n",
      "Iteration 38, loss = 0.25966430\n",
      "Iteration 39, loss = 0.25099071\n",
      "Iteration 40, loss = 0.24276366\n",
      "Iteration 41, loss = 0.23483271\n",
      "Iteration 42, loss = 0.22733704\n",
      "Iteration 43, loss = 0.21995899\n",
      "Iteration 44, loss = 0.21279574\n",
      "Iteration 45, loss = 0.20600929\n",
      "Iteration 46, loss = 0.19947317\n",
      "Iteration 47, loss = 0.19325478\n",
      "Iteration 48, loss = 0.18721514\n",
      "Iteration 49, loss = 0.18138786\n",
      "Iteration 50, loss = 0.17585487\n",
      "Iteration 51, loss = 0.17050996\n",
      "Iteration 52, loss = 0.16538053\n",
      "Iteration 53, loss = 0.16040134\n",
      "Iteration 54, loss = 0.15563515\n",
      "Iteration 55, loss = 0.15105809\n",
      "Iteration 56, loss = 0.14667445\n",
      "Iteration 57, loss = 0.14242338\n",
      "Iteration 58, loss = 0.13835028\n",
      "Iteration 59, loss = 0.13439235\n",
      "Iteration 60, loss = 0.13060105\n",
      "Iteration 61, loss = 0.12694740\n",
      "Iteration 62, loss = 0.12342170\n",
      "Iteration 63, loss = 0.12002719\n",
      "Iteration 64, loss = 0.11674396\n",
      "Iteration 65, loss = 0.11358842\n",
      "Iteration 66, loss = 0.11052611\n",
      "Iteration 67, loss = 0.10758821\n",
      "Iteration 68, loss = 0.10477179\n",
      "Iteration 69, loss = 0.10222891\n",
      "Iteration 70, loss = 0.09932410\n",
      "Iteration 71, loss = 0.09679408\n",
      "Iteration 72, loss = 0.09422288\n",
      "Iteration 73, loss = 0.09180430\n",
      "Iteration 74, loss = 0.08947809\n",
      "Iteration 75, loss = 0.08724872\n",
      "Iteration 76, loss = 0.08509242\n",
      "Iteration 77, loss = 0.08301820\n",
      "Iteration 78, loss = 0.08101958\n",
      "Iteration 79, loss = 0.07907015\n",
      "Iteration 80, loss = 0.07718520\n",
      "Iteration 81, loss = 0.07539626\n",
      "Iteration 82, loss = 0.07365231\n",
      "Iteration 83, loss = 0.07192850\n",
      "Iteration 84, loss = 0.07029615\n",
      "Iteration 85, loss = 0.06870666\n",
      "Iteration 86, loss = 0.06717524\n",
      "Iteration 87, loss = 0.06569037\n",
      "Iteration 88, loss = 0.06426200\n",
      "Iteration 89, loss = 0.06286706\n",
      "Iteration 90, loss = 0.06151357\n",
      "Iteration 91, loss = 0.06021400\n",
      "Iteration 92, loss = 0.05894686\n",
      "Iteration 93, loss = 0.05772987\n",
      "Iteration 94, loss = 0.05655075\n",
      "Iteration 95, loss = 0.05541053\n",
      "Iteration 96, loss = 0.05429426\n",
      "Iteration 97, loss = 0.05322836\n",
      "Iteration 98, loss = 0.05218751\n",
      "Iteration 99, loss = 0.05118009\n",
      "Iteration 100, loss = 0.05021522\n",
      "Iteration 101, loss = 0.04926848\n",
      "Iteration 102, loss = 0.04835180\n",
      "Iteration 103, loss = 0.04747308\n",
      "Iteration 104, loss = 0.04662370\n",
      "Iteration 105, loss = 0.04579846\n",
      "Iteration 106, loss = 0.04499082\n",
      "Iteration 107, loss = 0.04420696\n",
      "Iteration 108, loss = 0.04345859\n",
      "Iteration 109, loss = 0.04272696\n",
      "Iteration 110, loss = 0.04203106\n",
      "Iteration 111, loss = 0.04134694\n",
      "Iteration 112, loss = 0.04068067\n",
      "Iteration 113, loss = 0.04003541\n",
      "Iteration 114, loss = 0.03942312\n",
      "Iteration 115, loss = 0.03881548\n",
      "Iteration 116, loss = 0.03822846\n",
      "Iteration 117, loss = 0.03766548\n",
      "Iteration 118, loss = 0.03711984\n",
      "Iteration 119, loss = 0.03660259\n",
      "Iteration 120, loss = 0.03607864\n",
      "Iteration 121, loss = 0.03558227\n",
      "Iteration 122, loss = 0.03510477\n",
      "Iteration 123, loss = 0.03471007\n",
      "Iteration 124, loss = 0.03677029\n",
      "Iteration 125, loss = 0.03371421\n",
      "Iteration 126, loss = 0.03118698\n",
      "Iteration 127, loss = 0.03031018\n",
      "Iteration 128, loss = 0.02937087\n",
      "Iteration 129, loss = 0.02889285\n",
      "Iteration 130, loss = 0.02844201\n",
      "Iteration 131, loss = 0.02803922\n",
      "Iteration 132, loss = 0.02764807\n",
      "Iteration 133, loss = 0.02727525\n",
      "Iteration 134, loss = 0.02691430\n",
      "Iteration 135, loss = 0.02656640\n",
      "Iteration 136, loss = 0.02622693\n",
      "Iteration 137, loss = 0.02590290\n",
      "Iteration 138, loss = 0.02558781\n",
      "Iteration 139, loss = 0.02528301\n",
      "Iteration 140, loss = 0.02498894\n",
      "Iteration 141, loss = 0.02470477\n",
      "Iteration 142, loss = 0.02442734\n",
      "Iteration 143, loss = 0.02416233\n",
      "Iteration 144, loss = 0.02390569\n",
      "Iteration 145, loss = 0.02365629\n",
      "Iteration 146, loss = 0.02341552\n",
      "Iteration 147, loss = 0.02318158\n",
      "Iteration 148, loss = 0.02295733\n",
      "Iteration 149, loss = 0.02273881\n",
      "Iteration 150, loss = 0.02252884\n",
      "Iteration 151, loss = 0.02232672\n",
      "Iteration 152, loss = 0.02212850\n",
      "Iteration 153, loss = 0.02193972\n",
      "Iteration 154, loss = 0.02175674\n",
      "Iteration 155, loss = 0.02158012\n",
      "Iteration 156, loss = 0.02140830\n",
      "Iteration 157, loss = 0.02124497\n",
      "Iteration 158, loss = 0.02108482\n",
      "Iteration 159, loss = 0.02093136\n",
      "Iteration 160, loss = 0.02078369\n",
      "Iteration 161, loss = 0.02064147\n",
      "Iteration 162, loss = 0.02050217\n",
      "Iteration 163, loss = 0.02037017\n",
      "Iteration 164, loss = 0.02024341\n",
      "Iteration 165, loss = 0.02011880\n",
      "Iteration 166, loss = 0.02000147\n",
      "Iteration 167, loss = 0.01988615\n",
      "Iteration 168, loss = 0.01977631\n",
      "Iteration 169, loss = 0.01967197\n",
      "Iteration 170, loss = 0.01956788\n",
      "Iteration 171, loss = 0.01946989\n",
      "Iteration 172, loss = 0.01937634\n",
      "Iteration 173, loss = 0.01928581\n",
      "Iteration 174, loss = 0.01919854\n",
      "Iteration 175, loss = 0.01911549\n",
      "Iteration 176, loss = 0.01903484\n",
      "Iteration 177, loss = 0.01895807\n",
      "Iteration 178, loss = 0.01888397\n",
      "Iteration 179, loss = 0.01881266\n",
      "Iteration 180, loss = 0.01874591\n",
      "Iteration 181, loss = 0.01867855\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55238146\n",
      "Iteration 2, loss = 0.36775048\n",
      "Iteration 3, loss = 0.28566574\n",
      "Iteration 4, loss = 0.25652423\n",
      "Iteration 5, loss = 0.23377544\n",
      "Iteration 6, loss = 0.21624299\n",
      "Iteration 7, loss = 0.20126791\n",
      "Iteration 8, loss = 0.18871550\n",
      "Iteration 9, loss = 0.17893619\n",
      "Iteration 10, loss = 0.17023435\n",
      "Iteration 11, loss = 0.16165652\n",
      "Iteration 12, loss = 0.15420168\n",
      "Iteration 13, loss = 0.14774292\n",
      "Iteration 14, loss = 0.14098010\n",
      "Iteration 15, loss = 0.13490805\n",
      "Iteration 16, loss = 0.12920641\n",
      "Iteration 17, loss = 0.12425224\n",
      "Iteration 18, loss = 0.11887665\n",
      "Iteration 19, loss = 0.11415091\n",
      "Iteration 20, loss = 0.10967735\n",
      "Iteration 21, loss = 0.10540577\n",
      "Iteration 22, loss = 0.10157748\n",
      "Iteration 23, loss = 0.09771002\n",
      "Iteration 24, loss = 0.09779872\n",
      "Iteration 25, loss = 0.09161163\n",
      "Iteration 26, loss = 0.08811111\n",
      "Iteration 27, loss = 0.08515591\n",
      "Iteration 28, loss = 0.08210607\n",
      "Iteration 29, loss = 0.07964216\n",
      "Iteration 30, loss = 0.07683856\n",
      "Iteration 31, loss = 0.07454398\n",
      "Iteration 32, loss = 0.07218994\n",
      "Iteration 33, loss = 0.07012998\n",
      "Iteration 34, loss = 0.06783586\n",
      "Iteration 35, loss = 0.06582255\n",
      "Iteration 36, loss = 0.06389191\n",
      "Iteration 37, loss = 0.06194218\n",
      "Iteration 38, loss = 0.06012078\n",
      "Iteration 39, loss = 0.05836261\n",
      "Iteration 40, loss = 0.05635037\n",
      "Iteration 41, loss = 0.05409494\n",
      "Iteration 42, loss = 0.05264884\n",
      "Iteration 43, loss = 0.05076850\n",
      "Iteration 44, loss = 0.04934007\n",
      "Iteration 45, loss = 0.04785137\n",
      "Iteration 46, loss = 0.04663589\n",
      "Iteration 47, loss = 0.04539018\n",
      "Iteration 48, loss = 0.04418319\n",
      "Iteration 49, loss = 0.04300828\n",
      "Iteration 50, loss = 0.04193715\n",
      "Iteration 51, loss = 0.04097689\n",
      "Iteration 52, loss = 0.03995997\n",
      "Iteration 53, loss = 0.03905951\n",
      "Iteration 54, loss = 0.03818780\n",
      "Iteration 55, loss = 0.03733628\n",
      "Iteration 56, loss = 0.03660237\n",
      "Iteration 57, loss = 0.03574708\n",
      "Iteration 58, loss = 0.03503931\n",
      "Iteration 59, loss = 0.03431616\n",
      "Iteration 60, loss = 0.03365258\n",
      "Iteration 61, loss = 0.03298825\n",
      "Iteration 62, loss = 0.03234859\n",
      "Iteration 63, loss = 0.03174418\n",
      "Iteration 64, loss = 0.03116902\n",
      "Iteration 65, loss = 0.03059428\n",
      "Iteration 66, loss = 0.03007171\n",
      "Iteration 67, loss = 0.02954535\n",
      "Iteration 68, loss = 0.02905068\n",
      "Iteration 69, loss = 0.02890797\n",
      "Iteration 70, loss = 0.02857161\n",
      "Iteration 71, loss = 0.02820929\n",
      "Iteration 72, loss = 0.02784332\n",
      "Iteration 73, loss = 0.02748097\n",
      "Iteration 74, loss = 0.02713887\n",
      "Iteration 75, loss = 0.02679252\n",
      "Iteration 76, loss = 0.02646527\n",
      "Iteration 77, loss = 0.02613574\n",
      "Iteration 78, loss = 0.02582337\n",
      "Iteration 79, loss = 0.02552622\n",
      "Iteration 80, loss = 0.02523234\n",
      "Iteration 81, loss = 0.02493985\n",
      "Iteration 82, loss = 0.02465575\n",
      "Iteration 83, loss = 0.02438310\n",
      "Iteration 84, loss = 0.02410694\n",
      "Iteration 85, loss = 0.02384227\n",
      "Iteration 86, loss = 0.02359362\n",
      "Iteration 87, loss = 0.02335207\n",
      "Iteration 88, loss = 0.02311497\n",
      "Iteration 89, loss = 0.02287758\n",
      "Iteration 90, loss = 0.02264971\n",
      "Iteration 91, loss = 0.02243030\n",
      "Iteration 92, loss = 0.02221463\n",
      "Iteration 93, loss = 0.02200733\n",
      "Iteration 94, loss = 0.02180942\n",
      "Iteration 95, loss = 0.02161434\n",
      "Iteration 96, loss = 0.02142059\n",
      "Iteration 97, loss = 0.02123700\n",
      "Iteration 98, loss = 0.02106342\n",
      "Iteration 99, loss = 0.02088833\n",
      "Iteration 100, loss = 0.02072257\n",
      "Iteration 101, loss = 0.02055798\n",
      "Iteration 102, loss = 0.02040223\n",
      "Iteration 103, loss = 0.02024832\n",
      "Iteration 104, loss = 0.02010523\n",
      "Iteration 105, loss = 0.01995506\n",
      "Iteration 106, loss = 0.01981792\n",
      "Iteration 107, loss = 0.01968012\n",
      "Iteration 108, loss = 0.01954661\n",
      "Iteration 109, loss = 0.01942186\n",
      "Iteration 110, loss = 0.01929741\n",
      "Iteration 111, loss = 0.01917869\n",
      "Iteration 112, loss = 0.01906220\n",
      "Iteration 113, loss = 0.01895158\n",
      "Iteration 114, loss = 0.01884096\n",
      "Iteration 115, loss = 0.01874106\n",
      "Iteration 116, loss = 0.01864081\n",
      "Iteration 117, loss = 0.01854378\n",
      "Iteration 118, loss = 0.01845081\n",
      "Iteration 119, loss = 0.01836143\n",
      "Iteration 120, loss = 0.01827475\n",
      "Iteration 121, loss = 0.01819301\n",
      "Iteration 122, loss = 0.01811148\n",
      "Iteration 123, loss = 0.01803691\n",
      "Iteration 124, loss = 0.02165144\n",
      "Iteration 125, loss = 0.03463432\n",
      "Iteration 126, loss = 0.02022062\n",
      "Iteration 127, loss = 0.01789507\n",
      "Iteration 128, loss = 0.01725585\n",
      "Iteration 129, loss = 0.01702203\n",
      "Iteration 130, loss = 0.01686769\n",
      "Iteration 131, loss = 0.01671439\n",
      "Iteration 132, loss = 0.01661802\n",
      "Iteration 133, loss = 0.01652199\n",
      "Iteration 134, loss = 0.01645169\n",
      "Iteration 135, loss = 0.01638033\n",
      "Iteration 136, loss = 0.01630146\n",
      "Iteration 137, loss = 0.01623877\n",
      "Iteration 138, loss = 0.01617616\n",
      "Iteration 139, loss = 0.01612323\n",
      "Iteration 140, loss = 0.01607390\n",
      "Iteration 141, loss = 0.01602470\n",
      "Iteration 142, loss = 0.01597494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58761509\n",
      "Iteration 2, loss = 0.39790111\n",
      "Iteration 3, loss = 0.18347359\n",
      "Iteration 4, loss = 0.08753247\n",
      "Iteration 5, loss = 0.06374479\n",
      "Iteration 6, loss = 0.05320473\n",
      "Iteration 7, loss = 0.04616068\n",
      "Iteration 8, loss = 0.04193124\n",
      "Iteration 9, loss = 0.03804704\n",
      "Iteration 10, loss = 0.03504639\n",
      "Iteration 11, loss = 0.04361953\n",
      "Iteration 12, loss = 0.03378648\n",
      "Iteration 13, loss = 0.03230719\n",
      "Iteration 14, loss = 0.03114977\n",
      "Iteration 15, loss = 0.03039827\n",
      "Iteration 16, loss = 0.02972281\n",
      "Iteration 17, loss = 0.02857217\n",
      "Iteration 18, loss = 0.02786944\n",
      "Iteration 19, loss = 0.02738865\n",
      "Iteration 20, loss = 0.02655726\n",
      "Iteration 21, loss = 0.02599433\n",
      "Iteration 22, loss = 0.02538521\n",
      "Iteration 23, loss = 0.02468649\n",
      "Iteration 24, loss = 0.02406765\n",
      "Iteration 25, loss = 0.02346570\n",
      "Iteration 26, loss = 0.02294520\n",
      "Iteration 27, loss = 0.02230598\n",
      "Iteration 28, loss = 0.02187652\n",
      "Iteration 29, loss = 0.02129930\n",
      "Iteration 30, loss = 0.02079084\n",
      "Iteration 31, loss = 0.02021918\n",
      "Iteration 32, loss = 0.02005579\n",
      "Iteration 33, loss = 0.01943210\n",
      "Iteration 34, loss = 0.01879471\n",
      "Iteration 35, loss = 0.01826427\n",
      "Iteration 36, loss = 0.01777208\n",
      "Iteration 37, loss = 0.01730405\n",
      "Iteration 38, loss = 0.01697441\n",
      "Iteration 39, loss = 0.01633716\n",
      "Iteration 40, loss = 0.01592755\n",
      "Iteration 41, loss = 0.01558286\n",
      "Iteration 42, loss = 0.01502218\n",
      "Iteration 43, loss = 0.01457794\n",
      "Iteration 44, loss = 0.01434887\n",
      "Iteration 45, loss = 0.01386828\n",
      "Iteration 46, loss = 0.01350759\n",
      "Iteration 47, loss = 0.01309851\n",
      "Iteration 48, loss = 0.01281147\n",
      "Iteration 49, loss = 0.01357872\n",
      "Iteration 50, loss = 0.01212319\n",
      "Iteration 51, loss = 0.01187891\n",
      "Iteration 52, loss = 0.01193142\n",
      "Iteration 53, loss = 0.01128264\n",
      "Iteration 54, loss = 0.01059057\n",
      "Iteration 55, loss = 0.01028771\n",
      "Iteration 56, loss = 0.00975904\n",
      "Iteration 57, loss = 0.00955813\n",
      "Iteration 58, loss = 0.00907954\n",
      "Iteration 59, loss = 0.00890091\n",
      "Iteration 60, loss = 0.00848621\n",
      "Iteration 61, loss = 0.00810097\n",
      "Iteration 62, loss = 0.00792557\n",
      "Iteration 63, loss = 0.00771217\n",
      "Iteration 64, loss = 0.00722281\n",
      "Iteration 65, loss = 0.01219827\n",
      "Iteration 66, loss = 0.00792487\n",
      "Iteration 67, loss = 0.00708570\n",
      "Iteration 68, loss = 0.00660188\n",
      "Iteration 69, loss = 0.00642412\n",
      "Iteration 70, loss = 0.00602104\n",
      "Iteration 71, loss = 0.00585813\n",
      "Iteration 72, loss = 0.00575808\n",
      "Iteration 73, loss = 0.00524181\n",
      "Iteration 74, loss = 0.00505052\n",
      "Iteration 75, loss = 0.00484855\n",
      "Iteration 76, loss = 0.00472117\n",
      "Iteration 77, loss = 0.00436437\n",
      "Iteration 78, loss = 0.00414148\n",
      "Iteration 79, loss = 0.00395212\n",
      "Iteration 80, loss = 0.00387895\n",
      "Iteration 81, loss = 0.00345966\n",
      "Iteration 82, loss = 0.00353778\n",
      "Iteration 83, loss = 0.00316918\n",
      "Iteration 84, loss = 0.00301867\n",
      "Iteration 85, loss = 0.00281740\n",
      "Iteration 86, loss = 0.00265667\n",
      "Iteration 87, loss = 0.00261259\n",
      "Iteration 88, loss = 0.00248188\n",
      "Iteration 89, loss = 0.00232685\n",
      "Iteration 90, loss = 0.00217581\n",
      "Iteration 91, loss = 0.00207733\n",
      "Iteration 92, loss = 0.00201168\n",
      "Iteration 93, loss = 0.00190814\n",
      "Iteration 94, loss = 0.00176623\n",
      "Iteration 95, loss = 0.00169677\n",
      "Iteration 96, loss = 0.00156819\n",
      "Iteration 97, loss = 0.00149947\n",
      "Iteration 98, loss = 0.00136827\n",
      "Iteration 99, loss = 0.00130736\n",
      "Iteration 100, loss = 0.00562306\n",
      "Iteration 101, loss = 0.00186724\n",
      "Iteration 102, loss = 0.00133781\n",
      "Iteration 103, loss = 0.00118514\n",
      "Iteration 104, loss = 0.00110915\n",
      "Iteration 105, loss = 0.00106446\n",
      "Iteration 106, loss = 0.00094178\n",
      "Iteration 107, loss = 0.00090167\n",
      "Iteration 108, loss = 0.00087010\n",
      "Iteration 109, loss = 0.00082230\n",
      "Iteration 110, loss = 0.00076410\n",
      "Iteration 111, loss = 0.00082430\n",
      "Iteration 112, loss = 0.00076697\n",
      "Iteration 113, loss = 0.00069051\n",
      "Iteration 114, loss = 0.00066226\n",
      "Iteration 115, loss = 0.00065806\n",
      "Iteration 116, loss = 0.00060764\n",
      "Iteration 117, loss = 0.00057671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52283145\n",
      "Iteration 2, loss = 0.34873590\n",
      "Iteration 3, loss = 0.27118852\n",
      "Iteration 4, loss = 0.22361780\n",
      "Iteration 5, loss = 0.18783670\n",
      "Iteration 6, loss = 0.16003203\n",
      "Iteration 7, loss = 0.12956169\n",
      "Iteration 8, loss = 0.07134230\n",
      "Iteration 9, loss = 0.04738794\n",
      "Iteration 10, loss = 0.03590988\n",
      "Iteration 11, loss = 0.03078901\n",
      "Iteration 12, loss = 0.02784030\n",
      "Iteration 13, loss = 0.02523943\n",
      "Iteration 14, loss = 0.02230883\n",
      "Iteration 15, loss = 0.02118015\n",
      "Iteration 16, loss = 0.01980335\n",
      "Iteration 17, loss = 0.01799175\n",
      "Iteration 18, loss = 0.01711538\n",
      "Iteration 19, loss = 0.01606957\n",
      "Iteration 20, loss = 0.01519832\n",
      "Iteration 21, loss = 0.01976934\n",
      "Iteration 22, loss = 0.01447521\n",
      "Iteration 23, loss = 0.01356579\n",
      "Iteration 24, loss = 0.01229466\n",
      "Iteration 25, loss = 0.01193905\n",
      "Iteration 26, loss = 0.01129992\n",
      "Iteration 27, loss = 0.01495875\n",
      "Iteration 28, loss = 0.01073109\n",
      "Iteration 29, loss = 0.01026737\n",
      "Iteration 30, loss = 0.00950195\n",
      "Iteration 31, loss = 0.00908668\n",
      "Iteration 32, loss = 0.00832078\n",
      "Iteration 33, loss = 0.00809188\n",
      "Iteration 34, loss = 0.00752999\n",
      "Iteration 35, loss = 0.00735147\n",
      "Iteration 36, loss = 0.00689896\n",
      "Iteration 37, loss = 0.00664459\n",
      "Iteration 38, loss = 0.00642516\n",
      "Iteration 39, loss = 0.00589733\n",
      "Iteration 40, loss = 0.00535584\n",
      "Iteration 41, loss = 0.00529032\n",
      "Iteration 42, loss = 0.00490927\n",
      "Iteration 43, loss = 0.00465797\n",
      "Iteration 44, loss = 0.00445643\n",
      "Iteration 45, loss = 0.00415812\n",
      "Iteration 46, loss = 0.00392712\n",
      "Iteration 47, loss = 0.00365413\n",
      "Iteration 48, loss = 0.00345386\n",
      "Iteration 49, loss = 0.00488486\n",
      "Iteration 50, loss = 0.00399304\n",
      "Iteration 51, loss = 0.00313345\n",
      "Iteration 52, loss = 0.00299098\n",
      "Iteration 53, loss = 0.00278324\n",
      "Iteration 54, loss = 0.00258633\n",
      "Iteration 55, loss = 0.00242550\n",
      "Iteration 56, loss = 0.00225082\n",
      "Iteration 57, loss = 0.00406131\n",
      "Iteration 58, loss = 0.00313436\n",
      "Iteration 59, loss = 0.00346004\n",
      "Iteration 60, loss = 0.00240191\n",
      "Iteration 61, loss = 0.00201449\n",
      "Iteration 62, loss = 0.00187341\n",
      "Iteration 63, loss = 0.00178595\n",
      "Iteration 64, loss = 0.00178746\n",
      "Iteration 65, loss = 0.00169379\n",
      "Iteration 66, loss = 0.00163148\n",
      "Iteration 67, loss = 0.00159219\n",
      "Iteration 68, loss = 0.00154812\n",
      "Iteration 69, loss = 0.00152763\n",
      "Iteration 70, loss = 0.00150239\n",
      "Iteration 71, loss = 0.00117637\n",
      "Iteration 72, loss = 0.00110745\n",
      "Iteration 73, loss = 0.00103117\n",
      "Iteration 74, loss = 0.00099187\n",
      "Iteration 75, loss = 0.00095051\n",
      "Iteration 76, loss = 0.00090187\n",
      "Iteration 77, loss = 0.00088331\n",
      "Iteration 78, loss = 0.00084277\n",
      "Iteration 79, loss = 0.00083081\n",
      "Iteration 80, loss = 0.00079835\n",
      "Iteration 81, loss = 0.00076724\n",
      "Iteration 82, loss = 0.00073613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.83940263\n",
      "Iteration 2, loss = 0.82087893\n",
      "Iteration 3, loss = 0.80380314\n",
      "Iteration 4, loss = 0.78797664\n",
      "Iteration 5, loss = 0.77394160\n",
      "Iteration 6, loss = 0.76034568\n",
      "Iteration 7, loss = 0.74830986\n",
      "Iteration 8, loss = 0.73670743\n",
      "Iteration 9, loss = 0.72657172\n",
      "Iteration 10, loss = 0.71673466\n",
      "Iteration 11, loss = 0.70769529\n",
      "Iteration 12, loss = 0.69942895\n",
      "Iteration 13, loss = 0.69197870\n",
      "Iteration 14, loss = 0.68522811\n",
      "Iteration 15, loss = 0.67915354\n",
      "Iteration 16, loss = 0.67371197\n",
      "Iteration 17, loss = 0.66930241\n",
      "Iteration 18, loss = 0.66544317\n",
      "Iteration 19, loss = 0.66153677\n",
      "Iteration 20, loss = 0.65801464\n",
      "Iteration 21, loss = 0.65488614\n",
      "Iteration 22, loss = 0.65253459\n",
      "Iteration 23, loss = 0.65015464\n",
      "Iteration 24, loss = 0.64834521\n",
      "Iteration 25, loss = 0.64650387\n",
      "Iteration 26, loss = 0.64484648\n",
      "Iteration 27, loss = 0.64371541\n",
      "Iteration 28, loss = 0.64277497\n",
      "Iteration 29, loss = 0.64169482\n",
      "Iteration 30, loss = 0.64074129\n",
      "Iteration 31, loss = 0.63993972\n",
      "Iteration 32, loss = 0.63947383\n",
      "Iteration 33, loss = 0.63889654\n",
      "Iteration 34, loss = 0.63858401\n",
      "Iteration 35, loss = 0.63833203\n",
      "Iteration 36, loss = 0.63796128\n",
      "Iteration 37, loss = 0.63764149\n",
      "Iteration 38, loss = 0.63737461\n",
      "Iteration 39, loss = 0.63716659\n",
      "Iteration 40, loss = 0.63710061\n",
      "Iteration 41, loss = 0.63706500\n",
      "Iteration 42, loss = 0.63694155\n",
      "Iteration 43, loss = 0.63690086\n",
      "Iteration 44, loss = 0.63681775\n",
      "Iteration 45, loss = 0.63673395\n",
      "Iteration 46, loss = 0.63672871\n",
      "Iteration 47, loss = 0.63667661\n",
      "Iteration 48, loss = 0.63668656\n",
      "Iteration 49, loss = 0.63664107\n",
      "Iteration 50, loss = 0.63664692\n",
      "Iteration 51, loss = 0.63661986\n",
      "Iteration 52, loss = 0.63659016\n",
      "Iteration 53, loss = 0.63660062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51386175\n",
      "Iteration 2, loss = 0.28103041\n",
      "Iteration 3, loss = 0.16615958\n",
      "Iteration 4, loss = 0.10245178\n",
      "Iteration 5, loss = 0.07200034\n",
      "Iteration 6, loss = 0.05466461\n",
      "Iteration 7, loss = 0.04449872\n",
      "Iteration 8, loss = 0.03861624\n",
      "Iteration 9, loss = 0.03391841\n",
      "Iteration 10, loss = 0.03064939\n",
      "Iteration 11, loss = 0.02811427\n",
      "Iteration 12, loss = 0.02552745\n",
      "Iteration 13, loss = 0.02374271\n",
      "Iteration 14, loss = 0.02258564\n",
      "Iteration 15, loss = 0.02076007\n",
      "Iteration 16, loss = 0.01957438\n",
      "Iteration 17, loss = 0.01894481\n",
      "Iteration 18, loss = 0.01746410\n",
      "Iteration 19, loss = 0.01668484\n",
      "Iteration 20, loss = 0.01532881\n",
      "Iteration 21, loss = 0.01489876\n",
      "Iteration 22, loss = 0.01397999\n",
      "Iteration 23, loss = 0.01300619\n",
      "Iteration 24, loss = 0.01214894\n",
      "Iteration 25, loss = 0.01147133\n",
      "Iteration 26, loss = 0.01069253\n",
      "Iteration 27, loss = 0.00978798\n",
      "Iteration 28, loss = 0.00926775\n",
      "Iteration 29, loss = 0.00849521\n",
      "Iteration 30, loss = 0.00832797\n",
      "Iteration 31, loss = 0.00730209\n",
      "Iteration 32, loss = 0.00693077\n",
      "Iteration 33, loss = 0.00665256\n",
      "Iteration 34, loss = 0.00632088\n",
      "Iteration 35, loss = 0.00565545\n",
      "Iteration 36, loss = 0.00498046\n",
      "Iteration 37, loss = 0.00464179\n",
      "Iteration 38, loss = 0.00418436\n",
      "Iteration 39, loss = 0.00387391\n",
      "Iteration 40, loss = 0.00372896\n",
      "Iteration 41, loss = 0.00342771\n",
      "Iteration 42, loss = 0.00318952\n",
      "Iteration 43, loss = 0.00292921\n",
      "Iteration 44, loss = 0.00277900\n",
      "Iteration 45, loss = 0.00261301\n",
      "Iteration 46, loss = 0.00254742\n",
      "Iteration 47, loss = 0.00244508\n",
      "Iteration 48, loss = 0.00228719\n",
      "Iteration 49, loss = 0.00217129\n",
      "Iteration 50, loss = 0.00202020\n",
      "Iteration 51, loss = 0.00195658\n",
      "Iteration 52, loss = 0.00192409\n",
      "Iteration 53, loss = 0.00184022\n",
      "Iteration 54, loss = 0.00177600\n",
      "Iteration 55, loss = 0.00169746\n",
      "Iteration 56, loss = 0.00168925\n",
      "Iteration 57, loss = 0.00159475\n",
      "Iteration 58, loss = 0.00156457\n",
      "Iteration 59, loss = 0.00152349\n",
      "Iteration 60, loss = 0.00149418\n",
      "Iteration 61, loss = 0.00145417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60182394\n",
      "Iteration 2, loss = 0.44504653\n",
      "Iteration 3, loss = 0.28234883\n",
      "Iteration 4, loss = 0.15419015\n",
      "Iteration 5, loss = 0.09004023\n",
      "Iteration 6, loss = 0.07231300\n",
      "Iteration 7, loss = 0.05551425\n",
      "Iteration 8, loss = 0.04762261\n",
      "Iteration 9, loss = 0.04303565\n",
      "Iteration 10, loss = 0.04024999\n",
      "Iteration 11, loss = 0.03771735\n",
      "Iteration 12, loss = 0.03485999\n",
      "Iteration 13, loss = 0.03258730\n",
      "Iteration 14, loss = 0.03086835\n",
      "Iteration 15, loss = 0.02904678\n",
      "Iteration 16, loss = 0.02763752\n",
      "Iteration 17, loss = 0.02622275\n",
      "Iteration 18, loss = 0.02499261\n",
      "Iteration 19, loss = 0.02393258\n",
      "Iteration 20, loss = 0.02300823\n",
      "Iteration 21, loss = 0.02195852\n",
      "Iteration 22, loss = 0.02097466\n",
      "Iteration 23, loss = 0.02014799\n",
      "Iteration 24, loss = 0.01955958\n",
      "Iteration 25, loss = 0.01842364\n",
      "Iteration 26, loss = 0.01752488\n",
      "Iteration 27, loss = 0.01681891\n",
      "Iteration 28, loss = 0.01624817\n",
      "Iteration 29, loss = 0.01535639\n",
      "Iteration 30, loss = 0.01461237\n",
      "Iteration 31, loss = 0.01400858\n",
      "Iteration 32, loss = 0.01316415\n",
      "Iteration 33, loss = 0.01292531\n",
      "Iteration 34, loss = 0.01228916\n",
      "Iteration 35, loss = 0.01166315\n",
      "Iteration 36, loss = 0.01100234\n",
      "Iteration 37, loss = 0.01035217\n",
      "Iteration 38, loss = 0.00998206\n",
      "Iteration 39, loss = 0.00982867\n",
      "Iteration 40, loss = 0.00913223\n",
      "Iteration 41, loss = 0.00873786\n",
      "Iteration 42, loss = 0.00807043\n",
      "Iteration 43, loss = 0.00766805\n",
      "Iteration 44, loss = 0.00721682\n",
      "Iteration 45, loss = 0.00670144\n",
      "Iteration 46, loss = 0.00650512\n",
      "Iteration 47, loss = 0.00614504\n",
      "Iteration 48, loss = 0.00553293\n",
      "Iteration 49, loss = 0.00861534\n",
      "Iteration 50, loss = 0.00508507\n",
      "Iteration 51, loss = 0.00477862\n",
      "Iteration 52, loss = 0.00991732\n",
      "Iteration 53, loss = 0.00539860\n",
      "Iteration 54, loss = 0.00492340\n",
      "Iteration 55, loss = 0.00421843\n",
      "Iteration 56, loss = 0.00385341\n",
      "Iteration 57, loss = 0.00370359\n",
      "Iteration 58, loss = 0.00343958\n",
      "Iteration 59, loss = 0.00348285\n",
      "Iteration 60, loss = 0.00309184\n",
      "Iteration 61, loss = 0.00313785\n",
      "Iteration 62, loss = 0.00281825\n",
      "Iteration 63, loss = 0.00265748\n",
      "Iteration 64, loss = 0.00251415\n",
      "Iteration 65, loss = 0.00232949\n",
      "Iteration 66, loss = 0.00221778\n",
      "Iteration 67, loss = 0.00209272\n",
      "Iteration 68, loss = 0.00192528\n",
      "Iteration 69, loss = 0.00169572\n",
      "Iteration 70, loss = 0.00155205\n",
      "Iteration 71, loss = 0.00146346\n",
      "Iteration 72, loss = 0.00138831\n",
      "Iteration 73, loss = 0.00119261\n",
      "Iteration 74, loss = 0.00114702\n",
      "Iteration 75, loss = 0.00104668\n",
      "Iteration 76, loss = 0.00098683\n",
      "Iteration 77, loss = 0.00091924\n",
      "Iteration 78, loss = 0.00083641\n",
      "Iteration 79, loss = 0.00080354\n",
      "Iteration 80, loss = 0.00075432\n",
      "Iteration 81, loss = 0.00069254\n",
      "Iteration 82, loss = 0.00068446\n",
      "Iteration 83, loss = 0.00065431\n",
      "Iteration 84, loss = 0.00059976\n",
      "Iteration 85, loss = 0.00063428\n",
      "Iteration 86, loss = 0.00054735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61653068\n",
      "Iteration 2, loss = 0.46859429\n",
      "Iteration 3, loss = 0.28462050\n",
      "Iteration 4, loss = 0.16253944\n",
      "Iteration 5, loss = 0.09826817\n",
      "Iteration 6, loss = 0.07166271\n",
      "Iteration 7, loss = 0.05940698\n",
      "Iteration 8, loss = 0.05182573\n",
      "Iteration 9, loss = 0.04712044\n",
      "Iteration 10, loss = 0.04273852\n",
      "Iteration 11, loss = 0.03927169\n",
      "Iteration 12, loss = 0.03639351\n",
      "Iteration 13, loss = 0.03399559\n",
      "Iteration 14, loss = 0.03218040\n",
      "Iteration 15, loss = 0.04101768\n",
      "Iteration 16, loss = 0.03077436\n",
      "Iteration 17, loss = 0.02977864\n",
      "Iteration 18, loss = 0.02883406\n",
      "Iteration 19, loss = 0.02805022\n",
      "Iteration 20, loss = 0.02728838\n",
      "Iteration 21, loss = 0.02667949\n",
      "Iteration 22, loss = 0.02606187\n",
      "Iteration 23, loss = 0.02543674\n",
      "Iteration 24, loss = 0.02478318\n",
      "Iteration 25, loss = 0.02419012\n",
      "Iteration 26, loss = 0.02364135\n",
      "Iteration 27, loss = 0.02310903\n",
      "Iteration 28, loss = 0.02260143\n",
      "Iteration 29, loss = 0.02217533\n",
      "Iteration 30, loss = 0.02169733\n",
      "Iteration 31, loss = 0.02116513\n",
      "Iteration 32, loss = 0.02076081\n",
      "Iteration 33, loss = 0.02027804\n",
      "Iteration 34, loss = 0.01995611\n",
      "Iteration 35, loss = 0.01934250\n",
      "Iteration 36, loss = 0.01895954\n",
      "Iteration 37, loss = 0.01842830\n",
      "Iteration 38, loss = 0.01813740\n",
      "Iteration 39, loss = 0.01766292\n",
      "Iteration 40, loss = 0.01715324\n",
      "Iteration 41, loss = 0.01667290\n",
      "Iteration 42, loss = 0.01646370\n",
      "Iteration 43, loss = 0.01616664\n",
      "Iteration 44, loss = 0.01563774\n",
      "Iteration 45, loss = 0.01526926\n",
      "Iteration 46, loss = 0.01478336\n",
      "Iteration 47, loss = 0.01462834\n",
      "Iteration 48, loss = 0.01407069\n",
      "Iteration 49, loss = 0.01362911\n",
      "Iteration 50, loss = 0.01918568\n",
      "Iteration 51, loss = 0.01531863\n",
      "Iteration 52, loss = 0.01436080\n",
      "Iteration 53, loss = 0.01354152\n",
      "Iteration 54, loss = 0.01315352\n",
      "Iteration 55, loss = 0.01257306\n",
      "Iteration 56, loss = 0.01243249\n",
      "Iteration 57, loss = 0.01167550\n",
      "Iteration 58, loss = 0.01128194\n",
      "Iteration 59, loss = 0.01119674\n",
      "Iteration 60, loss = 0.01073651\n",
      "Iteration 61, loss = 0.01034127\n",
      "Iteration 62, loss = 0.01007923\n",
      "Iteration 63, loss = 0.00981586\n",
      "Iteration 64, loss = 0.00958326\n",
      "Iteration 65, loss = 0.01019448\n",
      "Iteration 66, loss = 0.00921745\n",
      "Iteration 67, loss = 0.00870421\n",
      "Iteration 68, loss = 0.00852005\n",
      "Iteration 69, loss = 0.00832159\n",
      "Iteration 70, loss = 0.00813841\n",
      "Iteration 71, loss = 0.00770147\n",
      "Iteration 72, loss = 0.00753534\n",
      "Iteration 73, loss = 0.00736898\n",
      "Iteration 74, loss = 0.00726650\n",
      "Iteration 75, loss = 0.00688516\n",
      "Iteration 76, loss = 0.00660906\n",
      "Iteration 77, loss = 0.00620578\n",
      "Iteration 78, loss = 0.00596459\n",
      "Iteration 79, loss = 0.00562566\n",
      "Iteration 80, loss = 0.00551067\n",
      "Iteration 81, loss = 0.00523987\n",
      "Iteration 82, loss = 0.00509609\n",
      "Iteration 83, loss = 0.00503726\n",
      "Iteration 84, loss = 0.00448452\n",
      "Iteration 85, loss = 0.00428502\n",
      "Iteration 86, loss = 0.00417264\n",
      "Iteration 87, loss = 0.00387413\n",
      "Iteration 88, loss = 0.00396276\n",
      "Iteration 89, loss = 0.00349154\n",
      "Iteration 90, loss = 0.00338024\n",
      "Iteration 91, loss = 0.00314884\n",
      "Iteration 92, loss = 0.00291414\n",
      "Iteration 93, loss = 0.00259568\n",
      "Iteration 94, loss = 0.00245164\n",
      "Iteration 95, loss = 0.00249039\n",
      "Iteration 96, loss = 0.00232694\n",
      "Iteration 97, loss = 0.00206575\n",
      "Iteration 98, loss = 0.00187444\n",
      "Iteration 99, loss = 0.00177498\n",
      "Iteration 100, loss = 0.00159220\n",
      "Iteration 101, loss = 0.00152199\n",
      "Iteration 102, loss = 0.00138347\n",
      "Iteration 103, loss = 0.00124607\n",
      "Iteration 104, loss = 0.00106712\n",
      "Iteration 105, loss = 0.00099324\n",
      "Iteration 106, loss = 0.00090393\n",
      "Iteration 107, loss = 0.00082110\n",
      "Iteration 108, loss = 0.00079531\n",
      "Iteration 109, loss = 0.00071455\n",
      "Iteration 110, loss = 0.00072617\n",
      "Iteration 111, loss = 0.00064111\n",
      "Iteration 112, loss = 0.00059919\n",
      "Iteration 113, loss = 0.00057096\n",
      "Iteration 114, loss = 0.00053404\n",
      "Iteration 115, loss = 0.00049971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54947347\n",
      "Iteration 2, loss = 0.33008610\n",
      "Iteration 3, loss = 0.16460050\n",
      "Iteration 4, loss = 0.08790589\n",
      "Iteration 5, loss = 0.06016741\n",
      "Iteration 6, loss = 0.04801203\n",
      "Iteration 7, loss = 0.04108026\n",
      "Iteration 8, loss = 0.03649383\n",
      "Iteration 9, loss = 0.03297340\n",
      "Iteration 10, loss = 0.03015097\n",
      "Iteration 11, loss = 0.02762367\n",
      "Iteration 12, loss = 0.02569695\n",
      "Iteration 13, loss = 0.02448163\n",
      "Iteration 14, loss = 0.02291326\n",
      "Iteration 15, loss = 0.02153485\n",
      "Iteration 16, loss = 0.02040710\n",
      "Iteration 17, loss = 0.01938689\n",
      "Iteration 18, loss = 0.01853746\n",
      "Iteration 19, loss = 0.01746773\n",
      "Iteration 20, loss = 0.01654534\n",
      "Iteration 21, loss = 0.01594091\n",
      "Iteration 22, loss = 0.01522619\n",
      "Iteration 23, loss = 0.01459064\n",
      "Iteration 24, loss = 0.01412440\n",
      "Iteration 25, loss = 0.01320552\n",
      "Iteration 26, loss = 0.01274724\n",
      "Iteration 27, loss = 0.01217054\n",
      "Iteration 28, loss = 0.01162367\n",
      "Iteration 29, loss = 0.01087071\n",
      "Iteration 30, loss = 0.01061035\n",
      "Iteration 31, loss = 0.01069191\n",
      "Iteration 32, loss = 0.00945601\n",
      "Iteration 33, loss = 0.00917648\n",
      "Iteration 34, loss = 0.00859033\n",
      "Iteration 35, loss = 0.00850739\n",
      "Iteration 36, loss = 0.00797772\n",
      "Iteration 37, loss = 0.00757833\n",
      "Iteration 38, loss = 0.00925115\n",
      "Iteration 39, loss = 0.00736693\n",
      "Iteration 40, loss = 0.00684070\n",
      "Iteration 41, loss = 0.00667172\n",
      "Iteration 42, loss = 0.00633351\n",
      "Iteration 43, loss = 0.00620936\n",
      "Iteration 44, loss = 0.00587336\n",
      "Iteration 45, loss = 0.00548511\n",
      "Iteration 46, loss = 0.00506182\n",
      "Iteration 47, loss = 0.00476400\n",
      "Iteration 48, loss = 0.00461821\n",
      "Iteration 49, loss = 0.00427376\n",
      "Iteration 50, loss = 0.00399416\n",
      "Iteration 51, loss = 0.00376200\n",
      "Iteration 52, loss = 0.00334940\n",
      "Iteration 53, loss = 0.00301789\n",
      "Iteration 54, loss = 0.00288022\n",
      "Iteration 55, loss = 0.00269014\n",
      "Iteration 56, loss = 0.00254235\n",
      "Iteration 57, loss = 0.00242866\n",
      "Iteration 58, loss = 0.00224109\n",
      "Iteration 59, loss = 0.00216223\n",
      "Iteration 60, loss = 0.00207941\n",
      "Iteration 61, loss = 0.00198744\n",
      "Iteration 62, loss = 0.00195458\n",
      "Iteration 63, loss = 0.00186175\n",
      "Iteration 64, loss = 0.00178404\n",
      "Iteration 65, loss = 0.00173357\n",
      "Iteration 66, loss = 0.00172505\n",
      "Iteration 67, loss = 0.00160400\n",
      "Iteration 68, loss = 0.00153407\n",
      "Iteration 69, loss = 0.00139474\n",
      "Iteration 70, loss = 0.00118314\n",
      "Iteration 71, loss = 0.00106519\n",
      "Iteration 72, loss = 0.00093417\n",
      "Iteration 73, loss = 0.00073046\n",
      "Iteration 74, loss = 0.00055459\n",
      "Iteration 75, loss = 0.00052087\n",
      "Iteration 76, loss = 0.00047487\n",
      "Iteration 77, loss = 0.00042472\n",
      "Iteration 78, loss = 0.00046498\n",
      "Iteration 79, loss = 0.00038565\n",
      "Iteration 80, loss = 0.00035714\n",
      "Iteration 81, loss = 0.00033674\n",
      "Iteration 82, loss = 0.00032152\n",
      "Iteration 83, loss = 0.00031725\n",
      "Iteration 84, loss = 0.00029717\n",
      "Iteration 85, loss = 0.00028258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53187331\n",
      "Iteration 2, loss = 0.24357661\n",
      "Iteration 3, loss = 0.11829479\n",
      "Iteration 4, loss = 0.07532501\n",
      "Iteration 5, loss = 0.05652067\n",
      "Iteration 6, loss = 0.04625903\n",
      "Iteration 7, loss = 0.03976546\n",
      "Iteration 8, loss = 0.03652505\n",
      "Iteration 9, loss = 0.03242228\n",
      "Iteration 10, loss = 0.02969840\n",
      "Iteration 11, loss = 0.02751290\n",
      "Iteration 12, loss = 0.02566070\n",
      "Iteration 13, loss = 0.02405180\n",
      "Iteration 14, loss = 0.02261242\n",
      "Iteration 15, loss = 0.02154999\n",
      "Iteration 16, loss = 0.02028900\n",
      "Iteration 17, loss = 0.02120138\n",
      "Iteration 18, loss = 0.01858585\n",
      "Iteration 19, loss = 0.01765382\n",
      "Iteration 20, loss = 0.01675257\n",
      "Iteration 21, loss = 0.02272953\n",
      "Iteration 22, loss = 0.01664313\n",
      "Iteration 23, loss = 0.01583099\n",
      "Iteration 24, loss = 0.03885601\n",
      "Iteration 25, loss = 0.01751342\n",
      "Iteration 26, loss = 0.01588052\n",
      "Iteration 27, loss = 0.01508018\n",
      "Iteration 28, loss = 0.01439713\n",
      "Iteration 29, loss = 0.01388216\n",
      "Iteration 30, loss = 0.01345374\n",
      "Iteration 31, loss = 0.01313105\n",
      "Iteration 32, loss = 0.01279476\n",
      "Iteration 33, loss = 0.01237494\n",
      "Iteration 34, loss = 0.01214621\n",
      "Iteration 35, loss = 0.01179980\n",
      "Iteration 36, loss = 0.01160561\n",
      "Iteration 37, loss = 0.01124040\n",
      "Iteration 38, loss = 0.01096767\n",
      "Iteration 39, loss = 0.01065912\n",
      "Iteration 40, loss = 0.01048369\n",
      "Iteration 41, loss = 0.01018590\n",
      "Iteration 42, loss = 0.00992539\n",
      "Iteration 43, loss = 0.00974413\n",
      "Iteration 44, loss = 0.00942562\n",
      "Iteration 45, loss = 0.00915631\n",
      "Iteration 46, loss = 0.00896651\n",
      "Iteration 47, loss = 0.00873267\n",
      "Iteration 48, loss = 0.00843725\n",
      "Iteration 49, loss = 0.00827339\n",
      "Iteration 50, loss = 0.00809564\n",
      "Iteration 51, loss = 0.00773457\n",
      "Iteration 52, loss = 0.00759352\n",
      "Iteration 53, loss = 0.00731704\n",
      "Iteration 54, loss = 0.00705211\n",
      "Iteration 55, loss = 0.00690788\n",
      "Iteration 56, loss = 0.00665963\n",
      "Iteration 57, loss = 0.00685162\n",
      "Iteration 58, loss = 0.00623866\n",
      "Iteration 59, loss = 0.00596800\n",
      "Iteration 60, loss = 0.00572802\n",
      "Iteration 61, loss = 0.00549119\n",
      "Iteration 62, loss = 0.00519793\n",
      "Iteration 63, loss = 0.00507773\n",
      "Iteration 64, loss = 0.00486333\n",
      "Iteration 65, loss = 0.00467470\n",
      "Iteration 66, loss = 0.00467406\n",
      "Iteration 67, loss = 0.00427398\n",
      "Iteration 68, loss = 0.00421381\n",
      "Iteration 69, loss = 0.00388998\n",
      "Iteration 70, loss = 0.00375380\n",
      "Iteration 71, loss = 0.00347368\n",
      "Iteration 72, loss = 0.00317468\n",
      "Iteration 73, loss = 0.00301296\n",
      "Iteration 74, loss = 0.00278684\n",
      "Iteration 75, loss = 0.00268065\n",
      "Iteration 76, loss = 0.00252217\n",
      "Iteration 77, loss = 0.00231816\n",
      "Iteration 78, loss = 0.00219329\n",
      "Iteration 79, loss = 0.00206712\n",
      "Iteration 80, loss = 0.00179097\n",
      "Iteration 81, loss = 0.00164617\n",
      "Iteration 82, loss = 0.00137321\n",
      "Iteration 83, loss = 0.00121008\n",
      "Iteration 84, loss = 0.00108993\n",
      "Iteration 85, loss = 0.00098444\n",
      "Iteration 86, loss = 0.00089518\n",
      "Iteration 87, loss = 0.00086103\n",
      "Iteration 88, loss = 0.00077700\n",
      "Iteration 89, loss = 0.00535335\n",
      "Iteration 90, loss = 0.00171196\n",
      "Iteration 91, loss = 0.00183099\n",
      "Iteration 92, loss = 0.00077028\n",
      "Iteration 93, loss = 0.00065402\n",
      "Iteration 94, loss = 0.00059396\n",
      "Iteration 95, loss = 0.00053760\n",
      "Iteration 96, loss = 0.00050374\n",
      "Iteration 97, loss = 0.00047807\n",
      "Iteration 98, loss = 0.00044578\n",
      "Iteration 99, loss = 0.00042984\n",
      "Iteration 100, loss = 0.00040246\n",
      "Iteration 101, loss = 0.00038181\n",
      "Iteration 102, loss = 0.00036138\n",
      "Iteration 103, loss = 0.00034909\n",
      "Iteration 104, loss = 0.00033895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63661101\n",
      "Iteration 2, loss = 0.63365660\n",
      "Iteration 3, loss = 0.62581612\n",
      "Iteration 4, loss = 0.60577731\n",
      "Iteration 5, loss = 0.56422121\n",
      "Iteration 6, loss = 0.49610180\n",
      "Iteration 7, loss = 0.41176008\n",
      "Iteration 8, loss = 0.33022437\n",
      "Iteration 9, loss = 0.26311187\n",
      "Iteration 10, loss = 0.21255808\n",
      "Iteration 11, loss = 0.17543185\n",
      "Iteration 12, loss = 0.14821554\n",
      "Iteration 13, loss = 0.12781958\n",
      "Iteration 14, loss = 0.11239138\n",
      "Iteration 15, loss = 0.10017492\n",
      "Iteration 16, loss = 0.09060258\n",
      "Iteration 17, loss = 0.08276238\n",
      "Iteration 18, loss = 0.07631510\n",
      "Iteration 19, loss = 0.07084335\n",
      "Iteration 20, loss = 0.06621803\n",
      "Iteration 21, loss = 0.06238851\n",
      "Iteration 22, loss = 0.05879881\n",
      "Iteration 23, loss = 0.05575963\n",
      "Iteration 24, loss = 0.05303243\n",
      "Iteration 25, loss = 0.05059585\n",
      "Iteration 26, loss = 0.04835533\n",
      "Iteration 27, loss = 0.04643439\n",
      "Iteration 28, loss = 0.04457967\n",
      "Iteration 29, loss = 0.04291046\n",
      "Iteration 30, loss = 0.04146598\n",
      "Iteration 31, loss = 0.03996653\n",
      "Iteration 32, loss = 0.03875484\n",
      "Iteration 33, loss = 0.03748753\n",
      "Iteration 34, loss = 0.03641285\n",
      "Iteration 35, loss = 0.03556001\n",
      "Iteration 36, loss = 0.03451533\n",
      "Iteration 37, loss = 0.03361267\n",
      "Iteration 38, loss = 0.03277472\n",
      "Iteration 39, loss = 0.03207225\n",
      "Iteration 40, loss = 0.03127389\n",
      "Iteration 41, loss = 0.03058440\n",
      "Iteration 42, loss = 0.02983887\n",
      "Iteration 43, loss = 0.02928022\n",
      "Iteration 44, loss = 0.02862665\n",
      "Iteration 45, loss = 0.02815625\n",
      "Iteration 46, loss = 0.02769720\n",
      "Iteration 47, loss = 0.02717486\n",
      "Iteration 48, loss = 0.02675198\n",
      "Iteration 49, loss = 0.02623468\n",
      "Iteration 50, loss = 0.02590166\n",
      "Iteration 51, loss = 0.02541127\n",
      "Iteration 52, loss = 0.02497428\n",
      "Iteration 53, loss = 0.02469989\n",
      "Iteration 54, loss = 0.02412641\n",
      "Iteration 55, loss = 0.02367921\n",
      "Iteration 56, loss = 0.02320850\n",
      "Iteration 57, loss = 0.02275537\n",
      "Iteration 58, loss = 0.02230286\n",
      "Iteration 59, loss = 0.02194147\n",
      "Iteration 60, loss = 0.02153157\n",
      "Iteration 61, loss = 0.02122654\n",
      "Iteration 62, loss = 0.02077805\n",
      "Iteration 63, loss = 0.02049269\n",
      "Iteration 64, loss = 0.02009513\n",
      "Iteration 65, loss = 0.01982720\n",
      "Iteration 66, loss = 0.01952383\n",
      "Iteration 67, loss = 0.01921945\n",
      "Iteration 68, loss = 0.01894307\n",
      "Iteration 69, loss = 0.01865583\n",
      "Iteration 70, loss = 0.01846337\n",
      "Iteration 71, loss = 0.01821704\n",
      "Iteration 72, loss = 0.01782468\n",
      "Iteration 73, loss = 0.01763264\n",
      "Iteration 74, loss = 0.01738866\n",
      "Iteration 75, loss = 0.01713093\n",
      "Iteration 76, loss = 0.01695036\n",
      "Iteration 77, loss = 0.01671227\n",
      "Iteration 78, loss = 0.01645599\n",
      "Iteration 79, loss = 0.01628155\n",
      "Iteration 80, loss = 0.01601270\n",
      "Iteration 81, loss = 0.01582252\n",
      "Iteration 82, loss = 0.01561239\n",
      "Iteration 83, loss = 0.01542850\n",
      "Iteration 84, loss = 0.01520841\n",
      "Iteration 85, loss = 0.01505592\n",
      "Iteration 86, loss = 0.01481808\n",
      "Iteration 87, loss = 0.01467446\n",
      "Iteration 88, loss = 0.01446326\n",
      "Iteration 89, loss = 0.01435749\n",
      "Iteration 90, loss = 0.01413994\n",
      "Iteration 91, loss = 0.01399965\n",
      "Iteration 92, loss = 0.01393124\n",
      "Iteration 93, loss = 0.01377303\n",
      "Iteration 94, loss = 0.01364002\n",
      "Iteration 95, loss = 0.01347709\n",
      "Iteration 96, loss = 0.01342787\n",
      "Iteration 97, loss = 0.01320029\n",
      "Iteration 98, loss = 0.01321825\n",
      "Iteration 99, loss = 0.01305742\n",
      "Iteration 100, loss = 0.01292002\n",
      "Iteration 101, loss = 0.01274195\n",
      "Iteration 102, loss = 0.01240417\n",
      "Iteration 103, loss = 0.01221752\n",
      "Iteration 104, loss = 0.01210541\n",
      "Iteration 105, loss = 0.01200099\n",
      "Iteration 106, loss = 0.01192371\n",
      "Iteration 107, loss = 0.01185326\n",
      "Iteration 108, loss = 0.01180820\n",
      "Iteration 109, loss = 0.01180176\n",
      "Iteration 110, loss = 0.01164137\n",
      "Iteration 111, loss = 0.01165974\n",
      "Iteration 112, loss = 0.01157647\n",
      "Iteration 113, loss = 0.01147716\n",
      "Iteration 114, loss = 0.01142523\n",
      "Iteration 115, loss = 0.01136088\n",
      "Iteration 116, loss = 0.01135597\n",
      "Iteration 117, loss = 0.01122510\n",
      "Iteration 118, loss = 0.01115215\n",
      "Iteration 119, loss = 0.01105883\n",
      "Iteration 120, loss = 0.01094009\n",
      "Iteration 121, loss = 0.01084119\n",
      "Iteration 122, loss = 0.01070760\n",
      "Iteration 123, loss = 0.01054523\n",
      "Iteration 124, loss = 0.01047622\n",
      "Iteration 125, loss = 0.01046775\n",
      "Iteration 126, loss = 0.01041337\n",
      "Iteration 127, loss = 0.01035564\n",
      "Iteration 128, loss = 0.01035940\n",
      "Iteration 129, loss = 0.01030458\n",
      "Iteration 130, loss = 0.01028885\n",
      "Iteration 131, loss = 0.01025720\n",
      "Iteration 132, loss = 0.01024129\n",
      "Iteration 133, loss = 0.01021193\n",
      "Iteration 134, loss = 0.01019627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74322219\n",
      "Iteration 2, loss = 0.68194154\n",
      "Iteration 3, loss = 0.64957355\n",
      "Iteration 4, loss = 0.63084420\n",
      "Iteration 5, loss = 0.61441871\n",
      "Iteration 6, loss = 0.59213226\n",
      "Iteration 7, loss = 0.56079369\n",
      "Iteration 8, loss = 0.52018612\n",
      "Iteration 9, loss = 0.47036546\n",
      "Iteration 10, loss = 0.41299241\n",
      "Iteration 11, loss = 0.35292913\n",
      "Iteration 12, loss = 0.29626905\n",
      "Iteration 13, loss = 0.24706182\n",
      "Iteration 14, loss = 0.20802473\n",
      "Iteration 15, loss = 0.17738302\n",
      "Iteration 16, loss = 0.15339731\n",
      "Iteration 17, loss = 0.13464631\n",
      "Iteration 18, loss = 0.11966302\n",
      "Iteration 19, loss = 0.10757201\n",
      "Iteration 20, loss = 0.09781414\n",
      "Iteration 21, loss = 0.08966848\n",
      "Iteration 22, loss = 0.08295660\n",
      "Iteration 23, loss = 0.07720112\n",
      "Iteration 24, loss = 0.07230846\n",
      "Iteration 25, loss = 0.07087333\n",
      "Iteration 26, loss = 0.06557469\n",
      "Iteration 27, loss = 0.06256480\n",
      "Iteration 28, loss = 0.06007140\n",
      "Iteration 29, loss = 0.05783301\n",
      "Iteration 30, loss = 0.05579043\n",
      "Iteration 31, loss = 0.05403964\n",
      "Iteration 32, loss = 0.05216433\n",
      "Iteration 33, loss = 0.05059501\n",
      "Iteration 34, loss = 0.04913375\n",
      "Iteration 35, loss = 0.04777666\n",
      "Iteration 36, loss = 0.04654763\n",
      "Iteration 37, loss = 0.04536903\n",
      "Iteration 38, loss = 0.04426938\n",
      "Iteration 39, loss = 0.04326676\n",
      "Iteration 40, loss = 0.04232126\n",
      "Iteration 41, loss = 0.04140716\n",
      "Iteration 42, loss = 0.04055114\n",
      "Iteration 43, loss = 0.03970922\n",
      "Iteration 44, loss = 0.03886881\n",
      "Iteration 45, loss = 0.03815323\n",
      "Iteration 46, loss = 0.03740410\n",
      "Iteration 47, loss = 0.03672314\n",
      "Iteration 48, loss = 0.03601824\n",
      "Iteration 49, loss = 0.03544427\n",
      "Iteration 50, loss = 0.03469552\n",
      "Iteration 51, loss = 0.03408838\n",
      "Iteration 52, loss = 0.03349591\n",
      "Iteration 53, loss = 0.03294100\n",
      "Iteration 54, loss = 0.03243461\n",
      "Iteration 55, loss = 0.03190421\n",
      "Iteration 56, loss = 0.03141921\n",
      "Iteration 57, loss = 0.03083067\n",
      "Iteration 58, loss = 0.03036957\n",
      "Iteration 59, loss = 0.02980889\n",
      "Iteration 60, loss = 0.02933583\n",
      "Iteration 61, loss = 0.02889463\n",
      "Iteration 62, loss = 0.02842762\n",
      "Iteration 63, loss = 0.02794676\n",
      "Iteration 64, loss = 0.02746052\n",
      "Iteration 65, loss = 0.02705247\n",
      "Iteration 66, loss = 0.02658158\n",
      "Iteration 67, loss = 0.02621281\n",
      "Iteration 68, loss = 0.03220350\n",
      "Iteration 69, loss = 0.02656101\n",
      "Iteration 70, loss = 0.02592133\n",
      "Iteration 71, loss = 0.02559449\n",
      "Iteration 72, loss = 0.02530930\n",
      "Iteration 73, loss = 0.02507788\n",
      "Iteration 74, loss = 0.02489244\n",
      "Iteration 75, loss = 0.02465588\n",
      "Iteration 76, loss = 0.02465437\n",
      "Iteration 77, loss = 0.02446328\n",
      "Iteration 78, loss = 0.02423537\n",
      "Iteration 79, loss = 0.02412447\n",
      "Iteration 80, loss = 0.02395179\n",
      "Iteration 81, loss = 0.02376877\n",
      "Iteration 82, loss = 0.02362475\n",
      "Iteration 83, loss = 0.02347174\n",
      "Iteration 84, loss = 0.02334812\n",
      "Iteration 85, loss = 0.02319549\n",
      "Iteration 86, loss = 0.02304116\n",
      "Iteration 87, loss = 0.02297967\n",
      "Iteration 88, loss = 0.02281160\n",
      "Iteration 89, loss = 0.02269992\n",
      "Iteration 90, loss = 0.02251915\n",
      "Iteration 91, loss = 0.02241671\n",
      "Iteration 92, loss = 0.02228850\n",
      "Iteration 93, loss = 0.02215136\n",
      "Iteration 94, loss = 0.02209729\n",
      "Iteration 95, loss = 0.02197923\n",
      "Iteration 96, loss = 0.02182046\n",
      "Iteration 97, loss = 0.02167183\n",
      "Iteration 98, loss = 0.02161674\n",
      "Iteration 99, loss = 0.02146156\n",
      "Iteration 100, loss = 0.02135163\n",
      "Iteration 101, loss = 0.02123883\n",
      "Iteration 102, loss = 0.02111320\n",
      "Iteration 103, loss = 0.02099365\n",
      "Iteration 104, loss = 0.02087845\n",
      "Iteration 105, loss = 0.02076862\n",
      "Iteration 106, loss = 0.02066094\n",
      "Iteration 107, loss = 0.02057269\n",
      "Iteration 108, loss = 0.02041332\n",
      "Iteration 109, loss = 0.02032340\n",
      "Iteration 110, loss = 0.02019960\n",
      "Iteration 111, loss = 0.02003029\n",
      "Iteration 112, loss = 0.01995283\n",
      "Iteration 113, loss = 0.01995283\n",
      "Iteration 114, loss = 0.01971114\n",
      "Iteration 115, loss = 0.01949456\n",
      "Iteration 116, loss = 0.01942248\n",
      "Iteration 117, loss = 0.02121883\n",
      "Iteration 118, loss = 0.01938622\n",
      "Iteration 119, loss = 0.01912309\n",
      "Iteration 120, loss = 0.01901171\n",
      "Iteration 121, loss = 0.01889302\n",
      "Iteration 122, loss = 0.01878410\n",
      "Iteration 123, loss = 0.01871391\n",
      "Iteration 124, loss = 0.01861097\n",
      "Iteration 125, loss = 0.01851292\n",
      "Iteration 126, loss = 0.01842568\n",
      "Iteration 127, loss = 0.01827399\n",
      "Iteration 128, loss = 0.01824345\n",
      "Iteration 129, loss = 0.01807933\n",
      "Iteration 130, loss = 0.01803792\n",
      "Iteration 131, loss = 0.01787995\n",
      "Iteration 132, loss = 0.01777547\n",
      "Iteration 133, loss = 0.01762286\n",
      "Iteration 134, loss = 0.01753128\n",
      "Iteration 135, loss = 0.01735548\n",
      "Iteration 136, loss = 0.01728383\n",
      "Iteration 137, loss = 0.01706806\n",
      "Iteration 138, loss = 0.01694744\n",
      "Iteration 139, loss = 0.01674476\n",
      "Iteration 140, loss = 0.01669002\n",
      "Iteration 141, loss = 0.01646502\n",
      "Iteration 142, loss = 0.01634107\n",
      "Iteration 143, loss = 0.01623324\n",
      "Iteration 144, loss = 0.01612787\n",
      "Iteration 145, loss = 0.01607142\n",
      "Iteration 146, loss = 0.01581632\n",
      "Iteration 147, loss = 0.01575821\n",
      "Iteration 148, loss = 0.01569198\n",
      "Iteration 149, loss = 0.01557803\n",
      "Iteration 150, loss = 0.01540844\n",
      "Iteration 151, loss = 0.01521492\n",
      "Iteration 152, loss = 0.01511390\n",
      "Iteration 153, loss = 0.01485381\n",
      "Iteration 154, loss = 0.01462523\n",
      "Iteration 155, loss = 0.01445068\n",
      "Iteration 156, loss = 0.01433353\n",
      "Iteration 157, loss = 0.01431521\n",
      "Iteration 158, loss = 0.01440786\n",
      "Iteration 159, loss = 0.01417870\n",
      "Iteration 160, loss = 0.01404168\n",
      "Iteration 161, loss = 0.01392796\n",
      "Iteration 162, loss = 0.01384812\n",
      "Iteration 163, loss = 0.01379087\n",
      "Iteration 164, loss = 0.01371022\n",
      "Iteration 165, loss = 0.01368567\n",
      "Iteration 166, loss = 0.01358427\n",
      "Iteration 167, loss = 0.01350570\n",
      "Iteration 168, loss = 0.01342253\n",
      "Iteration 169, loss = 0.01328833\n",
      "Iteration 170, loss = 0.01320854\n",
      "Iteration 171, loss = 0.01304458\n",
      "Iteration 172, loss = 0.01290444\n",
      "Iteration 173, loss = 0.01246303\n",
      "Iteration 174, loss = 0.01233384\n",
      "Iteration 175, loss = 0.01192710\n",
      "Iteration 176, loss = 0.01164777\n",
      "Iteration 177, loss = 0.01141950\n",
      "Iteration 178, loss = 0.01132677\n",
      "Iteration 179, loss = 0.01115618\n",
      "Iteration 180, loss = 0.01101438\n",
      "Iteration 181, loss = 0.01094064\n",
      "Iteration 182, loss = 0.01090317\n",
      "Iteration 183, loss = 0.01075978\n",
      "Iteration 184, loss = 0.01070392\n",
      "Iteration 185, loss = 0.01063706\n",
      "Iteration 186, loss = 0.01060482\n",
      "Iteration 187, loss = 0.01056994\n",
      "Iteration 188, loss = 0.01049784\n",
      "Iteration 189, loss = 0.01042298\n",
      "Iteration 190, loss = 0.01037346\n",
      "Iteration 191, loss = 0.01031983\n",
      "Iteration 192, loss = 0.01024986\n",
      "Iteration 193, loss = 0.01019946\n",
      "Iteration 194, loss = 0.01010941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79104171\n",
      "Iteration 2, loss = 0.70058806\n",
      "Iteration 3, loss = 0.65729383\n",
      "Iteration 4, loss = 0.63607352\n",
      "Iteration 5, loss = 0.62104989\n",
      "Iteration 6, loss = 0.60827484\n",
      "Iteration 7, loss = 0.59337603\n",
      "Iteration 8, loss = 0.57355969\n",
      "Iteration 9, loss = 0.54869466\n",
      "Iteration 10, loss = 0.51655813\n",
      "Iteration 11, loss = 0.47489344\n",
      "Iteration 12, loss = 0.42488551\n",
      "Iteration 13, loss = 0.37084472\n",
      "Iteration 14, loss = 0.31709671\n",
      "Iteration 15, loss = 0.26910246\n",
      "Iteration 16, loss = 0.22914018\n",
      "Iteration 17, loss = 0.19657278\n",
      "Iteration 18, loss = 0.17017965\n",
      "Iteration 19, loss = 0.14900220\n",
      "Iteration 20, loss = 0.13206435\n",
      "Iteration 21, loss = 0.11861322\n",
      "Iteration 22, loss = 0.10742455\n",
      "Iteration 23, loss = 0.09826353\n",
      "Iteration 24, loss = 0.09066291\n",
      "Iteration 25, loss = 0.08416698\n",
      "Iteration 26, loss = 0.07856305\n",
      "Iteration 27, loss = 0.07380896\n",
      "Iteration 28, loss = 0.06961331\n",
      "Iteration 29, loss = 0.06603091\n",
      "Iteration 30, loss = 0.06278708\n",
      "Iteration 31, loss = 0.05981257\n",
      "Iteration 32, loss = 0.05722229\n",
      "Iteration 33, loss = 0.05481234\n",
      "Iteration 34, loss = 0.05267287\n",
      "Iteration 35, loss = 0.05062788\n",
      "Iteration 36, loss = 0.04893959\n",
      "Iteration 37, loss = 0.04719094\n",
      "Iteration 38, loss = 0.04566627\n",
      "Iteration 39, loss = 0.04425524\n",
      "Iteration 40, loss = 0.04272351\n",
      "Iteration 41, loss = 0.04137360\n",
      "Iteration 42, loss = 0.03996295\n",
      "Iteration 43, loss = 0.03869402\n",
      "Iteration 44, loss = 0.03765483\n",
      "Iteration 45, loss = 0.03644480\n",
      "Iteration 46, loss = 0.03539048\n",
      "Iteration 47, loss = 0.03441000\n",
      "Iteration 48, loss = 0.03350577\n",
      "Iteration 49, loss = 0.03261054\n",
      "Iteration 50, loss = 0.03185724\n",
      "Iteration 51, loss = 0.03102526\n",
      "Iteration 52, loss = 0.03024115\n",
      "Iteration 53, loss = 0.02945092\n",
      "Iteration 54, loss = 0.02866390\n",
      "Iteration 55, loss = 0.02801555\n",
      "Iteration 56, loss = 0.02753232\n",
      "Iteration 57, loss = 0.02687824\n",
      "Iteration 58, loss = 0.02625969\n",
      "Iteration 59, loss = 0.02585305\n",
      "Iteration 60, loss = 0.02528076\n",
      "Iteration 61, loss = 0.02762535\n",
      "Iteration 62, loss = 0.02512391\n",
      "Iteration 63, loss = 0.02475444\n",
      "Iteration 64, loss = 0.02443847\n",
      "Iteration 65, loss = 0.02417151\n",
      "Iteration 66, loss = 0.02388873\n",
      "Iteration 67, loss = 0.02366236\n",
      "Iteration 68, loss = 0.02343350\n",
      "Iteration 69, loss = 0.02319808\n",
      "Iteration 70, loss = 0.02303130\n",
      "Iteration 71, loss = 0.02279940\n",
      "Iteration 72, loss = 0.02261265\n",
      "Iteration 73, loss = 0.02242218\n",
      "Iteration 74, loss = 0.02223503\n",
      "Iteration 75, loss = 0.02205650\n",
      "Iteration 76, loss = 0.02188861\n",
      "Iteration 77, loss = 0.02170478\n",
      "Iteration 78, loss = 0.02156692\n",
      "Iteration 79, loss = 0.02134654\n",
      "Iteration 80, loss = 0.02259155\n",
      "Iteration 81, loss = 0.02134719\n",
      "Iteration 82, loss = 0.02116327\n",
      "Iteration 83, loss = 0.02099818\n",
      "Iteration 84, loss = 0.02079364\n",
      "Iteration 85, loss = 0.02061882\n",
      "Iteration 86, loss = 0.02050277\n",
      "Iteration 87, loss = 0.02033474\n",
      "Iteration 88, loss = 0.02020810\n",
      "Iteration 89, loss = 0.02006928\n",
      "Iteration 90, loss = 0.02000442\n",
      "Iteration 91, loss = 0.01987715\n",
      "Iteration 92, loss = 0.01974179\n",
      "Iteration 93, loss = 0.01969435\n",
      "Iteration 94, loss = 0.01958600\n",
      "Iteration 95, loss = 0.01943112\n",
      "Iteration 96, loss = 0.01934053\n",
      "Iteration 97, loss = 0.01923744\n",
      "Iteration 98, loss = 0.01914224\n",
      "Iteration 99, loss = 0.01905616\n",
      "Iteration 100, loss = 0.01894679\n",
      "Iteration 101, loss = 0.01884467\n",
      "Iteration 102, loss = 0.01882450\n",
      "Iteration 103, loss = 0.01870065\n",
      "Iteration 104, loss = 0.01848391\n",
      "Iteration 105, loss = 0.01834131\n",
      "Iteration 106, loss = 0.01828594\n",
      "Iteration 107, loss = 0.01803941\n",
      "Iteration 108, loss = 0.01783095\n",
      "Iteration 109, loss = 0.01772538\n",
      "Iteration 110, loss = 0.01754862\n",
      "Iteration 111, loss = 0.01732072\n",
      "Iteration 112, loss = 0.01724062\n",
      "Iteration 113, loss = 0.01705651\n",
      "Iteration 114, loss = 0.01690583\n",
      "Iteration 115, loss = 0.01675460\n",
      "Iteration 116, loss = 0.01662995\n",
      "Iteration 117, loss = 0.01654411\n",
      "Iteration 118, loss = 0.01641206\n",
      "Iteration 119, loss = 0.01631662\n",
      "Iteration 120, loss = 0.01618542\n",
      "Iteration 121, loss = 0.01614365\n",
      "Iteration 122, loss = 0.01602462\n",
      "Iteration 123, loss = 0.01600153\n",
      "Iteration 124, loss = 0.01585569\n",
      "Iteration 125, loss = 0.01584276\n",
      "Iteration 126, loss = 0.01561961\n",
      "Iteration 127, loss = 0.01555908\n",
      "Iteration 128, loss = 0.01546043\n",
      "Iteration 129, loss = 0.01535893\n",
      "Iteration 130, loss = 0.01527535\n",
      "Iteration 131, loss = 0.01523881\n",
      "Iteration 132, loss = 0.01509640\n",
      "Iteration 133, loss = 0.01502647\n",
      "Iteration 134, loss = 0.01493342\n",
      "Iteration 135, loss = 0.01484626\n",
      "Iteration 136, loss = 0.01475940\n",
      "Iteration 137, loss = 0.01467697\n",
      "Iteration 138, loss = 0.01457676\n",
      "Iteration 139, loss = 0.01447692\n",
      "Iteration 140, loss = 0.01434860\n",
      "Iteration 141, loss = 0.01421165\n",
      "Iteration 142, loss = 0.01409815\n",
      "Iteration 143, loss = 0.01398852\n",
      "Iteration 144, loss = 0.01391769\n",
      "Iteration 145, loss = 0.01377531\n",
      "Iteration 146, loss = 0.01368828\n",
      "Iteration 147, loss = 0.01361727\n",
      "Iteration 148, loss = 0.01356956\n",
      "Iteration 149, loss = 0.01347045\n",
      "Iteration 150, loss = 0.01340122\n",
      "Iteration 151, loss = 0.01337284\n",
      "Iteration 152, loss = 0.01335093\n",
      "Iteration 153, loss = 0.01329745\n",
      "Iteration 154, loss = 0.01322898\n",
      "Iteration 155, loss = 0.01322191\n",
      "Iteration 156, loss = 0.01317625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72832541\n",
      "Iteration 2, loss = 0.67172300\n",
      "Iteration 3, loss = 0.64601759\n",
      "Iteration 4, loss = 0.63359028\n",
      "Iteration 5, loss = 0.62283023\n",
      "Iteration 6, loss = 0.60618512\n",
      "Iteration 7, loss = 0.57867572\n",
      "Iteration 8, loss = 0.53844134\n",
      "Iteration 9, loss = 0.48384907\n",
      "Iteration 10, loss = 0.41990882\n",
      "Iteration 11, loss = 0.35666485\n",
      "Iteration 12, loss = 0.29958522\n",
      "Iteration 13, loss = 0.25269835\n",
      "Iteration 14, loss = 0.21598234\n",
      "Iteration 15, loss = 0.18658243\n",
      "Iteration 16, loss = 0.16373506\n",
      "Iteration 17, loss = 0.14535368\n",
      "Iteration 18, loss = 0.13068749\n",
      "Iteration 19, loss = 0.11853772\n",
      "Iteration 20, loss = 0.10852413\n",
      "Iteration 21, loss = 0.09987244\n",
      "Iteration 22, loss = 0.09254595\n",
      "Iteration 23, loss = 0.08619753\n",
      "Iteration 24, loss = 0.08064145\n",
      "Iteration 25, loss = 0.07573522\n",
      "Iteration 26, loss = 0.07138809\n",
      "Iteration 27, loss = 0.06749262\n",
      "Iteration 28, loss = 0.06409191\n",
      "Iteration 29, loss = 0.06094853\n",
      "Iteration 30, loss = 0.05810294\n",
      "Iteration 31, loss = 0.05553166\n",
      "Iteration 32, loss = 0.05319122\n",
      "Iteration 33, loss = 0.05097504\n",
      "Iteration 34, loss = 0.04901608\n",
      "Iteration 35, loss = 0.04722988\n",
      "Iteration 36, loss = 0.04550515\n",
      "Iteration 37, loss = 0.04385746\n",
      "Iteration 38, loss = 0.04245180\n",
      "Iteration 39, loss = 0.04114870\n",
      "Iteration 40, loss = 0.03993517\n",
      "Iteration 41, loss = 0.04453070\n",
      "Iteration 42, loss = 0.03938358\n",
      "Iteration 43, loss = 0.03813986\n",
      "Iteration 44, loss = 0.03737487\n",
      "Iteration 45, loss = 0.03671665\n",
      "Iteration 46, loss = 0.03607214\n",
      "Iteration 47, loss = 0.03548394\n",
      "Iteration 48, loss = 0.03492404\n",
      "Iteration 49, loss = 0.03440696\n",
      "Iteration 50, loss = 0.03392608\n",
      "Iteration 51, loss = 0.03347011\n",
      "Iteration 52, loss = 0.03300928\n",
      "Iteration 53, loss = 0.03259674\n",
      "Iteration 54, loss = 0.03219974\n",
      "Iteration 55, loss = 0.03180383\n",
      "Iteration 56, loss = 0.03140388\n",
      "Iteration 57, loss = 0.03106018\n",
      "Iteration 58, loss = 0.03071387\n",
      "Iteration 59, loss = 0.03037020\n",
      "Iteration 60, loss = 0.03006092\n",
      "Iteration 61, loss = 0.02975119\n",
      "Iteration 62, loss = 0.02943731\n",
      "Iteration 63, loss = 0.02915363\n",
      "Iteration 64, loss = 0.02886893\n",
      "Iteration 65, loss = 0.02857588\n",
      "Iteration 66, loss = 0.02835479\n",
      "Iteration 67, loss = 0.02807226\n",
      "Iteration 68, loss = 0.02778664\n",
      "Iteration 69, loss = 0.02753164\n",
      "Iteration 70, loss = 0.02731352\n",
      "Iteration 71, loss = 0.02709061\n",
      "Iteration 72, loss = 0.02681729\n",
      "Iteration 73, loss = 0.02656345\n",
      "Iteration 74, loss = 0.02635304\n",
      "Iteration 75, loss = 0.02614070\n",
      "Iteration 76, loss = 0.02593451\n",
      "Iteration 77, loss = 0.02568328\n",
      "Iteration 78, loss = 0.02549581\n",
      "Iteration 79, loss = 0.02524222\n",
      "Iteration 80, loss = 0.02513505\n",
      "Iteration 81, loss = 0.02490783\n",
      "Iteration 82, loss = 0.02465948\n",
      "Iteration 83, loss = 0.02445357\n",
      "Iteration 84, loss = 0.02427005\n",
      "Iteration 85, loss = 0.02513171\n",
      "Iteration 86, loss = 0.02417083\n",
      "Iteration 87, loss = 0.02400753\n",
      "Iteration 88, loss = 0.02378476\n",
      "Iteration 89, loss = 0.02362700\n",
      "Iteration 90, loss = 0.02345757\n",
      "Iteration 91, loss = 0.02331257\n",
      "Iteration 92, loss = 0.02312150\n",
      "Iteration 93, loss = 0.02298544\n",
      "Iteration 94, loss = 0.02281355\n",
      "Iteration 95, loss = 0.02264273\n",
      "Iteration 96, loss = 0.02246672\n",
      "Iteration 97, loss = 0.02229257\n",
      "Iteration 98, loss = 0.02226377\n",
      "Iteration 99, loss = 0.02197816\n",
      "Iteration 100, loss = 0.02182894\n",
      "Iteration 101, loss = 0.02157753\n",
      "Iteration 102, loss = 0.02150471\n",
      "Iteration 103, loss = 0.02124473\n",
      "Iteration 104, loss = 0.02108198\n",
      "Iteration 105, loss = 0.02091346\n",
      "Iteration 106, loss = 0.02076047\n",
      "Iteration 107, loss = 0.02050735\n",
      "Iteration 108, loss = 0.02031421\n",
      "Iteration 109, loss = 0.02005619\n",
      "Iteration 110, loss = 0.01996856\n",
      "Iteration 111, loss = 0.01974403\n",
      "Iteration 112, loss = 0.01961500\n",
      "Iteration 113, loss = 0.01943621\n",
      "Iteration 114, loss = 0.01928191\n",
      "Iteration 115, loss = 0.01912219\n",
      "Iteration 116, loss = 0.01900273\n",
      "Iteration 117, loss = 0.01889230\n",
      "Iteration 118, loss = 0.01868939\n",
      "Iteration 119, loss = 0.01850433\n",
      "Iteration 120, loss = 0.01834901\n",
      "Iteration 121, loss = 0.01812688\n",
      "Iteration 122, loss = 0.01785173\n",
      "Iteration 123, loss = 0.01749770\n",
      "Iteration 124, loss = 0.01730976\n",
      "Iteration 125, loss = 0.01711241\n",
      "Iteration 126, loss = 0.01694405\n",
      "Iteration 127, loss = 0.01676014\n",
      "Iteration 128, loss = 0.01662590\n",
      "Iteration 129, loss = 0.01650708\n",
      "Iteration 130, loss = 0.01647236\n",
      "Iteration 131, loss = 0.01635962\n",
      "Iteration 132, loss = 0.01622577\n",
      "Iteration 133, loss = 0.01651768\n",
      "Iteration 134, loss = 0.01606938\n",
      "Iteration 135, loss = 0.01590908\n",
      "Iteration 136, loss = 0.01577489\n",
      "Iteration 137, loss = 0.01562187\n",
      "Iteration 138, loss = 0.01549152\n",
      "Iteration 139, loss = 0.01539059\n",
      "Iteration 140, loss = 0.01528201\n",
      "Iteration 141, loss = 0.01516082\n",
      "Iteration 142, loss = 0.01507927\n",
      "Iteration 143, loss = 0.01496416\n",
      "Iteration 144, loss = 0.01484236\n",
      "Iteration 145, loss = 0.01473490\n",
      "Iteration 146, loss = 0.01465150\n",
      "Iteration 147, loss = 0.01450952\n",
      "Iteration 148, loss = 0.01440836\n",
      "Iteration 149, loss = 0.01430214\n",
      "Iteration 150, loss = 0.01394232\n",
      "Iteration 151, loss = 0.01381390\n",
      "Iteration 152, loss = 0.01372915\n",
      "Iteration 153, loss = 0.01366375\n",
      "Iteration 154, loss = 0.01358357\n",
      "Iteration 155, loss = 0.01350664\n",
      "Iteration 156, loss = 0.01348230\n",
      "Iteration 157, loss = 0.01333495\n",
      "Iteration 158, loss = 0.01329854\n",
      "Iteration 159, loss = 0.01322003\n",
      "Iteration 160, loss = 0.01316087\n",
      "Iteration 161, loss = 0.01314286\n",
      "Iteration 162, loss = 0.01300709\n",
      "Iteration 163, loss = 0.01300869\n",
      "Iteration 164, loss = 0.01292509\n",
      "Iteration 165, loss = 0.01285930\n",
      "Iteration 166, loss = 0.01278393\n",
      "Iteration 167, loss = 0.01273452\n",
      "Iteration 168, loss = 0.01265875\n",
      "Iteration 169, loss = 0.01258106\n",
      "Iteration 170, loss = 0.01254161\n",
      "Iteration 171, loss = 0.01245947\n",
      "Iteration 172, loss = 0.01241091\n",
      "Iteration 173, loss = 0.01231980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71940903\n",
      "Iteration 2, loss = 0.66069191\n",
      "Iteration 3, loss = 0.63657855\n",
      "Iteration 4, loss = 0.62506481\n",
      "Iteration 5, loss = 0.61056494\n",
      "Iteration 6, loss = 0.59077160\n",
      "Iteration 7, loss = 0.56432193\n",
      "Iteration 8, loss = 0.53198621\n",
      "Iteration 9, loss = 0.49442563\n",
      "Iteration 10, loss = 0.45367160\n",
      "Iteration 11, loss = 0.40996299\n",
      "Iteration 12, loss = 0.36639087\n",
      "Iteration 13, loss = 0.32549686\n",
      "Iteration 14, loss = 0.28740247\n",
      "Iteration 15, loss = 0.25334542\n",
      "Iteration 16, loss = 0.22413194\n",
      "Iteration 17, loss = 0.19932036\n",
      "Iteration 18, loss = 0.17823309\n",
      "Iteration 19, loss = 0.16052451\n",
      "Iteration 20, loss = 0.14500580\n",
      "Iteration 21, loss = 0.13185808\n",
      "Iteration 22, loss = 0.12057957\n",
      "Iteration 23, loss = 0.11073054\n",
      "Iteration 24, loss = 0.10222521\n",
      "Iteration 25, loss = 0.09471603\n",
      "Iteration 26, loss = 0.08815672\n",
      "Iteration 27, loss = 0.08244693\n",
      "Iteration 28, loss = 0.07736420\n",
      "Iteration 29, loss = 0.07279093\n",
      "Iteration 30, loss = 0.06865165\n",
      "Iteration 31, loss = 0.06492843\n",
      "Iteration 32, loss = 0.06161789\n",
      "Iteration 33, loss = 0.05863506\n",
      "Iteration 34, loss = 0.05580400\n",
      "Iteration 35, loss = 0.05316280\n",
      "Iteration 36, loss = 0.05081744\n",
      "Iteration 37, loss = 0.04863800\n",
      "Iteration 38, loss = 0.04661497\n",
      "Iteration 39, loss = 0.04484474\n",
      "Iteration 40, loss = 0.04312915\n",
      "Iteration 41, loss = 0.04159108\n",
      "Iteration 42, loss = 0.04020907\n",
      "Iteration 43, loss = 0.03886657\n",
      "Iteration 44, loss = 0.04145561\n",
      "Iteration 45, loss = 0.03778469\n",
      "Iteration 46, loss = 0.03683296\n",
      "Iteration 47, loss = 0.03603629\n",
      "Iteration 48, loss = 0.03529911\n",
      "Iteration 49, loss = 0.03460444\n",
      "Iteration 50, loss = 0.03396285\n",
      "Iteration 51, loss = 0.03332205\n",
      "Iteration 52, loss = 0.03275262\n",
      "Iteration 53, loss = 0.03216098\n",
      "Iteration 54, loss = 0.03163956\n",
      "Iteration 55, loss = 0.03152486\n",
      "Iteration 56, loss = 0.03102142\n",
      "Iteration 57, loss = 0.03050544\n",
      "Iteration 58, loss = 0.03003454\n",
      "Iteration 59, loss = 0.02956314\n",
      "Iteration 60, loss = 0.02912549\n",
      "Iteration 61, loss = 0.02870199\n",
      "Iteration 62, loss = 0.02830743\n",
      "Iteration 63, loss = 0.02793067\n",
      "Iteration 64, loss = 0.02756315\n",
      "Iteration 65, loss = 0.02729452\n",
      "Iteration 66, loss = 0.02686798\n",
      "Iteration 67, loss = 0.02654476\n",
      "Iteration 68, loss = 0.02620969\n",
      "Iteration 69, loss = 0.02589958\n",
      "Iteration 70, loss = 0.02558193\n",
      "Iteration 71, loss = 0.02530821\n",
      "Iteration 72, loss = 0.02497940\n",
      "Iteration 73, loss = 0.02470340\n",
      "Iteration 74, loss = 0.02438998\n",
      "Iteration 75, loss = 0.02416902\n",
      "Iteration 76, loss = 0.02390155\n",
      "Iteration 77, loss = 0.02358482\n",
      "Iteration 78, loss = 0.02334898\n",
      "Iteration 79, loss = 0.02304317\n",
      "Iteration 80, loss = 0.02285446\n",
      "Iteration 81, loss = 0.02257692\n",
      "Iteration 82, loss = 0.02234596\n",
      "Iteration 83, loss = 0.02208225\n",
      "Iteration 84, loss = 0.02191388\n",
      "Iteration 85, loss = 0.02162327\n",
      "Iteration 86, loss = 0.02139863\n",
      "Iteration 87, loss = 0.02119969\n",
      "Iteration 88, loss = 0.02096352\n",
      "Iteration 89, loss = 0.02079254\n",
      "Iteration 90, loss = 0.02052766\n",
      "Iteration 91, loss = 0.02027863\n",
      "Iteration 92, loss = 0.02009602\n",
      "Iteration 93, loss = 0.01982131\n",
      "Iteration 94, loss = 0.01961892\n",
      "Iteration 95, loss = 0.01941007\n",
      "Iteration 96, loss = 0.01921499\n",
      "Iteration 97, loss = 0.01898719\n",
      "Iteration 98, loss = 0.01892297\n",
      "Iteration 99, loss = 0.01867047\n",
      "Iteration 100, loss = 0.01849835\n",
      "Iteration 101, loss = 0.01833710\n",
      "Iteration 102, loss = 0.01822856\n",
      "Iteration 103, loss = 0.01807881\n",
      "Iteration 104, loss = 0.01797265\n",
      "Iteration 105, loss = 0.01782458\n",
      "Iteration 106, loss = 0.01774295\n",
      "Iteration 107, loss = 0.01758614\n",
      "Iteration 108, loss = 0.01749151\n",
      "Iteration 109, loss = 0.01736263\n",
      "Iteration 110, loss = 0.01725479\n",
      "Iteration 111, loss = 0.01716892\n",
      "Iteration 112, loss = 0.01702623\n",
      "Iteration 113, loss = 0.01696449\n",
      "Iteration 114, loss = 0.01682570\n",
      "Iteration 115, loss = 0.01674709\n",
      "Iteration 116, loss = 0.01666133\n",
      "Iteration 117, loss = 0.01650442\n",
      "Iteration 118, loss = 0.01638878\n",
      "Iteration 119, loss = 0.01624012\n",
      "Iteration 120, loss = 0.01605209\n",
      "Iteration 121, loss = 0.01574171\n",
      "Iteration 122, loss = 0.01527244\n",
      "Iteration 123, loss = 0.01966931\n",
      "Iteration 124, loss = 0.01625657\n",
      "Iteration 125, loss = 0.01557064\n",
      "Iteration 126, loss = 0.01536918\n",
      "Iteration 127, loss = 0.01522140\n",
      "Iteration 128, loss = 0.01508927\n",
      "Iteration 129, loss = 0.01499034\n",
      "Iteration 130, loss = 0.01487058\n",
      "Iteration 131, loss = 0.01477338\n",
      "Iteration 132, loss = 0.01467139\n",
      "Iteration 133, loss = 0.01458933\n",
      "Iteration 134, loss = 0.01449067\n",
      "Iteration 135, loss = 0.01439546\n",
      "Iteration 136, loss = 0.01432163\n",
      "Iteration 137, loss = 0.01425843\n",
      "Iteration 138, loss = 0.01416629\n",
      "Iteration 139, loss = 0.01407534\n",
      "Iteration 140, loss = 0.01403348\n",
      "Iteration 141, loss = 0.01393999\n",
      "Iteration 142, loss = 0.01386086\n",
      "Iteration 143, loss = 0.01399430\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63614352\n",
      "Iteration 2, loss = 0.62786990\n",
      "Iteration 3, loss = 0.59963640\n",
      "Iteration 4, loss = 0.52965488\n",
      "Iteration 5, loss = 0.42322066\n",
      "Iteration 6, loss = 0.31738208\n",
      "Iteration 7, loss = 0.23751293\n",
      "Iteration 8, loss = 0.18354350\n",
      "Iteration 9, loss = 0.14750738\n",
      "Iteration 10, loss = 0.12305403\n",
      "Iteration 11, loss = 0.10555503\n",
      "Iteration 12, loss = 0.09281657\n",
      "Iteration 13, loss = 0.08287566\n",
      "Iteration 14, loss = 0.07527714\n",
      "Iteration 15, loss = 0.06903080\n",
      "Iteration 16, loss = 0.06390022\n",
      "Iteration 17, loss = 0.05952308\n",
      "Iteration 18, loss = 0.05575576\n",
      "Iteration 19, loss = 0.05252138\n",
      "Iteration 20, loss = 0.04977817\n",
      "Iteration 21, loss = 0.04731071\n",
      "Iteration 22, loss = 0.04508532\n",
      "Iteration 23, loss = 0.04295788\n",
      "Iteration 24, loss = 0.04119568\n",
      "Iteration 25, loss = 0.03983623\n",
      "Iteration 26, loss = 0.03813357\n",
      "Iteration 27, loss = 0.03673799\n",
      "Iteration 28, loss = 0.03549057\n",
      "Iteration 29, loss = 0.03430315\n",
      "Iteration 30, loss = 0.03329733\n",
      "Iteration 31, loss = 0.03237935\n",
      "Iteration 32, loss = 0.03151399\n",
      "Iteration 33, loss = 0.03051431\n",
      "Iteration 34, loss = 0.02974218\n",
      "Iteration 35, loss = 0.02901002\n",
      "Iteration 36, loss = 0.02823877\n",
      "Iteration 37, loss = 0.02754950\n",
      "Iteration 38, loss = 0.02694811\n",
      "Iteration 39, loss = 0.02621355\n",
      "Iteration 40, loss = 0.02558850\n",
      "Iteration 41, loss = 0.02498292\n",
      "Iteration 42, loss = 0.02438135\n",
      "Iteration 43, loss = 0.02380907\n",
      "Iteration 44, loss = 0.02345537\n",
      "Iteration 45, loss = 0.02282292\n",
      "Iteration 46, loss = 0.02235789\n",
      "Iteration 47, loss = 0.02192460\n",
      "Iteration 48, loss = 0.02146755\n",
      "Iteration 49, loss = 0.02097761\n",
      "Iteration 50, loss = 0.02065533\n",
      "Iteration 51, loss = 0.02024869\n",
      "Iteration 52, loss = 0.01983182\n",
      "Iteration 53, loss = 0.01960915\n",
      "Iteration 54, loss = 0.01917193\n",
      "Iteration 55, loss = 0.01885170\n",
      "Iteration 56, loss = 0.01849019\n",
      "Iteration 57, loss = 0.01826603\n",
      "Iteration 58, loss = 0.01785097\n",
      "Iteration 59, loss = 0.01777514\n",
      "Iteration 60, loss = 0.01729712\n",
      "Iteration 61, loss = 0.01688241\n",
      "Iteration 62, loss = 0.01673247\n",
      "Iteration 63, loss = 0.01627459\n",
      "Iteration 64, loss = 0.01612814\n",
      "Iteration 65, loss = 0.01585445\n",
      "Iteration 66, loss = 0.01555128\n",
      "Iteration 67, loss = 0.01526582\n",
      "Iteration 68, loss = 0.01507702\n",
      "Iteration 69, loss = 0.01481451\n",
      "Iteration 70, loss = 0.01476520\n",
      "Iteration 71, loss = 0.01441501\n",
      "Iteration 72, loss = 0.01427776\n",
      "Iteration 73, loss = 0.01386164\n",
      "Iteration 74, loss = 0.01354449\n",
      "Iteration 75, loss = 0.01335161\n",
      "Iteration 76, loss = 0.01322702\n",
      "Iteration 77, loss = 0.01309166\n",
      "Iteration 78, loss = 0.01293404\n",
      "Iteration 79, loss = 0.01277465\n",
      "Iteration 80, loss = 0.01268083\n",
      "Iteration 81, loss = 0.01252643\n",
      "Iteration 82, loss = 0.01250093\n",
      "Iteration 83, loss = 0.01233875\n",
      "Iteration 84, loss = 0.01223215\n",
      "Iteration 85, loss = 0.01211596\n",
      "Iteration 86, loss = 0.01206131\n",
      "Iteration 87, loss = 0.01194948\n",
      "Iteration 88, loss = 0.01184855\n",
      "Iteration 89, loss = 0.01179279\n",
      "Iteration 90, loss = 0.01169230\n",
      "Iteration 91, loss = 0.01155473\n",
      "Iteration 92, loss = 0.01173485\n",
      "Iteration 93, loss = 0.01142545\n",
      "Iteration 94, loss = 0.01141437\n",
      "Iteration 95, loss = 0.01128820\n",
      "Iteration 96, loss = 0.01117556\n",
      "Iteration 97, loss = 0.01114578\n",
      "Iteration 98, loss = 0.01107255\n",
      "Iteration 99, loss = 0.01097778\n",
      "Iteration 100, loss = 0.01089561\n",
      "Iteration 101, loss = 0.01072587\n",
      "Iteration 102, loss = 0.01058638\n",
      "Iteration 103, loss = 0.01050604\n",
      "Iteration 104, loss = 0.01049492\n",
      "Iteration 105, loss = 0.01041393\n",
      "Iteration 106, loss = 0.01037901\n",
      "Iteration 107, loss = 0.01033909\n",
      "Iteration 108, loss = 0.01030102\n",
      "Iteration 109, loss = 0.01025806\n",
      "Iteration 110, loss = 0.01024240\n",
      "Iteration 111, loss = 0.01020205\n",
      "Iteration 112, loss = 0.01020135\n",
      "Iteration 113, loss = 0.01012599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63734754\n",
      "Iteration 2, loss = 0.63179311\n",
      "Iteration 3, loss = 0.62227354\n",
      "Iteration 4, loss = 0.59878914\n",
      "Iteration 5, loss = 0.55194541\n",
      "Iteration 6, loss = 0.48246158\n",
      "Iteration 7, loss = 0.39802467\n",
      "Iteration 8, loss = 0.31569809\n",
      "Iteration 9, loss = 0.25052939\n",
      "Iteration 10, loss = 0.20145823\n",
      "Iteration 11, loss = 0.16635567\n",
      "Iteration 12, loss = 0.14123373\n",
      "Iteration 13, loss = 0.12245284\n",
      "Iteration 14, loss = 0.10818258\n",
      "Iteration 15, loss = 0.09665158\n",
      "Iteration 16, loss = 0.08764566\n",
      "Iteration 17, loss = 0.08009755\n",
      "Iteration 18, loss = 0.07527784\n",
      "Iteration 19, loss = 0.07056557\n",
      "Iteration 20, loss = 0.06637025\n",
      "Iteration 21, loss = 0.06240217\n",
      "Iteration 22, loss = 0.05890684\n",
      "Iteration 23, loss = 0.05593483\n",
      "Iteration 24, loss = 0.05328965\n",
      "Iteration 25, loss = 0.05070754\n",
      "Iteration 26, loss = 0.04857965\n",
      "Iteration 27, loss = 0.04652659\n",
      "Iteration 28, loss = 0.04471875\n",
      "Iteration 29, loss = 0.04297625\n",
      "Iteration 30, loss = 0.04145666\n",
      "Iteration 31, loss = 0.04001561\n",
      "Iteration 32, loss = 0.03868189\n",
      "Iteration 33, loss = 0.03758979\n",
      "Iteration 34, loss = 0.03633074\n",
      "Iteration 35, loss = 0.03533774\n",
      "Iteration 36, loss = 0.03435732\n",
      "Iteration 37, loss = 0.03348273\n",
      "Iteration 38, loss = 0.03261714\n",
      "Iteration 39, loss = 0.03174753\n",
      "Iteration 40, loss = 0.03090499\n",
      "Iteration 41, loss = 0.03027166\n",
      "Iteration 42, loss = 0.02927419\n",
      "Iteration 43, loss = 0.02865801\n",
      "Iteration 44, loss = 0.02799034\n",
      "Iteration 45, loss = 0.02739218\n",
      "Iteration 46, loss = 0.02682743\n",
      "Iteration 47, loss = 0.02620506\n",
      "Iteration 48, loss = 0.02580861\n",
      "Iteration 49, loss = 0.02519398\n",
      "Iteration 50, loss = 0.02483448\n",
      "Iteration 51, loss = 0.02435246\n",
      "Iteration 52, loss = 0.02398191\n",
      "Iteration 53, loss = 0.02366252\n",
      "Iteration 54, loss = 0.02328202\n",
      "Iteration 55, loss = 0.02293223\n",
      "Iteration 56, loss = 0.02262056\n",
      "Iteration 57, loss = 0.02227160\n",
      "Iteration 58, loss = 0.02194129\n",
      "Iteration 59, loss = 0.02163242\n",
      "Iteration 60, loss = 0.02133305\n",
      "Iteration 61, loss = 0.02097978\n",
      "Iteration 62, loss = 0.02070293\n",
      "Iteration 63, loss = 0.02044224\n",
      "Iteration 64, loss = 0.02026051\n",
      "Iteration 65, loss = 0.01993363\n",
      "Iteration 66, loss = 0.01980109\n",
      "Iteration 67, loss = 0.01948515\n",
      "Iteration 68, loss = 0.01928777\n",
      "Iteration 69, loss = 0.01898214\n",
      "Iteration 70, loss = 0.01895361\n",
      "Iteration 71, loss = 0.01853536\n",
      "Iteration 72, loss = 0.01822878\n",
      "Iteration 73, loss = 0.01794058\n",
      "Iteration 74, loss = 0.01947066\n",
      "Iteration 75, loss = 0.01807895\n",
      "Iteration 76, loss = 0.01768367\n",
      "Iteration 77, loss = 0.01741523\n",
      "Iteration 78, loss = 0.01704125\n",
      "Iteration 79, loss = 0.01676413\n",
      "Iteration 80, loss = 0.01653191\n",
      "Iteration 81, loss = 0.01629764\n",
      "Iteration 82, loss = 0.01672435\n",
      "Iteration 83, loss = 0.01563458\n",
      "Iteration 84, loss = 0.01544564\n",
      "Iteration 85, loss = 0.01517649\n",
      "Iteration 86, loss = 0.01498393\n",
      "Iteration 87, loss = 0.01486769\n",
      "Iteration 88, loss = 0.01460918\n",
      "Iteration 89, loss = 0.01445616\n",
      "Iteration 90, loss = 0.01420304\n",
      "Iteration 91, loss = 0.01414938\n",
      "Iteration 92, loss = 0.01387595\n",
      "Iteration 93, loss = 0.02310951\n",
      "Iteration 94, loss = 0.01630498\n",
      "Iteration 95, loss = 0.01528492\n",
      "Iteration 96, loss = 0.01490176\n",
      "Iteration 97, loss = 0.01457282\n",
      "Iteration 98, loss = 0.01430887\n",
      "Iteration 99, loss = 0.01410202\n",
      "Iteration 100, loss = 0.01396392\n",
      "Iteration 101, loss = 0.01377909\n",
      "Iteration 102, loss = 0.01365462\n",
      "Iteration 103, loss = 0.01354490\n",
      "Iteration 104, loss = 0.01344910\n",
      "Iteration 105, loss = 0.01336288\n",
      "Iteration 106, loss = 0.01332611\n",
      "Iteration 107, loss = 0.01321936\n",
      "Iteration 108, loss = 0.01316618\n",
      "Iteration 109, loss = 0.01309535\n",
      "Iteration 110, loss = 0.01304948\n",
      "Iteration 111, loss = 0.01299928\n",
      "Iteration 112, loss = 0.01296238\n",
      "Iteration 113, loss = 0.01290984\n",
      "Iteration 114, loss = 0.01287198\n",
      "Iteration 115, loss = 0.01285036\n",
      "Iteration 116, loss = 0.01278168\n",
      "Iteration 117, loss = 0.01274160\n",
      "Iteration 118, loss = 0.01270832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70937578\n",
      "Iteration 2, loss = 0.65624097\n",
      "Iteration 3, loss = 0.63405441\n",
      "Iteration 4, loss = 0.62126409\n",
      "Iteration 5, loss = 0.60303461\n",
      "Iteration 6, loss = 0.57075460\n",
      "Iteration 7, loss = 0.51889604\n",
      "Iteration 8, loss = 0.45151061\n",
      "Iteration 9, loss = 0.37792403\n",
      "Iteration 10, loss = 0.30986276\n",
      "Iteration 11, loss = 0.25475914\n",
      "Iteration 12, loss = 0.21307072\n",
      "Iteration 13, loss = 0.18135084\n",
      "Iteration 14, loss = 0.15730598\n",
      "Iteration 15, loss = 0.13864887\n",
      "Iteration 16, loss = 0.12364805\n",
      "Iteration 17, loss = 0.11177699\n",
      "Iteration 18, loss = 0.10234678\n",
      "Iteration 19, loss = 0.09405156\n",
      "Iteration 20, loss = 0.08738016\n",
      "Iteration 21, loss = 0.08187561\n",
      "Iteration 22, loss = 0.07682405\n",
      "Iteration 23, loss = 0.07235237\n",
      "Iteration 24, loss = 0.06868418\n",
      "Iteration 25, loss = 0.06527632\n",
      "Iteration 26, loss = 0.06217710\n",
      "Iteration 27, loss = 0.05940321\n",
      "Iteration 28, loss = 0.05669822\n",
      "Iteration 29, loss = 0.05438225\n",
      "Iteration 30, loss = 0.05185405\n",
      "Iteration 31, loss = 0.04981706\n",
      "Iteration 32, loss = 0.04764801\n",
      "Iteration 33, loss = 0.04605650\n",
      "Iteration 34, loss = 0.04424262\n",
      "Iteration 35, loss = 0.04285309\n",
      "Iteration 36, loss = 0.04158654\n",
      "Iteration 37, loss = 0.04018211\n",
      "Iteration 38, loss = 0.03900998\n",
      "Iteration 39, loss = 0.03800776\n",
      "Iteration 40, loss = 0.03691853\n",
      "Iteration 41, loss = 0.03581594\n",
      "Iteration 42, loss = 0.03484538\n",
      "Iteration 43, loss = 0.03387373\n",
      "Iteration 44, loss = 0.03293552\n",
      "Iteration 45, loss = 0.03207467\n",
      "Iteration 46, loss = 0.03126351\n",
      "Iteration 47, loss = 0.03036232\n",
      "Iteration 48, loss = 0.02967095\n",
      "Iteration 49, loss = 0.02879972\n",
      "Iteration 50, loss = 0.02840528\n",
      "Iteration 51, loss = 0.02753074\n",
      "Iteration 52, loss = 0.02684394\n",
      "Iteration 53, loss = 0.02633187\n",
      "Iteration 54, loss = 0.02581497\n",
      "Iteration 55, loss = 0.02530809\n",
      "Iteration 56, loss = 0.02480648\n",
      "Iteration 57, loss = 0.02450792\n",
      "Iteration 58, loss = 0.02390761\n",
      "Iteration 59, loss = 0.02359072\n",
      "Iteration 60, loss = 0.02317494\n",
      "Iteration 61, loss = 0.02290071\n",
      "Iteration 62, loss = 0.02242867\n",
      "Iteration 63, loss = 0.02210982\n",
      "Iteration 64, loss = 0.02179486\n",
      "Iteration 65, loss = 0.02139321\n",
      "Iteration 66, loss = 0.02120235\n",
      "Iteration 67, loss = 0.02074154\n",
      "Iteration 68, loss = 0.02111363\n",
      "Iteration 69, loss = 0.02045154\n",
      "Iteration 70, loss = 0.02000898\n",
      "Iteration 71, loss = 0.01969012\n",
      "Iteration 72, loss = 0.01955022\n",
      "Iteration 73, loss = 0.01929490\n",
      "Iteration 74, loss = 0.01903766\n",
      "Iteration 75, loss = 0.01890606\n",
      "Iteration 76, loss = 0.01866637\n",
      "Iteration 77, loss = 0.01849233\n",
      "Iteration 78, loss = 0.01841031\n",
      "Iteration 79, loss = 0.01815922\n",
      "Iteration 80, loss = 0.01798011\n",
      "Iteration 81, loss = 0.01784339\n",
      "Iteration 82, loss = 0.01773168\n",
      "Iteration 83, loss = 0.01746492\n",
      "Iteration 84, loss = 0.01736571\n",
      "Iteration 85, loss = 0.01727013\n",
      "Iteration 86, loss = 0.01700557\n",
      "Iteration 87, loss = 0.01697011\n",
      "Iteration 88, loss = 0.01667644\n",
      "Iteration 89, loss = 0.01651542\n",
      "Iteration 90, loss = 0.01637603\n",
      "Iteration 91, loss = 0.01620174\n",
      "Iteration 92, loss = 0.01609771\n",
      "Iteration 93, loss = 0.01589767\n",
      "Iteration 94, loss = 0.01587855\n",
      "Iteration 95, loss = 0.01565946\n",
      "Iteration 96, loss = 0.01554688\n",
      "Iteration 97, loss = 0.01523704\n",
      "Iteration 98, loss = 0.01510159\n",
      "Iteration 99, loss = 0.01498441\n",
      "Iteration 100, loss = 0.01487540\n",
      "Iteration 101, loss = 0.01481371\n",
      "Iteration 102, loss = 0.01470224\n",
      "Iteration 103, loss = 0.01466679\n",
      "Iteration 104, loss = 0.01455025\n",
      "Iteration 105, loss = 0.01450713\n",
      "Iteration 106, loss = 0.01446069\n",
      "Iteration 107, loss = 0.01439158\n",
      "Iteration 108, loss = 0.01436111\n",
      "Iteration 109, loss = 0.01432333\n",
      "Iteration 110, loss = 0.01424933\n",
      "Iteration 111, loss = 0.01421363\n",
      "Iteration 112, loss = 0.01417535\n",
      "Iteration 113, loss = 0.01411604\n",
      "Iteration 114, loss = 0.01417105\n",
      "Iteration 115, loss = 0.01405699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64930729\n",
      "Iteration 2, loss = 0.63464455\n",
      "Iteration 3, loss = 0.62746709\n",
      "Iteration 4, loss = 0.61469213\n",
      "Iteration 5, loss = 0.58899264\n",
      "Iteration 6, loss = 0.54241612\n",
      "Iteration 7, loss = 0.47260976\n",
      "Iteration 8, loss = 0.39023948\n",
      "Iteration 9, loss = 0.31202088\n",
      "Iteration 10, loss = 0.24996136\n",
      "Iteration 11, loss = 0.20429685\n",
      "Iteration 12, loss = 0.17082625\n",
      "Iteration 13, loss = 0.14646077\n",
      "Iteration 14, loss = 0.12788781\n",
      "Iteration 15, loss = 0.11309858\n",
      "Iteration 16, loss = 0.10173250\n",
      "Iteration 17, loss = 0.09393963\n",
      "Iteration 18, loss = 0.08702055\n",
      "Iteration 19, loss = 0.08152814\n",
      "Iteration 20, loss = 0.07699314\n",
      "Iteration 21, loss = 0.07312312\n",
      "Iteration 22, loss = 0.06970195\n",
      "Iteration 23, loss = 0.06667194\n",
      "Iteration 24, loss = 0.06395705\n",
      "Iteration 25, loss = 0.06156628\n",
      "Iteration 26, loss = 0.05942185\n",
      "Iteration 27, loss = 0.05742833\n",
      "Iteration 28, loss = 0.05553331\n",
      "Iteration 29, loss = 0.05744481\n",
      "Iteration 30, loss = 0.05350411\n",
      "Iteration 31, loss = 0.05207814\n",
      "Iteration 32, loss = 0.05102151\n",
      "Iteration 33, loss = 0.05001569\n",
      "Iteration 34, loss = 0.04908864\n",
      "Iteration 35, loss = 0.04818232\n",
      "Iteration 36, loss = 0.04732025\n",
      "Iteration 37, loss = 0.04655184\n",
      "Iteration 38, loss = 0.04572274\n",
      "Iteration 39, loss = 0.04497387\n",
      "Iteration 40, loss = 0.04424199\n",
      "Iteration 41, loss = 0.04356368\n",
      "Iteration 42, loss = 0.04283212\n",
      "Iteration 43, loss = 0.04216792\n",
      "Iteration 44, loss = 0.04151224\n",
      "Iteration 45, loss = 0.04082945\n",
      "Iteration 46, loss = 0.04135356\n",
      "Iteration 47, loss = 0.03973874\n",
      "Iteration 48, loss = 0.03917781\n",
      "Iteration 49, loss = 0.03851725\n",
      "Iteration 50, loss = 0.03789640\n",
      "Iteration 51, loss = 0.03730930\n",
      "Iteration 52, loss = 0.03673125\n",
      "Iteration 53, loss = 0.03619044\n",
      "Iteration 54, loss = 0.03565588\n",
      "Iteration 55, loss = 0.03510274\n",
      "Iteration 56, loss = 0.03456354\n",
      "Iteration 57, loss = 0.03409073\n",
      "Iteration 58, loss = 0.03360344\n",
      "Iteration 59, loss = 0.03312012\n",
      "Iteration 60, loss = 0.03270698\n",
      "Iteration 61, loss = 0.03226226\n",
      "Iteration 62, loss = 0.03188663\n",
      "Iteration 63, loss = 0.03148115\n",
      "Iteration 64, loss = 0.03108999\n",
      "Iteration 65, loss = 0.03059981\n",
      "Iteration 66, loss = 0.03023674\n",
      "Iteration 67, loss = 0.02988811\n",
      "Iteration 68, loss = 0.02952784\n",
      "Iteration 69, loss = 0.02909322\n",
      "Iteration 70, loss = 0.02880976\n",
      "Iteration 71, loss = 0.02856178\n",
      "Iteration 72, loss = 0.02803640\n",
      "Iteration 73, loss = 0.02773488\n",
      "Iteration 74, loss = 0.02743975\n",
      "Iteration 75, loss = 0.02715828\n",
      "Iteration 76, loss = 0.02674919\n",
      "Iteration 77, loss = 0.03409973\n",
      "Iteration 78, loss = 0.02722891\n",
      "Iteration 79, loss = 0.02682384\n",
      "Iteration 80, loss = 0.02655371\n",
      "Iteration 81, loss = 0.02630992\n",
      "Iteration 82, loss = 0.02608453\n",
      "Iteration 83, loss = 0.02588535\n",
      "Iteration 84, loss = 0.02568941\n",
      "Iteration 85, loss = 0.02552565\n",
      "Iteration 86, loss = 0.02534703\n",
      "Iteration 87, loss = 0.02516154\n",
      "Iteration 88, loss = 0.03422506\n",
      "Iteration 89, loss = 0.02661593\n",
      "Iteration 90, loss = 0.02572997\n",
      "Iteration 91, loss = 0.02548558\n",
      "Iteration 92, loss = 0.02528882\n",
      "Iteration 93, loss = 0.02511872\n",
      "Iteration 94, loss = 0.02497709\n",
      "Iteration 95, loss = 0.02485628\n",
      "Iteration 96, loss = 0.02471380\n",
      "Iteration 97, loss = 0.02456691\n",
      "Iteration 98, loss = 0.02445014\n",
      "Iteration 99, loss = 0.02433099\n",
      "Iteration 100, loss = 0.02422142\n",
      "Iteration 101, loss = 0.02411127\n",
      "Iteration 102, loss = 0.02400532\n",
      "Iteration 103, loss = 0.02389621\n",
      "Iteration 104, loss = 0.02379086\n",
      "Iteration 105, loss = 0.02368428\n",
      "Iteration 106, loss = 0.02360246\n",
      "Iteration 107, loss = 0.02348518\n",
      "Iteration 108, loss = 0.02336806\n",
      "Iteration 109, loss = 0.02325260\n",
      "Iteration 110, loss = 0.02314941\n",
      "Iteration 111, loss = 0.02306180\n",
      "Iteration 112, loss = 0.02293866\n",
      "Iteration 113, loss = 0.02280610\n",
      "Iteration 114, loss = 0.02274757\n",
      "Iteration 115, loss = 0.02258753\n",
      "Iteration 116, loss = 0.02248613\n",
      "Iteration 117, loss = 0.02235528\n",
      "Iteration 118, loss = 0.02222878\n",
      "Iteration 119, loss = 0.02212975\n",
      "Iteration 120, loss = 0.02201013\n",
      "Iteration 121, loss = 0.02188495\n",
      "Iteration 122, loss = 0.02176339\n",
      "Iteration 123, loss = 0.02169770\n",
      "Iteration 124, loss = 0.02156025\n",
      "Iteration 125, loss = 0.02136927\n",
      "Iteration 126, loss = 0.02127103\n",
      "Iteration 127, loss = 0.02111780\n",
      "Iteration 128, loss = 0.02104923\n",
      "Iteration 129, loss = 0.02855037\n",
      "Iteration 130, loss = 0.02255478\n",
      "Iteration 131, loss = 0.02178006\n",
      "Iteration 132, loss = 0.02159587\n",
      "Iteration 133, loss = 0.02144732\n",
      "Iteration 134, loss = 0.02132826\n",
      "Iteration 135, loss = 0.02119827\n",
      "Iteration 136, loss = 0.02112330\n",
      "Iteration 137, loss = 0.02102146\n",
      "Iteration 138, loss = 0.02091515\n",
      "Iteration 139, loss = 0.02082412\n",
      "Iteration 140, loss = 0.02075900\n",
      "Iteration 141, loss = 0.02067103\n",
      "Iteration 142, loss = 0.02442409\n",
      "Iteration 143, loss = 0.02107808\n",
      "Iteration 144, loss = 0.02078273\n",
      "Iteration 145, loss = 0.02067266\n",
      "Iteration 146, loss = 0.02057043\n",
      "Iteration 147, loss = 0.02048124\n",
      "Iteration 148, loss = 0.02040242\n",
      "Iteration 149, loss = 0.02031692\n",
      "Iteration 150, loss = 0.02024824\n",
      "Iteration 151, loss = 0.02018342\n",
      "Iteration 152, loss = 0.02011515\n",
      "Iteration 153, loss = 0.02004414\n",
      "Iteration 154, loss = 0.01994360\n",
      "Iteration 155, loss = 0.01989205\n",
      "Iteration 156, loss = 0.01980367\n",
      "Iteration 157, loss = 0.01974531\n",
      "Iteration 158, loss = 0.01966876\n",
      "Iteration 159, loss = 0.01959514\n",
      "Iteration 160, loss = 0.01951622\n",
      "Iteration 161, loss = 0.01946565\n",
      "Iteration 162, loss = 0.01937780\n",
      "Iteration 163, loss = 0.01932204\n",
      "Iteration 164, loss = 0.01921794\n",
      "Iteration 165, loss = 0.01913314\n",
      "Iteration 166, loss = 0.02105019\n",
      "Iteration 167, loss = 0.01930153\n",
      "Iteration 168, loss = 0.01911708\n",
      "Iteration 169, loss = 0.01897405\n",
      "Iteration 170, loss = 0.01884749\n",
      "Iteration 171, loss = 0.01872931\n",
      "Iteration 172, loss = 0.01861000\n",
      "Iteration 173, loss = 0.01849481\n",
      "Iteration 174, loss = 0.01842198\n",
      "Iteration 175, loss = 0.01828162\n",
      "Iteration 176, loss = 0.01814886\n",
      "Iteration 177, loss = 0.01803628\n",
      "Iteration 178, loss = 0.01790557\n",
      "Iteration 179, loss = 0.01776517\n",
      "Iteration 180, loss = 0.01766281\n",
      "Iteration 181, loss = 0.01751339\n",
      "Iteration 182, loss = 0.01743899\n",
      "Iteration 183, loss = 0.01727098\n",
      "Iteration 184, loss = 0.01715056\n",
      "Iteration 185, loss = 0.01700163\n",
      "Iteration 186, loss = 0.01692998\n",
      "Iteration 187, loss = 0.01686904\n",
      "Iteration 188, loss = 0.01673721\n",
      "Iteration 189, loss = 0.01661952\n",
      "Iteration 190, loss = 0.01646836\n",
      "Iteration 191, loss = 0.01638164\n",
      "Iteration 192, loss = 0.01628523\n",
      "Iteration 193, loss = 0.01613678\n",
      "Iteration 194, loss = 0.01604791\n",
      "Iteration 195, loss = 0.01596024\n",
      "Iteration 196, loss = 0.01585603\n",
      "Iteration 197, loss = 0.01576645\n",
      "Iteration 198, loss = 0.01570438\n",
      "Iteration 199, loss = 0.01562392\n",
      "Iteration 200, loss = 0.01554661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67507222\n",
      "Iteration 2, loss = 0.63681965\n",
      "Iteration 3, loss = 0.62416988\n",
      "Iteration 4, loss = 0.60810084\n",
      "Iteration 5, loss = 0.57835593\n",
      "Iteration 6, loss = 0.53022239\n",
      "Iteration 7, loss = 0.46156122\n",
      "Iteration 8, loss = 0.38096214\n",
      "Iteration 9, loss = 0.30391364\n",
      "Iteration 10, loss = 0.24073558\n",
      "Iteration 11, loss = 0.19332619\n",
      "Iteration 12, loss = 0.15857146\n",
      "Iteration 13, loss = 0.13350888\n",
      "Iteration 14, loss = 0.11462411\n",
      "Iteration 15, loss = 0.10059890\n",
      "Iteration 16, loss = 0.08966708\n",
      "Iteration 17, loss = 0.08102555\n",
      "Iteration 18, loss = 0.07388002\n",
      "Iteration 19, loss = 0.06788874\n",
      "Iteration 20, loss = 0.06292699\n",
      "Iteration 21, loss = 0.05870794\n",
      "Iteration 22, loss = 0.05511319\n",
      "Iteration 23, loss = 0.05201262\n",
      "Iteration 24, loss = 0.04908232\n",
      "Iteration 25, loss = 0.04665738\n",
      "Iteration 26, loss = 0.04424502\n",
      "Iteration 27, loss = 0.04226463\n",
      "Iteration 28, loss = 0.04049978\n",
      "Iteration 29, loss = 0.03877167\n",
      "Iteration 30, loss = 0.03734277\n",
      "Iteration 31, loss = 0.03592681\n",
      "Iteration 32, loss = 0.03466536\n",
      "Iteration 33, loss = 0.03354143\n",
      "Iteration 34, loss = 0.03241423\n",
      "Iteration 35, loss = 0.03142539\n",
      "Iteration 36, loss = 0.03055752\n",
      "Iteration 37, loss = 0.02959315\n",
      "Iteration 38, loss = 0.02877613\n",
      "Iteration 39, loss = 0.02794615\n",
      "Iteration 40, loss = 0.02720520\n",
      "Iteration 41, loss = 0.02650570\n",
      "Iteration 42, loss = 0.02599943\n",
      "Iteration 43, loss = 0.02515789\n",
      "Iteration 44, loss = 0.02450297\n",
      "Iteration 45, loss = 0.02376381\n",
      "Iteration 46, loss = 0.02326504\n",
      "Iteration 47, loss = 0.02373252\n",
      "Iteration 48, loss = 0.02218764\n",
      "Iteration 49, loss = 0.02167980\n",
      "Iteration 50, loss = 0.02129059\n",
      "Iteration 51, loss = 0.02095166\n",
      "Iteration 52, loss = 0.02051711\n",
      "Iteration 53, loss = 0.02020118\n",
      "Iteration 54, loss = 0.01987369\n",
      "Iteration 55, loss = 0.01953802\n",
      "Iteration 56, loss = 0.01921167\n",
      "Iteration 57, loss = 0.01887523\n",
      "Iteration 58, loss = 0.01856927\n",
      "Iteration 59, loss = 0.01826092\n",
      "Iteration 60, loss = 0.01798327\n",
      "Iteration 61, loss = 0.01768114\n",
      "Iteration 62, loss = 0.01743551\n",
      "Iteration 63, loss = 0.01715887\n",
      "Iteration 64, loss = 0.01700750\n",
      "Iteration 65, loss = 0.01670488\n",
      "Iteration 66, loss = 0.01650127\n",
      "Iteration 67, loss = 0.01633010\n",
      "Iteration 68, loss = 0.01609523\n",
      "Iteration 69, loss = 0.01588476\n",
      "Iteration 70, loss = 0.01574073\n",
      "Iteration 71, loss = 0.01555727\n",
      "Iteration 72, loss = 0.01538773\n",
      "Iteration 73, loss = 0.01523225\n",
      "Iteration 74, loss = 0.01503908\n",
      "Iteration 75, loss = 0.01479786\n",
      "Iteration 76, loss = 0.01473553\n",
      "Iteration 77, loss = 0.01448082\n",
      "Iteration 78, loss = 0.01436615\n",
      "Iteration 79, loss = 0.01421803\n",
      "Iteration 80, loss = 0.01406138\n",
      "Iteration 81, loss = 0.01389697\n",
      "Iteration 82, loss = 0.01378457\n",
      "Iteration 83, loss = 0.01368494\n",
      "Iteration 84, loss = 0.01459247\n",
      "Iteration 85, loss = 0.01361974\n",
      "Iteration 86, loss = 0.01334179\n",
      "Iteration 87, loss = 0.01321191\n",
      "Iteration 88, loss = 0.01310333\n",
      "Iteration 89, loss = 0.01304335\n",
      "Iteration 90, loss = 0.01298119\n",
      "Iteration 91, loss = 0.01286554\n",
      "Iteration 92, loss = 0.01280849\n",
      "Iteration 93, loss = 0.01275594\n",
      "Iteration 94, loss = 0.01267781\n",
      "Iteration 95, loss = 0.01261792\n",
      "Iteration 96, loss = 0.01258283\n",
      "Iteration 97, loss = 0.01252158\n",
      "Iteration 98, loss = 0.01249948\n",
      "Iteration 99, loss = 0.01244036\n",
      "Iteration 100, loss = 0.01239908\n",
      "Iteration 101, loss = 0.01235265\n",
      "Iteration 102, loss = 0.01232217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65247077\n",
      "Iteration 2, loss = 0.54450016\n",
      "Iteration 3, loss = 0.46770064\n",
      "Iteration 4, loss = 0.43841997\n",
      "Iteration 5, loss = 0.41707351\n",
      "Iteration 6, loss = 0.39908805\n",
      "Iteration 7, loss = 0.38273331\n",
      "Iteration 8, loss = 0.36749963\n",
      "Iteration 9, loss = 0.35339698\n",
      "Iteration 10, loss = 0.33961396\n",
      "Iteration 11, loss = 0.32661596\n",
      "Iteration 12, loss = 0.31437797\n",
      "Iteration 13, loss = 0.30252385\n",
      "Iteration 14, loss = 0.29093349\n",
      "Iteration 15, loss = 0.28028645\n",
      "Iteration 16, loss = 0.26969676\n",
      "Iteration 17, loss = 0.25971527\n",
      "Iteration 18, loss = 0.25030169\n",
      "Iteration 19, loss = 0.24095532\n",
      "Iteration 20, loss = 0.23193292\n",
      "Iteration 21, loss = 0.22421287\n",
      "Iteration 22, loss = 0.21531599\n",
      "Iteration 23, loss = 0.20745213\n",
      "Iteration 24, loss = 0.19990249\n",
      "Iteration 25, loss = 0.19263497\n",
      "Iteration 26, loss = 0.18584000\n",
      "Iteration 27, loss = 0.17917023\n",
      "Iteration 28, loss = 0.17254734\n",
      "Iteration 29, loss = 0.16622344\n",
      "Iteration 30, loss = 0.16044109\n",
      "Iteration 31, loss = 0.15447311\n",
      "Iteration 32, loss = 0.14920149\n",
      "Iteration 33, loss = 0.14380046\n",
      "Iteration 34, loss = 0.13882277\n",
      "Iteration 35, loss = 0.13407772\n",
      "Iteration 36, loss = 0.12931455\n",
      "Iteration 37, loss = 0.12487730\n",
      "Iteration 38, loss = 0.12048363\n",
      "Iteration 39, loss = 0.11657306\n",
      "Iteration 40, loss = 0.11273361\n",
      "Iteration 41, loss = 0.10907345\n",
      "Iteration 42, loss = 0.10533932\n",
      "Iteration 43, loss = 0.10210275\n",
      "Iteration 44, loss = 0.09871755\n",
      "Iteration 45, loss = 0.09565738\n",
      "Iteration 46, loss = 0.09270177\n",
      "Iteration 47, loss = 0.08971367\n",
      "Iteration 48, loss = 0.08692741\n",
      "Iteration 49, loss = 0.08418058\n",
      "Iteration 50, loss = 0.08155982\n",
      "Iteration 51, loss = 0.07914121\n",
      "Iteration 52, loss = 0.07679875\n",
      "Iteration 53, loss = 0.07448481\n",
      "Iteration 54, loss = 0.07229527\n",
      "Iteration 55, loss = 0.07019086\n",
      "Iteration 56, loss = 0.06811772\n",
      "Iteration 57, loss = 0.06614415\n",
      "Iteration 58, loss = 0.06431284\n",
      "Iteration 59, loss = 0.06246322\n",
      "Iteration 60, loss = 0.06071419\n",
      "Iteration 61, loss = 0.05905439\n",
      "Iteration 62, loss = 0.05741528\n",
      "Iteration 63, loss = 0.05585362\n",
      "Iteration 64, loss = 0.05434757\n",
      "Iteration 65, loss = 0.05286605\n",
      "Iteration 66, loss = 0.05150198\n",
      "Iteration 67, loss = 0.05011773\n",
      "Iteration 68, loss = 0.04881814\n",
      "Iteration 69, loss = 0.04754007\n",
      "Iteration 70, loss = 0.04635207\n",
      "Iteration 71, loss = 0.04515008\n",
      "Iteration 72, loss = 0.04402939\n",
      "Iteration 73, loss = 0.04290609\n",
      "Iteration 74, loss = 0.04184714\n",
      "Iteration 75, loss = 0.04082917\n",
      "Iteration 76, loss = 0.03984331\n",
      "Iteration 77, loss = 0.03889848\n",
      "Iteration 78, loss = 0.03797685\n",
      "Iteration 79, loss = 0.03708162\n",
      "Iteration 80, loss = 0.03620619\n",
      "Iteration 81, loss = 0.03540235\n",
      "Iteration 82, loss = 0.03458297\n",
      "Iteration 83, loss = 0.03380165\n",
      "Iteration 84, loss = 0.03303259\n",
      "Iteration 85, loss = 0.03230871\n",
      "Iteration 86, loss = 0.03160760\n",
      "Iteration 87, loss = 0.03091994\n",
      "Iteration 88, loss = 0.03023668\n",
      "Iteration 89, loss = 0.02960665\n",
      "Iteration 90, loss = 0.02898398\n",
      "Iteration 91, loss = 0.02842145\n",
      "Iteration 92, loss = 0.02780782\n",
      "Iteration 93, loss = 0.02724843\n",
      "Iteration 94, loss = 0.02669530\n",
      "Iteration 95, loss = 0.02616156\n",
      "Iteration 96, loss = 0.02565717\n",
      "Iteration 97, loss = 0.02516735\n",
      "Iteration 98, loss = 0.02469267\n",
      "Iteration 99, loss = 0.02422079\n",
      "Iteration 100, loss = 0.02377730\n",
      "Iteration 101, loss = 0.02334360\n",
      "Iteration 102, loss = 0.02292617\n",
      "Iteration 103, loss = 0.02250852\n",
      "Iteration 104, loss = 0.02211842\n",
      "Iteration 105, loss = 0.02173924\n",
      "Iteration 106, loss = 0.02136387\n",
      "Iteration 107, loss = 0.02100245\n",
      "Iteration 108, loss = 0.02065432\n",
      "Iteration 109, loss = 0.02032663\n",
      "Iteration 110, loss = 0.01999435\n",
      "Iteration 111, loss = 0.01967899\n",
      "Iteration 112, loss = 0.01937770\n",
      "Iteration 113, loss = 0.01908122\n",
      "Iteration 114, loss = 0.01879675\n",
      "Iteration 115, loss = 0.01852104\n",
      "Iteration 116, loss = 0.01825057\n",
      "Iteration 117, loss = 0.01800220\n",
      "Iteration 118, loss = 0.01773804\n",
      "Iteration 119, loss = 0.01749565\n",
      "Iteration 120, loss = 0.01725992\n",
      "Iteration 121, loss = 0.01703054\n",
      "Iteration 122, loss = 0.01680801\n",
      "Iteration 123, loss = 0.01659430\n",
      "Iteration 124, loss = 0.01638833\n",
      "Iteration 125, loss = 0.01618806\n",
      "Iteration 126, loss = 0.01869401\n",
      "Iteration 127, loss = 0.02332091\n",
      "Iteration 128, loss = 0.01832539\n",
      "Iteration 129, loss = 0.01724052\n",
      "Iteration 130, loss = 0.01557458\n",
      "Iteration 131, loss = 0.01521849\n",
      "Iteration 132, loss = 0.01500166\n",
      "Iteration 133, loss = 0.01483819\n",
      "Iteration 134, loss = 0.01469625\n",
      "Iteration 135, loss = 0.01442605\n",
      "Iteration 136, loss = 0.01410868\n",
      "Iteration 137, loss = 0.01394954\n",
      "Iteration 138, loss = 0.01381152\n",
      "Iteration 139, loss = 0.01367683\n",
      "Iteration 140, loss = 0.01354934\n",
      "Iteration 141, loss = 0.01342704\n",
      "Iteration 142, loss = 0.01330908\n",
      "Iteration 143, loss = 0.01319666\n",
      "Iteration 144, loss = 0.01308626\n",
      "Iteration 145, loss = 0.01298016\n",
      "Iteration 146, loss = 0.01287784\n",
      "Iteration 147, loss = 0.01278070\n",
      "Iteration 148, loss = 0.01268542\n",
      "Iteration 149, loss = 0.01259383\n",
      "Iteration 150, loss = 0.01250583\n",
      "Iteration 151, loss = 0.01242034\n",
      "Iteration 152, loss = 0.01233882\n",
      "Iteration 153, loss = 0.01226057\n",
      "Iteration 154, loss = 0.01218370\n",
      "Iteration 155, loss = 0.01211104\n",
      "Iteration 156, loss = 0.01204017\n",
      "Iteration 157, loss = 0.01197299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84842867\n",
      "Iteration 2, loss = 0.82938719\n",
      "Iteration 3, loss = 0.81180739\n",
      "Iteration 4, loss = 0.79540795\n",
      "Iteration 5, loss = 0.78019166\n",
      "Iteration 6, loss = 0.76682741\n",
      "Iteration 7, loss = 0.75381595\n",
      "Iteration 8, loss = 0.74239264\n",
      "Iteration 9, loss = 0.73132088\n",
      "Iteration 10, loss = 0.72107861\n",
      "Iteration 11, loss = 0.71169697\n",
      "Iteration 12, loss = 0.70378186\n",
      "Iteration 13, loss = 0.69661387\n",
      "Iteration 14, loss = 0.69009300\n",
      "Iteration 15, loss = 0.68419410\n",
      "Iteration 16, loss = 0.67883896\n",
      "Iteration 17, loss = 0.67400084\n",
      "Iteration 18, loss = 0.66918457\n",
      "Iteration 19, loss = 0.66526118\n",
      "Iteration 20, loss = 0.66180983\n",
      "Iteration 21, loss = 0.65832148\n",
      "Iteration 22, loss = 0.65557195\n",
      "Iteration 23, loss = 0.65281384\n",
      "Iteration 24, loss = 0.65034641\n",
      "Iteration 25, loss = 0.64852110\n",
      "Iteration 26, loss = 0.64699173\n",
      "Iteration 27, loss = 0.64533136\n",
      "Iteration 28, loss = 0.64413515\n",
      "Iteration 29, loss = 0.64286998\n",
      "Iteration 30, loss = 0.64174130\n",
      "Iteration 31, loss = 0.64102575\n",
      "Iteration 32, loss = 0.64043791\n",
      "Iteration 33, loss = 0.63972547\n",
      "Iteration 34, loss = 0.63928806\n",
      "Iteration 35, loss = 0.63875801\n",
      "Iteration 36, loss = 0.63844951\n",
      "Iteration 37, loss = 0.63806785\n",
      "Iteration 38, loss = 0.63786848\n",
      "Iteration 39, loss = 0.63758541\n",
      "Iteration 40, loss = 0.63744775\n",
      "Iteration 41, loss = 0.63725628\n",
      "Iteration 42, loss = 0.63717171\n",
      "Iteration 43, loss = 0.63714022\n",
      "Iteration 44, loss = 0.63699657\n",
      "Iteration 45, loss = 0.63696298\n",
      "Iteration 46, loss = 0.63685567\n",
      "Iteration 47, loss = 0.63675692\n",
      "Iteration 48, loss = 0.63669574\n",
      "Iteration 49, loss = 0.63664368\n",
      "Iteration 50, loss = 0.63661045\n",
      "Iteration 51, loss = 0.63661842\n",
      "Iteration 52, loss = 0.63659558\n",
      "Iteration 53, loss = 0.63658159\n",
      "Iteration 54, loss = 0.63658817\n",
      "Iteration 55, loss = 0.63657399\n",
      "Iteration 56, loss = 0.63656873\n",
      "Iteration 57, loss = 0.63657394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64392067\n",
      "Iteration 2, loss = 0.57057102\n",
      "Iteration 3, loss = 0.45288158\n",
      "Iteration 4, loss = 0.35814044\n",
      "Iteration 5, loss = 0.31270941\n",
      "Iteration 6, loss = 0.28654644\n",
      "Iteration 7, loss = 0.26735686\n",
      "Iteration 8, loss = 0.25196267\n",
      "Iteration 9, loss = 0.23831574\n",
      "Iteration 10, loss = 0.22639855\n",
      "Iteration 11, loss = 0.21580274\n",
      "Iteration 12, loss = 0.20561848\n",
      "Iteration 13, loss = 0.19611674\n",
      "Iteration 14, loss = 0.18728417\n",
      "Iteration 15, loss = 0.17915545\n",
      "Iteration 16, loss = 0.17139776\n",
      "Iteration 17, loss = 0.16490197\n",
      "Iteration 18, loss = 0.15817672\n",
      "Iteration 19, loss = 0.15197544\n",
      "Iteration 20, loss = 0.14600396\n",
      "Iteration 21, loss = 0.14019890\n",
      "Iteration 22, loss = 0.13481087\n",
      "Iteration 23, loss = 0.12986091\n",
      "Iteration 24, loss = 0.12506542\n",
      "Iteration 25, loss = 0.12050487\n",
      "Iteration 26, loss = 0.11640689\n",
      "Iteration 27, loss = 0.11203930\n",
      "Iteration 28, loss = 0.10799835\n",
      "Iteration 29, loss = 0.10424792\n",
      "Iteration 30, loss = 0.10066748\n",
      "Iteration 31, loss = 0.09725366\n",
      "Iteration 32, loss = 0.09396563\n",
      "Iteration 33, loss = 0.09083749\n",
      "Iteration 34, loss = 0.08762463\n",
      "Iteration 35, loss = 0.08475059\n",
      "Iteration 36, loss = 0.08203653\n",
      "Iteration 37, loss = 0.07945788\n",
      "Iteration 38, loss = 0.07694166\n",
      "Iteration 39, loss = 0.07451841\n",
      "Iteration 40, loss = 0.07214756\n",
      "Iteration 41, loss = 0.06974340\n",
      "Iteration 42, loss = 0.06763326\n",
      "Iteration 43, loss = 0.06569005\n",
      "Iteration 44, loss = 0.06338681\n",
      "Iteration 45, loss = 0.06139681\n",
      "Iteration 46, loss = 0.05963432\n",
      "Iteration 47, loss = 0.05778542\n",
      "Iteration 48, loss = 0.05594316\n",
      "Iteration 49, loss = 0.05419338\n",
      "Iteration 50, loss = 0.05264162\n",
      "Iteration 51, loss = 0.05113217\n",
      "Iteration 52, loss = 0.04966882\n",
      "Iteration 53, loss = 0.04805480\n",
      "Iteration 54, loss = 0.04673166\n",
      "Iteration 55, loss = 0.04525794\n",
      "Iteration 56, loss = 0.04398340\n",
      "Iteration 57, loss = 0.04260600\n",
      "Iteration 58, loss = 0.04146810\n",
      "Iteration 59, loss = 0.04284728\n",
      "Iteration 60, loss = 0.03957732\n",
      "Iteration 61, loss = 0.03846433\n",
      "Iteration 62, loss = 0.03735603\n",
      "Iteration 63, loss = 0.03625972\n",
      "Iteration 64, loss = 0.03535212\n",
      "Iteration 65, loss = 0.03429091\n",
      "Iteration 66, loss = 0.03351991\n",
      "Iteration 67, loss = 0.03262877\n",
      "Iteration 68, loss = 0.03178968\n",
      "Iteration 69, loss = 0.03102673\n",
      "Iteration 70, loss = 0.03018100\n",
      "Iteration 71, loss = 0.02950129\n",
      "Iteration 72, loss = 0.02879111\n",
      "Iteration 73, loss = 0.02804226\n",
      "Iteration 74, loss = 0.02740140\n",
      "Iteration 75, loss = 0.02671472\n",
      "Iteration 76, loss = 0.02610745\n",
      "Iteration 77, loss = 0.02550122\n",
      "Iteration 78, loss = 0.02493731\n",
      "Iteration 79, loss = 0.02439185\n",
      "Iteration 80, loss = 0.02381489\n",
      "Iteration 81, loss = 0.02330465\n",
      "Iteration 82, loss = 0.02285030\n",
      "Iteration 83, loss = 0.02233950\n",
      "Iteration 84, loss = 0.02193710\n",
      "Iteration 85, loss = 0.02148057\n",
      "Iteration 86, loss = 0.02101941\n",
      "Iteration 87, loss = 0.02064576\n",
      "Iteration 88, loss = 0.02025737\n",
      "Iteration 89, loss = 0.01988363\n",
      "Iteration 90, loss = 0.01955868\n",
      "Iteration 91, loss = 0.01921826\n",
      "Iteration 92, loss = 0.01883284\n",
      "Iteration 93, loss = 0.01852185\n",
      "Iteration 94, loss = 0.01822208\n",
      "Iteration 95, loss = 0.01792006\n",
      "Iteration 96, loss = 0.01763906\n",
      "Iteration 97, loss = 0.01735799\n",
      "Iteration 98, loss = 0.01708727\n",
      "Iteration 99, loss = 0.01682719\n",
      "Iteration 100, loss = 0.01657802\n",
      "Iteration 101, loss = 0.01635148\n",
      "Iteration 102, loss = 0.01612446\n",
      "Iteration 103, loss = 0.01589657\n",
      "Iteration 104, loss = 0.01567127\n",
      "Iteration 105, loss = 0.01546011\n",
      "Iteration 106, loss = 0.01526105\n",
      "Iteration 107, loss = 0.01506949\n",
      "Iteration 108, loss = 0.01487582\n",
      "Iteration 109, loss = 0.01469303\n",
      "Iteration 110, loss = 0.01451412\n",
      "Iteration 111, loss = 0.01435072\n",
      "Iteration 112, loss = 0.01418344\n",
      "Iteration 113, loss = 0.01402276\n",
      "Iteration 114, loss = 0.01387167\n",
      "Iteration 115, loss = 0.01372524\n",
      "Iteration 116, loss = 0.01358404\n",
      "Iteration 117, loss = 0.01344988\n",
      "Iteration 118, loss = 0.01331678\n",
      "Iteration 119, loss = 0.01319222\n",
      "Iteration 120, loss = 0.01306681\n",
      "Iteration 121, loss = 0.01294615\n",
      "Iteration 122, loss = 0.01282814\n",
      "Iteration 123, loss = 0.01271029\n",
      "Iteration 124, loss = 0.01260550\n",
      "Iteration 125, loss = 0.01250093\n",
      "Iteration 126, loss = 0.01240063\n",
      "Iteration 127, loss = 0.01229553\n",
      "Iteration 128, loss = 0.01220037\n",
      "Iteration 129, loss = 0.01210807\n",
      "Iteration 130, loss = 0.01202392\n",
      "Iteration 131, loss = 0.01194259\n",
      "Iteration 132, loss = 0.01186213\n",
      "Iteration 133, loss = 0.01178729\n",
      "Iteration 134, loss = 0.01171573\n",
      "Iteration 135, loss = 0.01164828\n",
      "Iteration 136, loss = 0.01235244\n",
      "Iteration 137, loss = 0.01159213\n",
      "Iteration 138, loss = 0.01121138\n",
      "Iteration 139, loss = 0.01105973\n",
      "Iteration 140, loss = 0.01098690\n",
      "Iteration 141, loss = 0.01092029\n",
      "Iteration 142, loss = 0.01085923\n",
      "Iteration 143, loss = 0.01079930\n",
      "Iteration 144, loss = 0.01074363\n",
      "Iteration 145, loss = 0.01069307\n",
      "Iteration 146, loss = 0.01064518\n",
      "Iteration 147, loss = 0.01059696\n",
      "Iteration 148, loss = 0.01054989\n",
      "Iteration 149, loss = 0.01050586\n",
      "Iteration 150, loss = 0.01046443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06821940\n",
      "Iteration 2, loss = 0.88458434\n",
      "Iteration 3, loss = 0.77877545\n",
      "Iteration 4, loss = 0.72230554\n",
      "Iteration 5, loss = 0.68790005\n",
      "Iteration 6, loss = 0.66634011\n",
      "Iteration 7, loss = 0.65504500\n",
      "Iteration 8, loss = 0.65213609\n",
      "Iteration 9, loss = 0.65022149\n",
      "Iteration 10, loss = 0.64843987\n",
      "Iteration 11, loss = 0.64681760\n",
      "Iteration 12, loss = 0.64532613\n",
      "Iteration 13, loss = 0.64400163\n",
      "Iteration 14, loss = 0.64281760\n",
      "Iteration 15, loss = 0.64178088\n",
      "Iteration 16, loss = 0.64087601\n",
      "Iteration 17, loss = 0.64008947\n",
      "Iteration 18, loss = 0.63942010\n",
      "Iteration 19, loss = 0.63885271\n",
      "Iteration 20, loss = 0.63855320\n",
      "Iteration 21, loss = 0.63815280\n",
      "Iteration 22, loss = 0.63778879\n",
      "Iteration 23, loss = 0.63750410\n",
      "Iteration 24, loss = 0.63739622\n",
      "Iteration 25, loss = 0.63719283\n",
      "Iteration 26, loss = 0.63712417\n",
      "Iteration 27, loss = 0.63710062\n",
      "Iteration 28, loss = 0.63695134\n",
      "Iteration 29, loss = 0.63691933\n",
      "Iteration 30, loss = 0.63682009\n",
      "Iteration 31, loss = 0.63673491\n",
      "Iteration 32, loss = 0.63667135\n",
      "Iteration 33, loss = 0.63662784\n",
      "Iteration 34, loss = 0.63659612\n",
      "Iteration 35, loss = 0.63658217\n",
      "Iteration 36, loss = 0.63656675\n",
      "Iteration 37, loss = 0.63656025\n",
      "Iteration 38, loss = 0.63656821\n",
      "Iteration 39, loss = 0.63658623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56501644\n",
      "Iteration 2, loss = 0.39801900\n",
      "Iteration 3, loss = 0.26562373\n",
      "Iteration 4, loss = 0.16156596\n",
      "Iteration 5, loss = 0.10048522\n",
      "Iteration 6, loss = 0.07068095\n",
      "Iteration 7, loss = 0.05514574\n",
      "Iteration 8, loss = 0.04642145\n",
      "Iteration 9, loss = 0.04038830\n",
      "Iteration 10, loss = 0.03604739\n",
      "Iteration 11, loss = 0.03287825\n",
      "Iteration 12, loss = 0.03039336\n",
      "Iteration 13, loss = 0.02835249\n",
      "Iteration 14, loss = 0.02728740\n",
      "Iteration 15, loss = 0.02734017\n",
      "Iteration 16, loss = 0.02414443\n",
      "Iteration 17, loss = 0.02316990\n",
      "Iteration 18, loss = 0.02235310\n",
      "Iteration 19, loss = 0.02125977\n",
      "Iteration 20, loss = 0.02058836\n",
      "Iteration 21, loss = 0.01968699\n",
      "Iteration 22, loss = 0.01918331\n",
      "Iteration 23, loss = 0.01839852\n",
      "Iteration 24, loss = 0.01809874\n",
      "Iteration 25, loss = 0.01716613\n",
      "Iteration 26, loss = 0.01709374\n",
      "Iteration 27, loss = 0.02079381\n",
      "Iteration 28, loss = 0.01649713\n",
      "Iteration 29, loss = 0.01566580\n",
      "Iteration 30, loss = 0.01520770\n",
      "Iteration 31, loss = 0.01472203\n",
      "Iteration 32, loss = 0.01422014\n",
      "Iteration 33, loss = 0.01396576\n",
      "Iteration 34, loss = 0.01345077\n",
      "Iteration 35, loss = 0.01309041\n",
      "Iteration 36, loss = 0.01307969\n",
      "Iteration 37, loss = 0.01226380\n",
      "Iteration 38, loss = 0.01172686\n",
      "Iteration 39, loss = 0.01158001\n",
      "Iteration 40, loss = 0.01108708\n",
      "Iteration 41, loss = 0.01057696\n",
      "Iteration 42, loss = 0.01058896\n",
      "Iteration 43, loss = 0.00990093\n",
      "Iteration 44, loss = 0.00959400\n",
      "Iteration 45, loss = 0.00916430\n",
      "Iteration 46, loss = 0.00867443\n",
      "Iteration 47, loss = 0.00835106\n",
      "Iteration 48, loss = 0.00793080\n",
      "Iteration 49, loss = 0.00773418\n",
      "Iteration 50, loss = 0.00717877\n",
      "Iteration 51, loss = 0.00697281\n",
      "Iteration 52, loss = 0.00661545\n",
      "Iteration 53, loss = 0.00640808\n",
      "Iteration 54, loss = 0.00605264\n",
      "Iteration 55, loss = 0.00573955\n",
      "Iteration 56, loss = 0.00545690\n",
      "Iteration 57, loss = 0.00514285\n",
      "Iteration 58, loss = 0.00472697\n",
      "Iteration 59, loss = 0.00431217\n",
      "Iteration 60, loss = 0.00419437\n",
      "Iteration 61, loss = 0.00388201\n",
      "Iteration 62, loss = 0.00353769\n",
      "Iteration 63, loss = 0.00327163\n",
      "Iteration 64, loss = 0.00300199\n",
      "Iteration 65, loss = 0.00282356\n",
      "Iteration 66, loss = 0.00266302\n",
      "Iteration 67, loss = 0.00243756\n",
      "Iteration 68, loss = 0.00235643\n",
      "Iteration 69, loss = 0.00221299\n",
      "Iteration 70, loss = 0.00211590\n",
      "Iteration 71, loss = 0.00192736\n",
      "Iteration 72, loss = 0.00184568\n",
      "Iteration 73, loss = 0.00175156\n",
      "Iteration 74, loss = 0.00166294\n",
      "Iteration 75, loss = 0.00155121\n",
      "Iteration 76, loss = 0.00148491\n",
      "Iteration 77, loss = 0.00138440\n",
      "Iteration 78, loss = 0.00128641\n",
      "Iteration 79, loss = 0.00125201\n",
      "Iteration 80, loss = 0.00115655\n",
      "Iteration 81, loss = 0.00108768\n",
      "Iteration 82, loss = 0.00100553\n",
      "Iteration 83, loss = 0.00096932\n",
      "Iteration 84, loss = 0.00156229\n",
      "Iteration 85, loss = 0.00094998\n",
      "Iteration 86, loss = 0.00086945\n",
      "Iteration 87, loss = 0.00081987\n",
      "Iteration 88, loss = 0.00078964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63475684\n",
      "Iteration 2, loss = 0.43732199\n",
      "Iteration 3, loss = 0.25779388\n",
      "Iteration 4, loss = 0.14030295\n",
      "Iteration 5, loss = 0.07530769\n",
      "Iteration 6, loss = 0.05576184\n",
      "Iteration 7, loss = 0.04706298\n",
      "Iteration 8, loss = 0.04228837\n",
      "Iteration 9, loss = 0.03828643\n",
      "Iteration 10, loss = 0.03475957\n",
      "Iteration 11, loss = 0.03190362\n",
      "Iteration 12, loss = 0.02962112\n",
      "Iteration 13, loss = 0.02737616\n",
      "Iteration 14, loss = 0.02623903\n",
      "Iteration 15, loss = 0.02470882\n",
      "Iteration 16, loss = 0.02325207\n",
      "Iteration 17, loss = 0.02224444\n",
      "Iteration 18, loss = 0.02114157\n",
      "Iteration 19, loss = 0.02050264\n",
      "Iteration 20, loss = 0.01940764\n",
      "Iteration 21, loss = 0.01858186\n",
      "Iteration 22, loss = 0.01810084\n",
      "Iteration 23, loss = 0.01741150\n",
      "Iteration 24, loss = 0.01665573\n",
      "Iteration 25, loss = 0.01595728\n",
      "Iteration 26, loss = 0.01549053\n",
      "Iteration 27, loss = 0.01469152\n",
      "Iteration 28, loss = 0.01410978\n",
      "Iteration 29, loss = 0.01362606\n",
      "Iteration 30, loss = 0.01314477\n",
      "Iteration 31, loss = 0.01263509\n",
      "Iteration 32, loss = 0.01224951\n",
      "Iteration 33, loss = 0.01189636\n",
      "Iteration 34, loss = 0.01159243\n",
      "Iteration 35, loss = 0.01089245\n",
      "Iteration 36, loss = 0.01045374\n",
      "Iteration 37, loss = 0.01012306\n",
      "Iteration 38, loss = 0.00977690\n",
      "Iteration 39, loss = 0.00949747\n",
      "Iteration 40, loss = 0.00915430\n",
      "Iteration 41, loss = 0.00922402\n",
      "Iteration 42, loss = 0.00864744\n",
      "Iteration 43, loss = 0.00838728\n",
      "Iteration 44, loss = 0.00815112\n",
      "Iteration 45, loss = 0.00798964\n",
      "Iteration 46, loss = 0.00769997\n",
      "Iteration 47, loss = 0.00749721\n",
      "Iteration 48, loss = 0.00734917\n",
      "Iteration 49, loss = 0.00716952\n",
      "Iteration 50, loss = 0.00696580\n",
      "Iteration 51, loss = 0.00687651\n",
      "Iteration 52, loss = 0.00676391\n",
      "Iteration 53, loss = 0.00664135\n",
      "Iteration 54, loss = 0.00656969\n",
      "Iteration 55, loss = 0.00632774\n",
      "Iteration 56, loss = 0.00604880\n",
      "Iteration 57, loss = 0.00582378\n",
      "Iteration 58, loss = 0.00575844\n",
      "Iteration 59, loss = 0.00512251\n",
      "Iteration 60, loss = 0.00496284\n",
      "Iteration 61, loss = 0.00487672\n",
      "Iteration 62, loss = 0.00486252\n",
      "Iteration 63, loss = 0.00451892\n",
      "Iteration 64, loss = 0.00456550\n",
      "Iteration 65, loss = 0.00435150\n",
      "Iteration 66, loss = 0.00422253\n",
      "Iteration 67, loss = 0.00411376\n",
      "Iteration 68, loss = 0.00405451\n",
      "Iteration 69, loss = 0.00394481\n",
      "Iteration 70, loss = 0.00377724\n",
      "Iteration 71, loss = 0.00372606\n",
      "Iteration 72, loss = 0.00357220\n",
      "Iteration 73, loss = 0.00320111\n",
      "Iteration 74, loss = 0.00299200\n",
      "Iteration 75, loss = 0.00281366\n",
      "Iteration 76, loss = 0.00265643\n",
      "Iteration 77, loss = 0.00249096\n",
      "Iteration 78, loss = 0.00238853\n",
      "Iteration 79, loss = 0.00221483\n",
      "Iteration 80, loss = 0.00211564\n",
      "Iteration 81, loss = 0.00202012\n",
      "Iteration 82, loss = 0.00192874\n",
      "Iteration 83, loss = 0.00190450\n",
      "Iteration 84, loss = 0.00173429\n",
      "Iteration 85, loss = 0.00161605\n",
      "Iteration 86, loss = 0.00148825\n",
      "Iteration 87, loss = 0.00136819\n",
      "Iteration 88, loss = 0.00139124\n",
      "Iteration 89, loss = 0.00117562\n",
      "Iteration 90, loss = 0.00100068\n",
      "Iteration 91, loss = 0.00132937\n",
      "Iteration 92, loss = 0.00342010\n",
      "Iteration 93, loss = 0.00381820\n",
      "Iteration 94, loss = 0.00226314\n",
      "Iteration 95, loss = 0.00164247\n",
      "Iteration 96, loss = 0.00159209\n",
      "Iteration 97, loss = 0.00103662\n",
      "Iteration 98, loss = 0.00061851\n",
      "Iteration 99, loss = 0.00054303\n",
      "Iteration 100, loss = 0.00048187\n",
      "Iteration 101, loss = 0.00041731\n",
      "Iteration 102, loss = 0.00038443\n",
      "Iteration 103, loss = 0.00034660\n",
      "Iteration 104, loss = 0.00032770\n",
      "Iteration 105, loss = 0.00029453\n",
      "Iteration 106, loss = 0.00027303\n",
      "Iteration 107, loss = 0.00026373\n",
      "Iteration 108, loss = 0.00024240\n",
      "Iteration 109, loss = 0.00022974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73028244\n",
      "Iteration 2, loss = 0.51968442\n",
      "Iteration 3, loss = 0.23987160\n",
      "Iteration 4, loss = 0.11398696\n",
      "Iteration 5, loss = 0.07864546\n",
      "Iteration 6, loss = 0.06140808\n",
      "Iteration 7, loss = 0.05231225\n",
      "Iteration 8, loss = 0.04592738\n",
      "Iteration 9, loss = 0.04153372\n",
      "Iteration 10, loss = 0.03816638\n",
      "Iteration 11, loss = 0.03568601\n",
      "Iteration 12, loss = 0.03305808\n",
      "Iteration 13, loss = 0.03081840\n",
      "Iteration 14, loss = 0.02941315\n",
      "Iteration 15, loss = 0.02777598\n",
      "Iteration 16, loss = 0.02591137\n",
      "Iteration 17, loss = 0.02484654\n",
      "Iteration 18, loss = 0.02378034\n",
      "Iteration 19, loss = 0.02275318\n",
      "Iteration 20, loss = 0.02151903\n",
      "Iteration 21, loss = 0.02067153\n",
      "Iteration 22, loss = 0.02010130\n",
      "Iteration 23, loss = 0.01911407\n",
      "Iteration 24, loss = 0.01847628\n",
      "Iteration 25, loss = 0.01746424\n",
      "Iteration 26, loss = 0.01713824\n",
      "Iteration 27, loss = 0.01652784\n",
      "Iteration 28, loss = 0.01566319\n",
      "Iteration 29, loss = 0.01545796\n",
      "Iteration 30, loss = 0.01444259\n",
      "Iteration 31, loss = 0.01396249\n",
      "Iteration 32, loss = 0.01365377\n",
      "Iteration 33, loss = 0.01292013\n",
      "Iteration 34, loss = 0.01241109\n",
      "Iteration 35, loss = 0.01214426\n",
      "Iteration 36, loss = 0.01133050\n",
      "Iteration 37, loss = 0.01069480\n",
      "Iteration 38, loss = 0.01246015\n",
      "Iteration 39, loss = 0.01035912\n",
      "Iteration 40, loss = 0.00956527\n",
      "Iteration 41, loss = 0.00920024\n",
      "Iteration 42, loss = 0.00874359\n",
      "Iteration 43, loss = 0.00833510\n",
      "Iteration 44, loss = 0.00815472\n",
      "Iteration 45, loss = 0.00765125\n",
      "Iteration 46, loss = 0.00719719\n",
      "Iteration 47, loss = 0.00721927\n",
      "Iteration 48, loss = 0.00699158\n",
      "Iteration 49, loss = 0.00627165\n",
      "Iteration 50, loss = 0.00594715\n",
      "Iteration 51, loss = 0.00574198\n",
      "Iteration 52, loss = 0.00554799\n",
      "Iteration 53, loss = 0.00515604\n",
      "Iteration 54, loss = 0.00491970\n",
      "Iteration 55, loss = 0.00465235\n",
      "Iteration 56, loss = 0.00456739\n",
      "Iteration 57, loss = 0.00436982\n",
      "Iteration 58, loss = 0.00407836\n",
      "Iteration 59, loss = 0.00376450\n",
      "Iteration 60, loss = 0.00367784\n",
      "Iteration 61, loss = 0.00346553\n",
      "Iteration 62, loss = 0.00329359\n",
      "Iteration 63, loss = 0.00308707\n",
      "Iteration 64, loss = 0.00298926\n",
      "Iteration 65, loss = 0.00280946\n",
      "Iteration 66, loss = 0.00260526\n",
      "Iteration 67, loss = 0.00273614\n",
      "Iteration 68, loss = 0.00237749\n",
      "Iteration 69, loss = 0.00230971\n",
      "Iteration 70, loss = 0.00213319\n",
      "Iteration 71, loss = 0.00216822\n",
      "Iteration 72, loss = 0.00194000\n",
      "Iteration 73, loss = 0.00177259\n",
      "Iteration 74, loss = 0.00169808\n",
      "Iteration 75, loss = 0.00162360\n",
      "Iteration 76, loss = 0.00151986\n",
      "Iteration 77, loss = 0.00143868\n",
      "Iteration 78, loss = 0.00118287\n",
      "Iteration 79, loss = 0.00105941\n",
      "Iteration 80, loss = 0.00099526\n",
      "Iteration 81, loss = 0.00088757\n",
      "Iteration 82, loss = 0.00085002\n",
      "Iteration 83, loss = 0.00079478\n",
      "Iteration 84, loss = 0.00071346\n",
      "Iteration 85, loss = 0.00069244\n",
      "Iteration 86, loss = 0.00068748\n",
      "Iteration 87, loss = 0.00060996\n",
      "Iteration 88, loss = 0.00058740\n",
      "Iteration 89, loss = 0.00054219\n",
      "Iteration 90, loss = 0.00051052\n",
      "Iteration 91, loss = 0.00050477\n",
      "Iteration 92, loss = 0.00044532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62780103\n",
      "Iteration 2, loss = 0.51275418\n",
      "Iteration 3, loss = 0.40167689\n",
      "Iteration 4, loss = 0.31938752\n",
      "Iteration 5, loss = 0.19686746\n",
      "Iteration 6, loss = 0.09270497\n",
      "Iteration 7, loss = 0.05934228\n",
      "Iteration 8, loss = 0.04857350\n",
      "Iteration 9, loss = 0.04250864\n",
      "Iteration 10, loss = 0.03810819\n",
      "Iteration 11, loss = 0.03450160\n",
      "Iteration 12, loss = 0.03183862\n",
      "Iteration 13, loss = 0.02957810\n",
      "Iteration 14, loss = 0.02788713\n",
      "Iteration 15, loss = 0.02582302\n",
      "Iteration 16, loss = 0.02440042\n",
      "Iteration 17, loss = 0.02291397\n",
      "Iteration 18, loss = 0.02213669\n",
      "Iteration 19, loss = 0.02094175\n",
      "Iteration 20, loss = 0.01983192\n",
      "Iteration 21, loss = 0.01874632\n",
      "Iteration 22, loss = 0.01809016\n",
      "Iteration 23, loss = 0.01700458\n",
      "Iteration 24, loss = 0.01616368\n",
      "Iteration 25, loss = 0.01830541\n",
      "Iteration 26, loss = 0.01489684\n",
      "Iteration 27, loss = 0.01406580\n",
      "Iteration 28, loss = 0.01321197\n",
      "Iteration 29, loss = 0.01267669\n",
      "Iteration 30, loss = 0.01212749\n",
      "Iteration 31, loss = 0.01145537\n",
      "Iteration 32, loss = 0.01143768\n",
      "Iteration 33, loss = 0.01056499\n",
      "Iteration 34, loss = 0.00995133\n",
      "Iteration 35, loss = 0.00945643\n",
      "Iteration 36, loss = 0.00917046\n",
      "Iteration 37, loss = 0.00830854\n",
      "Iteration 38, loss = 0.00803827\n",
      "Iteration 39, loss = 0.00742019\n",
      "Iteration 40, loss = 0.01229910\n",
      "Iteration 41, loss = 0.00783510\n",
      "Iteration 42, loss = 0.00703767\n",
      "Iteration 43, loss = 0.00642099\n",
      "Iteration 44, loss = 0.00603710\n",
      "Iteration 45, loss = 0.00557664\n",
      "Iteration 46, loss = 0.00519779\n",
      "Iteration 47, loss = 0.00488419\n",
      "Iteration 48, loss = 0.00455620\n",
      "Iteration 49, loss = 0.00423956\n",
      "Iteration 50, loss = 0.00405705\n",
      "Iteration 51, loss = 0.00383194\n",
      "Iteration 52, loss = 0.00361538\n",
      "Iteration 53, loss = 0.00313447\n",
      "Iteration 54, loss = 0.00290726\n",
      "Iteration 55, loss = 0.00275480\n",
      "Iteration 56, loss = 0.00256991\n",
      "Iteration 57, loss = 0.00531789\n",
      "Iteration 58, loss = 0.00274408\n",
      "Iteration 59, loss = 0.00212559\n",
      "Iteration 60, loss = 0.00195601\n",
      "Iteration 61, loss = 0.00179400\n",
      "Iteration 62, loss = 0.00174776\n",
      "Iteration 63, loss = 0.00160557\n",
      "Iteration 64, loss = 0.00146694\n",
      "Iteration 65, loss = 0.00141718\n",
      "Iteration 66, loss = 0.00129611\n",
      "Iteration 67, loss = 0.00126340\n",
      "Iteration 68, loss = 0.00111879\n",
      "Iteration 69, loss = 0.00105962\n",
      "Iteration 70, loss = 0.00102335\n",
      "Iteration 71, loss = 0.00091958\n",
      "Iteration 72, loss = 0.00088395\n",
      "Iteration 73, loss = 0.00082248\n",
      "Iteration 74, loss = 0.00077565\n",
      "Iteration 75, loss = 0.00191370\n",
      "Iteration 76, loss = 0.00089285\n",
      "Iteration 77, loss = 0.00886465\n",
      "Iteration 78, loss = 0.00159364\n",
      "Iteration 79, loss = 0.00096976\n",
      "Iteration 80, loss = 0.00078231\n",
      "Iteration 81, loss = 0.00070655\n",
      "Iteration 82, loss = 0.00063512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59389928\n",
      "Iteration 2, loss = 0.35354112\n",
      "Iteration 3, loss = 0.16669158\n",
      "Iteration 4, loss = 0.10207506\n",
      "Iteration 5, loss = 0.07745499\n",
      "Iteration 6, loss = 0.06493434\n",
      "Iteration 7, loss = 0.05685244\n",
      "Iteration 8, loss = 0.05204061\n",
      "Iteration 9, loss = 0.04714711\n",
      "Iteration 10, loss = 0.04369711\n",
      "Iteration 11, loss = 0.04074529\n",
      "Iteration 12, loss = 0.03827180\n",
      "Iteration 13, loss = 0.03584525\n",
      "Iteration 14, loss = 0.03395571\n",
      "Iteration 15, loss = 0.03210957\n",
      "Iteration 16, loss = 0.03067852\n",
      "Iteration 17, loss = 0.02953544\n",
      "Iteration 18, loss = 0.02780023\n",
      "Iteration 19, loss = 0.02671557\n",
      "Iteration 20, loss = 0.02523058\n",
      "Iteration 21, loss = 0.02433683\n",
      "Iteration 22, loss = 0.02349929\n",
      "Iteration 23, loss = 0.02332688\n",
      "Iteration 24, loss = 0.02168610\n",
      "Iteration 25, loss = 0.02067709\n",
      "Iteration 26, loss = 0.02036810\n",
      "Iteration 27, loss = 0.01942394\n",
      "Iteration 28, loss = 0.01854777\n",
      "Iteration 29, loss = 0.01797866\n",
      "Iteration 30, loss = 0.01723168\n",
      "Iteration 31, loss = 0.01639011\n",
      "Iteration 32, loss = 0.01586446\n",
      "Iteration 33, loss = 0.01541597\n",
      "Iteration 34, loss = 0.01467815\n",
      "Iteration 35, loss = 0.01416763\n",
      "Iteration 36, loss = 0.01376821\n",
      "Iteration 37, loss = 0.01331198\n",
      "Iteration 38, loss = 0.01276609\n",
      "Iteration 39, loss = 0.01218357\n",
      "Iteration 40, loss = 0.01161829\n",
      "Iteration 41, loss = 0.01138227\n",
      "Iteration 42, loss = 0.01105184\n",
      "Iteration 43, loss = 0.01054760\n",
      "Iteration 44, loss = 0.00998949\n",
      "Iteration 45, loss = 0.00986865\n",
      "Iteration 46, loss = 0.00943946\n",
      "Iteration 47, loss = 0.00897355\n",
      "Iteration 48, loss = 0.01448994\n",
      "Iteration 49, loss = 0.00914017\n",
      "Iteration 50, loss = 0.00860650\n",
      "Iteration 51, loss = 0.00809728\n",
      "Iteration 52, loss = 0.00785056\n",
      "Iteration 53, loss = 0.00763468\n",
      "Iteration 54, loss = 0.00724918\n",
      "Iteration 55, loss = 0.00707902\n",
      "Iteration 56, loss = 0.00673128\n",
      "Iteration 57, loss = 0.00643568\n",
      "Iteration 58, loss = 0.00624415\n",
      "Iteration 59, loss = 0.00599507\n",
      "Iteration 60, loss = 0.00573901\n",
      "Iteration 61, loss = 0.00557118\n",
      "Iteration 62, loss = 0.00525901\n",
      "Iteration 63, loss = 0.00504938\n",
      "Iteration 64, loss = 0.00491080\n",
      "Iteration 65, loss = 0.00458024\n",
      "Iteration 66, loss = 0.00435230\n",
      "Iteration 67, loss = 0.00413248\n",
      "Iteration 68, loss = 0.00393141\n",
      "Iteration 69, loss = 0.00381181\n",
      "Iteration 70, loss = 0.00366765\n",
      "Iteration 71, loss = 0.00354603\n",
      "Iteration 72, loss = 0.00332214\n",
      "Iteration 73, loss = 0.00321334\n",
      "Iteration 74, loss = 0.00311002\n",
      "Iteration 75, loss = 0.00292206\n",
      "Iteration 76, loss = 0.00285121\n",
      "Iteration 77, loss = 0.00268526\n",
      "Iteration 78, loss = 0.00256974\n",
      "Iteration 79, loss = 0.00244781\n",
      "Iteration 80, loss = 0.00233703\n",
      "Iteration 81, loss = 0.00228386\n",
      "Iteration 82, loss = 0.00219708\n",
      "Iteration 83, loss = 0.00212268\n",
      "Iteration 84, loss = 0.00212877\n",
      "Iteration 85, loss = 0.00200874\n",
      "Iteration 86, loss = 0.00194632\n",
      "Iteration 87, loss = 0.00190685\n",
      "Iteration 88, loss = 0.00181995\n",
      "Iteration 89, loss = 0.00175161\n",
      "Iteration 90, loss = 0.00168289\n",
      "Iteration 91, loss = 0.00160995\n",
      "Iteration 92, loss = 0.00157112\n",
      "Iteration 93, loss = 0.00150932\n",
      "Iteration 94, loss = 0.00147069\n",
      "Iteration 95, loss = 0.00141569\n",
      "Iteration 96, loss = 0.00135367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.80219697\n",
      "Iteration 2, loss = 0.58256352\n",
      "Iteration 3, loss = 0.32323981\n",
      "Iteration 4, loss = 0.15633313\n",
      "Iteration 5, loss = 0.09029925\n",
      "Iteration 6, loss = 0.06525118\n",
      "Iteration 7, loss = 0.05302630\n",
      "Iteration 8, loss = 0.04568579\n",
      "Iteration 9, loss = 0.04056349\n",
      "Iteration 10, loss = 0.03695807\n",
      "Iteration 11, loss = 0.03389834\n",
      "Iteration 12, loss = 0.03146994\n",
      "Iteration 13, loss = 0.02938953\n",
      "Iteration 14, loss = 0.02755601\n",
      "Iteration 15, loss = 0.02602097\n",
      "Iteration 16, loss = 0.02475830\n",
      "Iteration 17, loss = 0.02370727\n",
      "Iteration 18, loss = 0.02246124\n",
      "Iteration 19, loss = 0.02150086\n",
      "Iteration 20, loss = 0.02104313\n",
      "Iteration 21, loss = 0.01976152\n",
      "Iteration 22, loss = 0.01924662\n",
      "Iteration 23, loss = 0.01840882\n",
      "Iteration 24, loss = 0.01751602\n",
      "Iteration 25, loss = 0.01701168\n",
      "Iteration 26, loss = 0.01629483\n",
      "Iteration 27, loss = 0.01566754\n",
      "Iteration 28, loss = 0.01535026\n",
      "Iteration 29, loss = 0.01448041\n",
      "Iteration 30, loss = 0.01395932\n",
      "Iteration 31, loss = 0.01332976\n",
      "Iteration 32, loss = 0.01278561\n",
      "Iteration 33, loss = 0.01219285\n",
      "Iteration 34, loss = 0.01194801\n",
      "Iteration 35, loss = 0.01154385\n",
      "Iteration 36, loss = 0.01076798\n",
      "Iteration 37, loss = 0.01058265\n",
      "Iteration 38, loss = 0.00999906\n",
      "Iteration 39, loss = 0.00975280\n",
      "Iteration 40, loss = 0.00928608\n",
      "Iteration 41, loss = 0.00885181\n",
      "Iteration 42, loss = 0.00855966\n",
      "Iteration 43, loss = 0.00831552\n",
      "Iteration 44, loss = 0.00773614\n",
      "Iteration 45, loss = 0.00773014\n",
      "Iteration 46, loss = 0.00718987\n",
      "Iteration 47, loss = 0.00674507\n",
      "Iteration 48, loss = 0.00654507\n",
      "Iteration 49, loss = 0.00634112\n",
      "Iteration 50, loss = 0.00610010\n",
      "Iteration 51, loss = 0.00564888\n",
      "Iteration 52, loss = 0.00536158\n",
      "Iteration 53, loss = 0.00510300\n",
      "Iteration 54, loss = 0.00492777\n",
      "Iteration 55, loss = 0.00457439\n",
      "Iteration 56, loss = 0.00436928\n",
      "Iteration 57, loss = 0.00412987\n",
      "Iteration 58, loss = 0.00394909\n",
      "Iteration 59, loss = 0.00401101\n",
      "Iteration 60, loss = 0.00357032\n",
      "Iteration 61, loss = 0.00338797\n",
      "Iteration 62, loss = 0.00315125\n",
      "Iteration 63, loss = 0.00300482\n",
      "Iteration 64, loss = 0.00287508\n",
      "Iteration 65, loss = 0.00267605\n",
      "Iteration 66, loss = 0.00251314\n",
      "Iteration 67, loss = 0.00239642\n",
      "Iteration 68, loss = 0.00225229\n",
      "Iteration 69, loss = 0.00215505\n",
      "Iteration 70, loss = 0.00203294\n",
      "Iteration 71, loss = 0.00195895\n",
      "Iteration 72, loss = 0.00183985\n",
      "Iteration 73, loss = 0.00174179\n",
      "Iteration 74, loss = 0.00166350\n",
      "Iteration 75, loss = 0.00175997\n",
      "Iteration 76, loss = 0.00152809\n",
      "Iteration 77, loss = 0.00145714\n",
      "Iteration 78, loss = 0.00134305\n",
      "Iteration 79, loss = 0.00130365\n",
      "Iteration 80, loss = 0.00121542\n",
      "Iteration 81, loss = 0.00113595\n",
      "Iteration 82, loss = 0.00109467\n",
      "Iteration 83, loss = 0.00103161\n",
      "Iteration 84, loss = 0.00097498\n",
      "Iteration 85, loss = 0.00091057\n",
      "Iteration 86, loss = 0.00086169\n",
      "Iteration 87, loss = 0.00081732\n",
      "Iteration 88, loss = 0.00077857\n",
      "Iteration 89, loss = 0.00073468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68818591\n",
      "Iteration 2, loss = 0.67948226\n",
      "Iteration 3, loss = 0.67204320\n",
      "Iteration 4, loss = 0.66560953\n",
      "Iteration 5, loss = 0.66018159\n",
      "Iteration 6, loss = 0.65561197\n",
      "Iteration 7, loss = 0.65177790\n",
      "Iteration 8, loss = 0.64861672\n",
      "Iteration 9, loss = 0.64600920\n",
      "Iteration 10, loss = 0.64390946\n",
      "Iteration 11, loss = 0.64217971\n",
      "Iteration 12, loss = 0.64084856\n",
      "Iteration 13, loss = 0.63977426\n",
      "Iteration 14, loss = 0.63894101\n",
      "Iteration 15, loss = 0.63830774\n",
      "Iteration 16, loss = 0.63782457\n",
      "Iteration 17, loss = 0.63745885\n",
      "Iteration 18, loss = 0.63719266\n",
      "Iteration 19, loss = 0.63699253\n",
      "Iteration 20, loss = 0.63684798\n",
      "Iteration 21, loss = 0.63674992\n",
      "Iteration 22, loss = 0.63667118\n",
      "Iteration 23, loss = 0.63662377\n",
      "Iteration 24, loss = 0.63658527\n",
      "Iteration 25, loss = 0.63657039\n",
      "Iteration 26, loss = 0.63655008\n",
      "Iteration 27, loss = 0.63654462\n",
      "Iteration 28, loss = 0.63653367\n",
      "Iteration 29, loss = 0.63652764\n",
      "Iteration 30, loss = 0.63652256\n",
      "Iteration 31, loss = 0.63652163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62630159\n",
      "Iteration 2, loss = 0.30457607\n",
      "Iteration 3, loss = 0.19285883\n",
      "Iteration 4, loss = 0.16896704\n",
      "Iteration 5, loss = 0.15535416\n",
      "Iteration 6, loss = 0.14550443\n",
      "Iteration 7, loss = 0.13719521\n",
      "Iteration 8, loss = 0.13175148\n",
      "Iteration 9, loss = 0.12425018\n",
      "Iteration 10, loss = 0.11858334\n",
      "Iteration 11, loss = 0.11328398\n",
      "Iteration 12, loss = 0.10824339\n",
      "Iteration 13, loss = 0.10352088\n",
      "Iteration 14, loss = 0.09905869\n",
      "Iteration 15, loss = 0.09468707\n",
      "Iteration 16, loss = 0.09094646\n",
      "Iteration 17, loss = 0.08712052\n",
      "Iteration 18, loss = 0.08353721\n",
      "Iteration 19, loss = 0.08010094\n",
      "Iteration 20, loss = 0.08367129\n",
      "Iteration 21, loss = 0.07528623\n",
      "Iteration 22, loss = 0.07215742\n",
      "Iteration 23, loss = 0.06950949\n",
      "Iteration 24, loss = 0.06685379\n",
      "Iteration 25, loss = 0.06448636\n",
      "Iteration 26, loss = 0.06214627\n",
      "Iteration 27, loss = 0.06004033\n",
      "Iteration 28, loss = 0.05787301\n",
      "Iteration 29, loss = 0.05592866\n",
      "Iteration 30, loss = 0.05387718\n",
      "Iteration 31, loss = 0.05214587\n",
      "Iteration 32, loss = 0.05058822\n",
      "Iteration 33, loss = 0.04879768\n",
      "Iteration 34, loss = 0.04706226\n",
      "Iteration 35, loss = 0.04553102\n",
      "Iteration 36, loss = 0.04399337\n",
      "Iteration 37, loss = 0.04253997\n",
      "Iteration 38, loss = 0.04104531\n",
      "Iteration 39, loss = 0.03969574\n",
      "Iteration 40, loss = 0.03832867\n",
      "Iteration 41, loss = 0.03715866\n",
      "Iteration 42, loss = 0.03588712\n",
      "Iteration 43, loss = 0.03478830\n",
      "Iteration 44, loss = 0.03375045\n",
      "Iteration 45, loss = 0.03257965\n",
      "Iteration 46, loss = 0.03156673\n",
      "Iteration 47, loss = 0.03056282\n",
      "Iteration 48, loss = 0.02962801\n",
      "Iteration 49, loss = 0.02870122\n",
      "Iteration 50, loss = 0.02841219\n",
      "Iteration 51, loss = 0.02757857\n",
      "Iteration 52, loss = 0.02636328\n",
      "Iteration 53, loss = 0.02550240\n",
      "Iteration 54, loss = 0.02467674\n",
      "Iteration 55, loss = 0.02400013\n",
      "Iteration 56, loss = 0.02331553\n",
      "Iteration 57, loss = 0.02263558\n",
      "Iteration 58, loss = 0.02202890\n",
      "Iteration 59, loss = 0.02146818\n",
      "Iteration 60, loss = 0.02085246\n",
      "Iteration 61, loss = 0.02029399\n",
      "Iteration 62, loss = 0.01974906\n",
      "Iteration 63, loss = 0.01926592\n",
      "Iteration 64, loss = 0.01878254\n",
      "Iteration 65, loss = 0.01833413\n",
      "Iteration 66, loss = 0.01787236\n",
      "Iteration 67, loss = 0.01741800\n",
      "Iteration 68, loss = 0.01701470\n",
      "Iteration 69, loss = 0.01665780\n",
      "Iteration 70, loss = 0.01623509\n",
      "Iteration 71, loss = 0.01585497\n",
      "Iteration 72, loss = 0.01553341\n",
      "Iteration 73, loss = 0.01519935\n",
      "Iteration 74, loss = 0.01486930\n",
      "Iteration 75, loss = 0.01458443\n",
      "Iteration 76, loss = 0.01428199\n",
      "Iteration 77, loss = 0.01399027\n",
      "Iteration 78, loss = 0.01371649\n",
      "Iteration 79, loss = 0.01345329\n",
      "Iteration 80, loss = 0.01320437\n",
      "Iteration 81, loss = 0.01295510\n",
      "Iteration 82, loss = 0.01274445\n",
      "Iteration 83, loss = 0.01250317\n",
      "Iteration 84, loss = 0.01228101\n",
      "Iteration 85, loss = 0.01204554\n",
      "Iteration 86, loss = 0.01186053\n",
      "Iteration 87, loss = 0.01165402\n",
      "Iteration 88, loss = 0.01145667\n",
      "Iteration 89, loss = 0.01127906\n",
      "Iteration 90, loss = 0.01109725\n",
      "Iteration 91, loss = 0.01092481\n",
      "Iteration 92, loss = 0.01075485\n",
      "Iteration 93, loss = 0.01060969\n",
      "Iteration 94, loss = 0.01044528\n",
      "Iteration 95, loss = 0.01029544\n",
      "Iteration 96, loss = 0.01014722\n",
      "Iteration 97, loss = 0.01002068\n",
      "Iteration 98, loss = 0.00987147\n",
      "Iteration 99, loss = 0.00973739\n",
      "Iteration 100, loss = 0.00961359\n",
      "Iteration 101, loss = 0.00949526\n",
      "Iteration 102, loss = 0.00936907\n",
      "Iteration 103, loss = 0.00925868\n",
      "Iteration 104, loss = 0.00915418\n",
      "Iteration 105, loss = 0.00904422\n",
      "Iteration 106, loss = 0.00894358\n",
      "Iteration 107, loss = 0.00884601\n",
      "Iteration 108, loss = 0.00875209\n",
      "Iteration 109, loss = 0.00865788\n",
      "Iteration 110, loss = 0.00856451\n",
      "Iteration 111, loss = 0.00848475\n",
      "Iteration 112, loss = 0.00871960\n",
      "Iteration 113, loss = 0.00842339\n",
      "Iteration 114, loss = 0.00826249\n",
      "Iteration 115, loss = 0.00817149\n",
      "Iteration 116, loss = 0.00809573\n",
      "Iteration 117, loss = 0.00802765\n",
      "Iteration 118, loss = 0.00796373\n",
      "Iteration 119, loss = 0.00789354\n",
      "Iteration 120, loss = 0.00782927\n",
      "Iteration 121, loss = 0.00776788\n",
      "Iteration 122, loss = 0.00771152\n",
      "Iteration 123, loss = 0.00765535\n",
      "Iteration 124, loss = 0.00760582\n",
      "Iteration 125, loss = 0.00755456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75100452\n",
      "Iteration 2, loss = 0.68100729\n",
      "Iteration 3, loss = 0.61458038\n",
      "Iteration 4, loss = 0.55921686\n",
      "Iteration 5, loss = 0.51754221\n",
      "Iteration 6, loss = 0.48217337\n",
      "Iteration 7, loss = 0.45665860\n",
      "Iteration 8, loss = 0.43506302\n",
      "Iteration 9, loss = 0.41528317\n",
      "Iteration 10, loss = 0.39670227\n",
      "Iteration 11, loss = 0.37925560\n",
      "Iteration 12, loss = 0.36307891\n",
      "Iteration 13, loss = 0.34769892\n",
      "Iteration 14, loss = 0.33366750\n",
      "Iteration 15, loss = 0.31990806\n",
      "Iteration 16, loss = 0.30684862\n",
      "Iteration 17, loss = 0.29445010\n",
      "Iteration 18, loss = 0.28304535\n",
      "Iteration 19, loss = 0.27201340\n",
      "Iteration 20, loss = 0.26135935\n",
      "Iteration 21, loss = 0.25123781\n",
      "Iteration 22, loss = 0.24156737\n",
      "Iteration 23, loss = 0.23266244\n",
      "Iteration 24, loss = 0.22434663\n",
      "Iteration 25, loss = 0.21576470\n",
      "Iteration 26, loss = 0.20800514\n",
      "Iteration 27, loss = 0.20054906\n",
      "Iteration 28, loss = 0.19350899\n",
      "Iteration 29, loss = 0.18685660\n",
      "Iteration 30, loss = 0.18047729\n",
      "Iteration 31, loss = 0.17436311\n",
      "Iteration 32, loss = 0.16843096\n",
      "Iteration 33, loss = 0.16275404\n",
      "Iteration 34, loss = 0.15734403\n",
      "Iteration 35, loss = 0.15219141\n",
      "Iteration 36, loss = 0.14736411\n",
      "Iteration 37, loss = 0.14273751\n",
      "Iteration 38, loss = 0.13816822\n",
      "Iteration 39, loss = 0.13396711\n",
      "Iteration 40, loss = 0.12964437\n",
      "Iteration 41, loss = 0.12563193\n",
      "Iteration 42, loss = 0.12192613\n",
      "Iteration 43, loss = 0.11828745\n",
      "Iteration 44, loss = 0.11477976\n",
      "Iteration 45, loss = 0.11142966\n",
      "Iteration 46, loss = 0.10822835\n",
      "Iteration 47, loss = 0.10518340\n",
      "Iteration 48, loss = 0.10219911\n",
      "Iteration 49, loss = 0.09936629\n",
      "Iteration 50, loss = 0.09668371\n",
      "Iteration 51, loss = 0.09415473\n",
      "Iteration 52, loss = 0.09168268\n",
      "Iteration 53, loss = 0.08932915\n",
      "Iteration 54, loss = 0.08698541\n",
      "Iteration 55, loss = 0.08473484\n",
      "Iteration 56, loss = 0.08256872\n",
      "Iteration 57, loss = 0.08046941\n",
      "Iteration 58, loss = 0.07847246\n",
      "Iteration 59, loss = 0.07655844\n",
      "Iteration 60, loss = 0.07472167\n",
      "Iteration 61, loss = 0.07295795\n",
      "Iteration 62, loss = 0.07121235\n",
      "Iteration 63, loss = 0.06956579\n",
      "Iteration 64, loss = 0.06795821\n",
      "Iteration 65, loss = 0.06641661\n",
      "Iteration 66, loss = 0.06492312\n",
      "Iteration 67, loss = 0.06348608\n",
      "Iteration 68, loss = 0.06210441\n",
      "Iteration 69, loss = 0.06077303\n",
      "Iteration 70, loss = 0.05948767\n",
      "Iteration 71, loss = 0.05826992\n",
      "Iteration 72, loss = 0.05713185\n",
      "Iteration 73, loss = 0.05595400\n",
      "Iteration 74, loss = 0.05484702\n",
      "Iteration 75, loss = 0.05378180\n",
      "Iteration 76, loss = 0.05275795\n",
      "Iteration 77, loss = 0.05175213\n",
      "Iteration 78, loss = 0.05079487\n",
      "Iteration 79, loss = 0.04988310\n",
      "Iteration 80, loss = 0.04900419\n",
      "Iteration 81, loss = 0.04812394\n",
      "Iteration 82, loss = 0.04726775\n",
      "Iteration 83, loss = 0.04643879\n",
      "Iteration 84, loss = 0.04566776\n",
      "Iteration 85, loss = 0.04490326\n",
      "Iteration 86, loss = 0.04417202\n",
      "Iteration 87, loss = 0.04346106\n",
      "Iteration 88, loss = 0.04279146\n",
      "Iteration 89, loss = 0.04213789\n",
      "Iteration 90, loss = 0.04149684\n",
      "Iteration 91, loss = 0.04087458\n",
      "Iteration 92, loss = 0.04025835\n",
      "Iteration 93, loss = 0.03966623\n",
      "Iteration 94, loss = 0.03910287\n",
      "Iteration 95, loss = 0.03857464\n",
      "Iteration 96, loss = 0.03806636\n",
      "Iteration 97, loss = 0.03755397\n",
      "Iteration 98, loss = 0.03707346\n",
      "Iteration 99, loss = 0.03659616\n",
      "Iteration 100, loss = 0.03613535\n",
      "Iteration 101, loss = 0.03569367\n",
      "Iteration 102, loss = 0.03527402\n",
      "Iteration 103, loss = 0.03485946\n",
      "Iteration 104, loss = 0.03446262\n",
      "Iteration 105, loss = 0.03408278\n",
      "Iteration 106, loss = 0.03371450\n",
      "Iteration 107, loss = 0.03338467\n",
      "Iteration 108, loss = 0.03302701\n",
      "Iteration 109, loss = 0.03270645\n",
      "Iteration 110, loss = 0.03237602\n",
      "Iteration 111, loss = 0.03206436\n",
      "Iteration 112, loss = 0.03176352\n",
      "Iteration 113, loss = 0.03146791\n",
      "Iteration 114, loss = 0.03118412\n",
      "Iteration 115, loss = 0.03091117\n",
      "Iteration 116, loss = 0.03065280\n",
      "Iteration 117, loss = 0.03040336\n",
      "Iteration 118, loss = 0.03015785\n",
      "Iteration 119, loss = 0.02992464\n",
      "Iteration 120, loss = 0.02970322\n",
      "Iteration 121, loss = 0.02948490\n",
      "Iteration 122, loss = 0.02927697\n",
      "Iteration 123, loss = 0.02907917\n",
      "Iteration 124, loss = 0.02888743\n",
      "Iteration 125, loss = 0.02870143\n",
      "Iteration 126, loss = 0.02853145\n",
      "Iteration 127, loss = 0.02836953\n",
      "Iteration 128, loss = 0.02820568\n",
      "Iteration 129, loss = 0.02804271\n",
      "Iteration 130, loss = 0.02789194\n",
      "Iteration 131, loss = 0.02774082\n",
      "Iteration 132, loss = 0.02760406\n",
      "Iteration 133, loss = 0.03191002\n",
      "Iteration 134, loss = 0.02969006\n",
      "Iteration 135, loss = 0.02827877\n",
      "Iteration 136, loss = 0.02726279\n",
      "Iteration 137, loss = 0.02704107\n",
      "Iteration 138, loss = 0.02690952\n",
      "Iteration 139, loss = 0.02679765\n",
      "Iteration 140, loss = 0.02669530\n",
      "Iteration 141, loss = 0.02659570\n",
      "Iteration 142, loss = 0.02650659\n",
      "Iteration 143, loss = 0.02641629\n",
      "Iteration 144, loss = 0.02633221\n",
      "Iteration 145, loss = 0.02625229\n",
      "Iteration 146, loss = 0.02617850\n",
      "Iteration 147, loss = 0.02610558\n",
      "Iteration 148, loss = 0.02603897\n",
      "Iteration 149, loss = 0.02597589\n",
      "Iteration 150, loss = 0.02591326\n",
      "Iteration 151, loss = 0.02585294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55944890\n",
      "Iteration 2, loss = 0.34102605\n",
      "Iteration 3, loss = 0.26641573\n",
      "Iteration 4, loss = 0.23516010\n",
      "Iteration 5, loss = 0.21936293\n",
      "Iteration 6, loss = 0.20467311\n",
      "Iteration 7, loss = 0.19308442\n",
      "Iteration 8, loss = 0.18292673\n",
      "Iteration 9, loss = 0.17380081\n",
      "Iteration 10, loss = 0.17073244\n",
      "Iteration 11, loss = 0.16183376\n",
      "Iteration 12, loss = 0.15639048\n",
      "Iteration 13, loss = 0.15027914\n",
      "Iteration 14, loss = 0.14491937\n",
      "Iteration 15, loss = 0.13968518\n",
      "Iteration 16, loss = 0.13472854\n",
      "Iteration 17, loss = 0.12999283\n",
      "Iteration 18, loss = 0.12520344\n",
      "Iteration 19, loss = 0.12084704\n",
      "Iteration 20, loss = 0.11656010\n",
      "Iteration 21, loss = 0.11250801\n",
      "Iteration 22, loss = 0.10854105\n",
      "Iteration 23, loss = 0.10524996\n",
      "Iteration 24, loss = 0.10112247\n",
      "Iteration 25, loss = 0.09732470\n",
      "Iteration 26, loss = 0.09367966\n",
      "Iteration 27, loss = 0.09029568\n",
      "Iteration 28, loss = 0.08688635\n",
      "Iteration 29, loss = 0.08370007\n",
      "Iteration 30, loss = 0.08083546\n",
      "Iteration 31, loss = 0.07772046\n",
      "Iteration 32, loss = 0.07475562\n",
      "Iteration 33, loss = 0.07226785\n",
      "Iteration 34, loss = 0.06942285\n",
      "Iteration 35, loss = 0.06698327\n",
      "Iteration 36, loss = 0.06458722\n",
      "Iteration 37, loss = 0.06236648\n",
      "Iteration 38, loss = 0.06036640\n",
      "Iteration 39, loss = 0.05818849\n",
      "Iteration 40, loss = 0.05623402\n",
      "Iteration 41, loss = 0.05423849\n",
      "Iteration 42, loss = 0.05232056\n",
      "Iteration 43, loss = 0.05057692\n",
      "Iteration 44, loss = 0.04886977\n",
      "Iteration 45, loss = 0.04730096\n",
      "Iteration 46, loss = 0.04585728\n",
      "Iteration 47, loss = 0.04424119\n",
      "Iteration 48, loss = 0.04271660\n",
      "Iteration 49, loss = 0.04136293\n",
      "Iteration 50, loss = 0.04012904\n",
      "Iteration 51, loss = 0.03892200\n",
      "Iteration 52, loss = 0.03762627\n",
      "Iteration 53, loss = 0.03646864\n",
      "Iteration 54, loss = 0.03526863\n",
      "Iteration 55, loss = 0.03427288\n",
      "Iteration 56, loss = 0.03316779\n",
      "Iteration 57, loss = 0.03217805\n",
      "Iteration 58, loss = 0.03108759\n",
      "Iteration 59, loss = 0.03012035\n",
      "Iteration 60, loss = 0.02930327\n",
      "Iteration 61, loss = 0.02849511\n",
      "Iteration 62, loss = 0.02761431\n",
      "Iteration 63, loss = 0.02681358\n",
      "Iteration 64, loss = 0.02600914\n",
      "Iteration 65, loss = 0.02526729\n",
      "Iteration 66, loss = 0.02465799\n",
      "Iteration 67, loss = 0.02381561\n",
      "Iteration 68, loss = 0.02317498\n",
      "Iteration 69, loss = 0.02245266\n",
      "Iteration 70, loss = 0.02180087\n",
      "Iteration 71, loss = 0.02117516\n",
      "Iteration 72, loss = 0.02051859\n",
      "Iteration 73, loss = 0.01990813\n",
      "Iteration 74, loss = 0.01926653\n",
      "Iteration 75, loss = 0.01870808\n",
      "Iteration 76, loss = 0.01808754\n",
      "Iteration 77, loss = 0.01752413\n",
      "Iteration 78, loss = 0.01705517\n",
      "Iteration 79, loss = 0.01657413\n",
      "Iteration 80, loss = 0.01605494\n",
      "Iteration 81, loss = 0.01562664\n",
      "Iteration 82, loss = 0.02328487\n",
      "Iteration 83, loss = 0.01591748\n",
      "Iteration 84, loss = 0.01503857\n",
      "Iteration 85, loss = 0.01448528\n",
      "Iteration 86, loss = 0.01408455\n",
      "Iteration 87, loss = 0.01370334\n",
      "Iteration 88, loss = 0.01335515\n",
      "Iteration 89, loss = 0.01302462\n",
      "Iteration 90, loss = 0.01269794\n",
      "Iteration 91, loss = 0.01747021\n",
      "Iteration 92, loss = 0.01276592\n",
      "Iteration 93, loss = 0.01211824\n",
      "Iteration 94, loss = 0.01175428\n",
      "Iteration 95, loss = 0.01146324\n",
      "Iteration 96, loss = 0.01120965\n",
      "Iteration 97, loss = 0.01092161\n",
      "Iteration 98, loss = 0.01065984\n",
      "Iteration 99, loss = 0.01041557\n",
      "Iteration 100, loss = 0.01017114\n",
      "Iteration 101, loss = 0.00996530\n",
      "Iteration 102, loss = 0.00975511\n",
      "Iteration 103, loss = 0.00956451\n",
      "Iteration 104, loss = 0.00937711\n",
      "Iteration 105, loss = 0.00919514\n",
      "Iteration 106, loss = 0.00901579\n",
      "Iteration 107, loss = 0.00884481\n",
      "Iteration 108, loss = 0.00867563\n",
      "Iteration 109, loss = 0.00852013\n",
      "Iteration 110, loss = 0.00837138\n",
      "Iteration 111, loss = 0.00821813\n",
      "Iteration 112, loss = 0.00806804\n",
      "Iteration 113, loss = 0.00792379\n",
      "Iteration 114, loss = 0.00779802\n",
      "Iteration 115, loss = 0.00765451\n",
      "Iteration 116, loss = 0.00752835\n",
      "Iteration 117, loss = 0.00740839\n",
      "Iteration 118, loss = 0.00729128\n",
      "Iteration 119, loss = 0.00717785\n",
      "Iteration 120, loss = 0.00706844\n",
      "Iteration 121, loss = 0.00695994\n",
      "Iteration 122, loss = 0.00685177\n",
      "Iteration 123, loss = 0.00675346\n",
      "Iteration 124, loss = 0.00665700\n",
      "Iteration 125, loss = 0.00656337\n",
      "Iteration 126, loss = 0.00646304\n",
      "Iteration 127, loss = 0.00637743\n",
      "Iteration 128, loss = 0.00628240\n",
      "Iteration 129, loss = 0.00619946\n",
      "Iteration 130, loss = 0.00611681\n",
      "Iteration 131, loss = 0.00603709\n",
      "Iteration 132, loss = 0.00596028\n",
      "Iteration 133, loss = 0.00588318\n",
      "Iteration 134, loss = 0.00580833\n",
      "Iteration 135, loss = 0.00574094\n",
      "Iteration 136, loss = 0.00567463\n",
      "Iteration 137, loss = 0.00560817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72731778\n",
      "Iteration 2, loss = 0.71523453\n",
      "Iteration 3, loss = 0.70524031\n",
      "Iteration 4, loss = 0.69573686\n",
      "Iteration 5, loss = 0.68722309\n",
      "Iteration 6, loss = 0.68034505\n",
      "Iteration 7, loss = 0.67435135\n",
      "Iteration 8, loss = 0.66912417\n",
      "Iteration 9, loss = 0.66449585\n",
      "Iteration 10, loss = 0.66051884\n",
      "Iteration 11, loss = 0.65652253\n",
      "Iteration 12, loss = 0.65307186\n",
      "Iteration 13, loss = 0.65009993\n",
      "Iteration 14, loss = 0.64798509\n",
      "Iteration 15, loss = 0.64587044\n",
      "Iteration 16, loss = 0.64404366\n",
      "Iteration 17, loss = 0.64251035\n",
      "Iteration 18, loss = 0.64154779\n",
      "Iteration 19, loss = 0.64052049\n",
      "Iteration 20, loss = 0.63961834\n",
      "Iteration 21, loss = 0.63913229\n",
      "Iteration 22, loss = 0.63854310\n",
      "Iteration 23, loss = 0.63805093\n",
      "Iteration 24, loss = 0.63766189\n",
      "Iteration 25, loss = 0.63735970\n",
      "Iteration 26, loss = 0.63712166\n",
      "Iteration 27, loss = 0.63705052\n",
      "Iteration 28, loss = 0.63702688\n",
      "Iteration 29, loss = 0.63688566\n",
      "Iteration 30, loss = 0.63677380\n",
      "Iteration 31, loss = 0.63669038\n",
      "Iteration 32, loss = 0.63669964\n",
      "Iteration 33, loss = 0.63664733\n",
      "Iteration 34, loss = 0.63661172\n",
      "Iteration 35, loss = 0.63658191\n",
      "Iteration 36, loss = 0.63656760\n",
      "Iteration 37, loss = 0.63656259\n",
      "Iteration 38, loss = 0.63655965\n",
      "Iteration 39, loss = 0.63656773\n",
      "Iteration 40, loss = 0.63658128\n",
      "Iteration 41, loss = 0.63655804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.90254962\n",
      "Iteration 2, loss = 0.84869371\n",
      "Iteration 3, loss = 0.80978341\n",
      "Iteration 4, loss = 0.77660585\n",
      "Iteration 5, loss = 0.74359770\n",
      "Iteration 6, loss = 0.71124922\n",
      "Iteration 7, loss = 0.68218880\n",
      "Iteration 8, loss = 0.65787092\n",
      "Iteration 9, loss = 0.63701320\n",
      "Iteration 10, loss = 0.61352938\n",
      "Iteration 11, loss = 0.58217894\n",
      "Iteration 12, loss = 0.54393203\n",
      "Iteration 13, loss = 0.50414455\n",
      "Iteration 14, loss = 0.46707092\n",
      "Iteration 15, loss = 0.43404950\n",
      "Iteration 16, loss = 0.40508972\n",
      "Iteration 17, loss = 0.37978342\n",
      "Iteration 18, loss = 0.35766416\n",
      "Iteration 19, loss = 0.33825514\n",
      "Iteration 20, loss = 0.32094518\n",
      "Iteration 21, loss = 0.30546860\n",
      "Iteration 22, loss = 0.29141956\n",
      "Iteration 23, loss = 0.27859024\n",
      "Iteration 24, loss = 0.26676730\n",
      "Iteration 25, loss = 0.25581536\n",
      "Iteration 26, loss = 0.24558352\n",
      "Iteration 27, loss = 0.23606873\n",
      "Iteration 28, loss = 0.22699517\n",
      "Iteration 29, loss = 0.21858521\n",
      "Iteration 30, loss = 0.21055611\n",
      "Iteration 31, loss = 0.20296612\n",
      "Iteration 32, loss = 0.19575248\n",
      "Iteration 33, loss = 0.18894503\n",
      "Iteration 34, loss = 0.18236852\n",
      "Iteration 35, loss = 0.17616222\n",
      "Iteration 36, loss = 0.17023083\n",
      "Iteration 37, loss = 0.16451371\n",
      "Iteration 38, loss = 0.15909450\n",
      "Iteration 39, loss = 0.15397856\n",
      "Iteration 40, loss = 0.14905392\n",
      "Iteration 41, loss = 0.14428325\n",
      "Iteration 42, loss = 0.13977146\n",
      "Iteration 43, loss = 0.13541631\n",
      "Iteration 44, loss = 0.13127261\n",
      "Iteration 45, loss = 0.12731766\n",
      "Iteration 46, loss = 0.12353763\n",
      "Iteration 47, loss = 0.11990166\n",
      "Iteration 48, loss = 0.11642491\n",
      "Iteration 49, loss = 0.11304302\n",
      "Iteration 50, loss = 0.10973833\n",
      "Iteration 51, loss = 0.10664513\n",
      "Iteration 52, loss = 0.10335685\n",
      "Iteration 53, loss = 0.10028150\n",
      "Iteration 54, loss = 0.09703528\n",
      "Iteration 55, loss = 0.09426489\n",
      "Iteration 56, loss = 0.09144607\n",
      "Iteration 57, loss = 0.08882825\n",
      "Iteration 58, loss = 0.08641128\n",
      "Iteration 59, loss = 0.08412892\n",
      "Iteration 60, loss = 0.08188720\n",
      "Iteration 61, loss = 0.07981541\n",
      "Iteration 62, loss = 0.07775950\n",
      "Iteration 63, loss = 0.07579618\n",
      "Iteration 64, loss = 0.07393016\n",
      "Iteration 65, loss = 0.07210449\n",
      "Iteration 66, loss = 0.07037993\n",
      "Iteration 67, loss = 0.06869335\n",
      "Iteration 68, loss = 0.06705679\n",
      "Iteration 69, loss = 0.06541808\n",
      "Iteration 70, loss = 0.06391021\n",
      "Iteration 71, loss = 0.06242777\n",
      "Iteration 72, loss = 0.06099607\n",
      "Iteration 73, loss = 0.05963588\n",
      "Iteration 74, loss = 0.05837322\n",
      "Iteration 75, loss = 0.05707355\n",
      "Iteration 76, loss = 0.05584899\n",
      "Iteration 77, loss = 0.05459191\n",
      "Iteration 78, loss = 0.05337532\n",
      "Iteration 79, loss = 0.05207054\n",
      "Iteration 80, loss = 0.05100586\n",
      "Iteration 81, loss = 0.04996202\n",
      "Iteration 82, loss = 0.04892809\n",
      "Iteration 83, loss = 0.04789070\n",
      "Iteration 84, loss = 0.04694466\n",
      "Iteration 85, loss = 0.04603355\n",
      "Iteration 86, loss = 0.04519696\n",
      "Iteration 87, loss = 0.04441836\n",
      "Iteration 88, loss = 0.04365584\n",
      "Iteration 89, loss = 0.04291896\n",
      "Iteration 90, loss = 0.04219537\n",
      "Iteration 91, loss = 0.04153444\n",
      "Iteration 92, loss = 0.04083969\n",
      "Iteration 93, loss = 0.04017121\n",
      "Iteration 94, loss = 0.03945681\n",
      "Iteration 95, loss = 0.03866640\n",
      "Iteration 96, loss = 0.03791267\n",
      "Iteration 97, loss = 0.03733229\n",
      "Iteration 98, loss = 0.03678401\n",
      "Iteration 99, loss = 0.03625407\n",
      "Iteration 100, loss = 0.03569894\n",
      "Iteration 101, loss = 0.03510288\n",
      "Iteration 102, loss = 0.03462057\n",
      "Iteration 103, loss = 0.03416346\n",
      "Iteration 104, loss = 0.03371729\n",
      "Iteration 105, loss = 0.03329178\n",
      "Iteration 106, loss = 0.03288696\n",
      "Iteration 107, loss = 0.03249606\n",
      "Iteration 108, loss = 0.03211782\n",
      "Iteration 109, loss = 0.03175307\n",
      "Iteration 110, loss = 0.03140166\n",
      "Iteration 111, loss = 0.03106635\n",
      "Iteration 112, loss = 0.03073637\n",
      "Iteration 113, loss = 0.03041934\n",
      "Iteration 114, loss = 0.03011440\n",
      "Iteration 115, loss = 0.02982032\n",
      "Iteration 116, loss = 0.02953366\n",
      "Iteration 117, loss = 0.02926287\n",
      "Iteration 118, loss = 0.02899163\n",
      "Iteration 119, loss = 0.02873078\n",
      "Iteration 120, loss = 0.02847384\n",
      "Iteration 121, loss = 0.02821545\n",
      "Iteration 122, loss = 0.02784940\n",
      "Iteration 123, loss = 0.02754875\n",
      "Iteration 124, loss = 0.02732724\n",
      "Iteration 125, loss = 0.02711254\n",
      "Iteration 126, loss = 0.02690552\n",
      "Iteration 127, loss = 0.02671077\n",
      "Iteration 128, loss = 0.02651558\n",
      "Iteration 129, loss = 0.02633295\n",
      "Iteration 130, loss = 0.02615420\n",
      "Iteration 131, loss = 0.02597962\n",
      "Iteration 132, loss = 0.02581202\n",
      "Iteration 133, loss = 0.02564298\n",
      "Iteration 134, loss = 0.02547007\n",
      "Iteration 135, loss = 0.02523706\n",
      "Iteration 136, loss = 0.02486437\n",
      "Iteration 137, loss = 0.02455090\n",
      "Iteration 138, loss = 0.02434480\n",
      "Iteration 139, loss = 0.02419320\n",
      "Iteration 140, loss = 0.02405625\n",
      "Iteration 141, loss = 0.02392982\n",
      "Iteration 142, loss = 0.02380898\n",
      "Iteration 143, loss = 0.02368957\n",
      "Iteration 144, loss = 0.02357532\n",
      "Iteration 145, loss = 0.02346284\n",
      "Iteration 146, loss = 0.02335853\n",
      "Iteration 147, loss = 0.02325318\n",
      "Iteration 148, loss = 0.02310527\n",
      "Iteration 149, loss = 0.02267323\n",
      "Iteration 150, loss = 0.02254670\n",
      "Iteration 151, loss = 0.02244949\n",
      "Iteration 152, loss = 0.02235741\n",
      "Iteration 153, loss = 0.02227599\n",
      "Iteration 154, loss = 0.02219036\n",
      "Iteration 155, loss = 0.02211493\n",
      "Iteration 156, loss = 0.02203770\n",
      "Iteration 157, loss = 0.02196532\n",
      "Iteration 158, loss = 0.02189304\n",
      "Iteration 159, loss = 0.02182117\n",
      "Iteration 160, loss = 0.02174824\n",
      "Iteration 161, loss = 0.02166359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63760830\n",
      "Iteration 2, loss = 0.63279776\n",
      "Iteration 3, loss = 0.62584314\n",
      "Iteration 4, loss = 0.61163068\n",
      "Iteration 5, loss = 0.58713667\n",
      "Iteration 6, loss = 0.55116724\n",
      "Iteration 7, loss = 0.50755193\n",
      "Iteration 8, loss = 0.46170807\n",
      "Iteration 9, loss = 0.41994428\n",
      "Iteration 10, loss = 0.38445433\n",
      "Iteration 11, loss = 0.35429864\n",
      "Iteration 12, loss = 0.32907977\n",
      "Iteration 13, loss = 0.30733450\n",
      "Iteration 14, loss = 0.28793354\n",
      "Iteration 15, loss = 0.27115754\n",
      "Iteration 16, loss = 0.25586148\n",
      "Iteration 17, loss = 0.24209653\n",
      "Iteration 18, loss = 0.22992065\n",
      "Iteration 19, loss = 0.21877450\n",
      "Iteration 20, loss = 0.20845433\n",
      "Iteration 21, loss = 0.19871794\n",
      "Iteration 22, loss = 0.18953429\n",
      "Iteration 23, loss = 0.18312277\n",
      "Iteration 24, loss = 0.17497177\n",
      "Iteration 25, loss = 0.16829070\n",
      "Iteration 26, loss = 0.16199947\n",
      "Iteration 27, loss = 0.15604350\n",
      "Iteration 28, loss = 0.15043726\n",
      "Iteration 29, loss = 0.14512861\n",
      "Iteration 30, loss = 0.14026716\n",
      "Iteration 31, loss = 0.13562392\n",
      "Iteration 32, loss = 0.13111324\n",
      "Iteration 33, loss = 0.12849505\n",
      "Iteration 34, loss = 0.12360326\n",
      "Iteration 35, loss = 0.11954346\n",
      "Iteration 36, loss = 0.11601144\n",
      "Iteration 37, loss = 0.11260305\n",
      "Iteration 38, loss = 0.10942483\n",
      "Iteration 39, loss = 0.10629062\n",
      "Iteration 40, loss = 0.10337624\n",
      "Iteration 41, loss = 0.10052449\n",
      "Iteration 42, loss = 0.09782168\n",
      "Iteration 43, loss = 0.09525822\n",
      "Iteration 44, loss = 0.09266636\n",
      "Iteration 45, loss = 0.09030276\n",
      "Iteration 46, loss = 0.08790484\n",
      "Iteration 47, loss = 0.08563626\n",
      "Iteration 48, loss = 0.08352687\n",
      "Iteration 49, loss = 0.08150059\n",
      "Iteration 50, loss = 0.07957824\n",
      "Iteration 51, loss = 0.07769448\n",
      "Iteration 52, loss = 0.07595262\n",
      "Iteration 53, loss = 0.07417913\n",
      "Iteration 54, loss = 0.07248348\n",
      "Iteration 55, loss = 0.07087047\n",
      "Iteration 56, loss = 0.06935122\n",
      "Iteration 57, loss = 0.06781884\n",
      "Iteration 58, loss = 0.06629308\n",
      "Iteration 59, loss = 0.06481828\n",
      "Iteration 60, loss = 0.06340914\n",
      "Iteration 61, loss = 0.06209761\n",
      "Iteration 62, loss = 0.06079285\n",
      "Iteration 63, loss = 0.05957348\n",
      "Iteration 64, loss = 0.05835955\n",
      "Iteration 65, loss = 0.05718322\n",
      "Iteration 66, loss = 0.05613653\n",
      "Iteration 67, loss = 0.05506676\n",
      "Iteration 68, loss = 0.05399952\n",
      "Iteration 69, loss = 0.05281135\n",
      "Iteration 70, loss = 0.05183917\n",
      "Iteration 71, loss = 0.05084978\n",
      "Iteration 72, loss = 0.04988406\n",
      "Iteration 73, loss = 0.04887924\n",
      "Iteration 74, loss = 0.04792386\n",
      "Iteration 75, loss = 0.04703850\n",
      "Iteration 76, loss = 0.04609854\n",
      "Iteration 77, loss = 0.04519019\n",
      "Iteration 78, loss = 0.04435893\n",
      "Iteration 79, loss = 0.04355066\n",
      "Iteration 80, loss = 0.04262842\n",
      "Iteration 81, loss = 0.04179619\n",
      "Iteration 82, loss = 0.04092149\n",
      "Iteration 83, loss = 0.04026219\n",
      "Iteration 84, loss = 0.03947247\n",
      "Iteration 85, loss = 0.03872394\n",
      "Iteration 86, loss = 0.03803335\n",
      "Iteration 87, loss = 0.03733257\n",
      "Iteration 88, loss = 0.03671740\n",
      "Iteration 89, loss = 0.03593993\n",
      "Iteration 90, loss = 0.04031125\n",
      "Iteration 91, loss = 0.03590251\n",
      "Iteration 92, loss = 0.03506469\n",
      "Iteration 93, loss = 0.03445137\n",
      "Iteration 94, loss = 0.03394495\n",
      "Iteration 95, loss = 0.03343512\n",
      "Iteration 96, loss = 0.03297276\n",
      "Iteration 97, loss = 0.03255555\n",
      "Iteration 98, loss = 0.03215811\n",
      "Iteration 99, loss = 0.03171940\n",
      "Iteration 100, loss = 0.03131476\n",
      "Iteration 101, loss = 0.03092637\n",
      "Iteration 102, loss = 0.03059766\n",
      "Iteration 103, loss = 0.03012750\n",
      "Iteration 104, loss = 0.02975084\n",
      "Iteration 105, loss = 0.02936705\n",
      "Iteration 106, loss = 0.02900220\n",
      "Iteration 107, loss = 0.02859482\n",
      "Iteration 108, loss = 0.02831447\n",
      "Iteration 109, loss = 0.02790967\n",
      "Iteration 110, loss = 0.02758032\n",
      "Iteration 111, loss = 0.02730281\n",
      "Iteration 112, loss = 0.02703859\n",
      "Iteration 113, loss = 0.02676701\n",
      "Iteration 114, loss = 0.02651901\n",
      "Iteration 115, loss = 0.02624964\n",
      "Iteration 116, loss = 0.02599472\n",
      "Iteration 117, loss = 0.02586483\n",
      "Iteration 118, loss = 0.02553037\n",
      "Iteration 119, loss = 0.02530540\n",
      "Iteration 120, loss = 0.02505963\n",
      "Iteration 121, loss = 0.02483125\n",
      "Iteration 122, loss = 0.02462671\n",
      "Iteration 123, loss = 0.02439307\n",
      "Iteration 124, loss = 0.02420186\n",
      "Iteration 125, loss = 0.02400441\n",
      "Iteration 126, loss = 0.02378907\n",
      "Iteration 127, loss = 0.02361154\n",
      "Iteration 128, loss = 0.02342058\n",
      "Iteration 129, loss = 0.02321244\n",
      "Iteration 130, loss = 0.02305428\n",
      "Iteration 131, loss = 0.02287648\n",
      "Iteration 132, loss = 0.02272809\n",
      "Iteration 133, loss = 0.02255763\n",
      "Iteration 134, loss = 0.02239478\n",
      "Iteration 135, loss = 0.02220822\n",
      "Iteration 136, loss = 0.02205531\n",
      "Iteration 137, loss = 0.02181434\n",
      "Iteration 138, loss = 0.02153538\n",
      "Iteration 139, loss = 0.02134521\n",
      "Iteration 140, loss = 0.02122518\n",
      "Iteration 141, loss = 0.02102262\n",
      "Iteration 142, loss = 0.02086558\n",
      "Iteration 143, loss = 0.02070583\n",
      "Iteration 144, loss = 0.02058271\n",
      "Iteration 145, loss = 0.02040294\n",
      "Iteration 146, loss = 0.02022825\n",
      "Iteration 147, loss = 0.02008732\n",
      "Iteration 148, loss = 0.01991958\n",
      "Iteration 149, loss = 0.01982962\n",
      "Iteration 150, loss = 0.01969909\n",
      "Iteration 151, loss = 0.01959607\n",
      "Iteration 152, loss = 0.01945228\n",
      "Iteration 153, loss = 0.01939601\n",
      "Iteration 154, loss = 0.01933382\n",
      "Iteration 155, loss = 0.01918615\n",
      "Iteration 156, loss = 0.01910641\n",
      "Iteration 157, loss = 0.01905531\n",
      "Iteration 158, loss = 0.01893605\n",
      "Iteration 159, loss = 0.01887299\n",
      "Iteration 160, loss = 0.01877662\n",
      "Iteration 161, loss = 0.01867169\n",
      "Iteration 162, loss = 0.01855245\n",
      "Iteration 163, loss = 0.01841350\n",
      "Iteration 164, loss = 0.01825253\n",
      "Iteration 165, loss = 0.01807408\n",
      "Iteration 166, loss = 0.01800228\n",
      "Iteration 167, loss = 0.01783980\n",
      "Iteration 168, loss = 0.01770623\n",
      "Iteration 169, loss = 0.01766742\n",
      "Iteration 170, loss = 0.01746123\n",
      "Iteration 171, loss = 0.01738132\n",
      "Iteration 172, loss = 0.01730516\n",
      "Iteration 173, loss = 0.01724481\n",
      "Iteration 174, loss = 0.01717544\n",
      "Iteration 175, loss = 0.01711752\n",
      "Iteration 176, loss = 0.01709119\n",
      "Iteration 177, loss = 0.01705617\n",
      "Iteration 178, loss = 0.01698280\n",
      "Iteration 179, loss = 0.01694226\n",
      "Iteration 180, loss = 0.01690230\n",
      "Iteration 181, loss = 0.01686041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70812854\n",
      "Iteration 2, loss = 0.66366258\n",
      "Iteration 3, loss = 0.64054189\n",
      "Iteration 4, loss = 0.62515292\n",
      "Iteration 5, loss = 0.60940701\n",
      "Iteration 6, loss = 0.58912765\n",
      "Iteration 7, loss = 0.56000235\n",
      "Iteration 8, loss = 0.51803114\n",
      "Iteration 9, loss = 0.46544809\n",
      "Iteration 10, loss = 0.41259352\n",
      "Iteration 11, loss = 0.36466701\n",
      "Iteration 12, loss = 0.32461683\n",
      "Iteration 13, loss = 0.29181475\n",
      "Iteration 14, loss = 0.26502096\n",
      "Iteration 15, loss = 0.24244213\n",
      "Iteration 16, loss = 0.22347362\n",
      "Iteration 17, loss = 0.20707873\n",
      "Iteration 18, loss = 0.19275866\n",
      "Iteration 19, loss = 0.18023312\n",
      "Iteration 20, loss = 0.16907413\n",
      "Iteration 21, loss = 0.15912709\n",
      "Iteration 22, loss = 0.15024005\n",
      "Iteration 23, loss = 0.14215260\n",
      "Iteration 24, loss = 0.13487307\n",
      "Iteration 25, loss = 0.12830273\n",
      "Iteration 26, loss = 0.12222536\n",
      "Iteration 27, loss = 0.11658980\n",
      "Iteration 28, loss = 0.11152793\n",
      "Iteration 29, loss = 0.10671725\n",
      "Iteration 30, loss = 0.10242406\n",
      "Iteration 31, loss = 0.09829156\n",
      "Iteration 32, loss = 0.09442179\n",
      "Iteration 33, loss = 0.09093890\n",
      "Iteration 34, loss = 0.08758178\n",
      "Iteration 35, loss = 0.08436179\n",
      "Iteration 36, loss = 0.08144709\n",
      "Iteration 37, loss = 0.07864913\n",
      "Iteration 38, loss = 0.07604253\n",
      "Iteration 39, loss = 0.07359756\n",
      "Iteration 40, loss = 0.07126646\n",
      "Iteration 41, loss = 0.06915753\n",
      "Iteration 42, loss = 0.06704380\n",
      "Iteration 43, loss = 0.06517876\n",
      "Iteration 44, loss = 0.06326937\n",
      "Iteration 45, loss = 0.06152741\n",
      "Iteration 46, loss = 0.05987462\n",
      "Iteration 47, loss = 0.06072795\n",
      "Iteration 48, loss = 0.05727791\n",
      "Iteration 49, loss = 0.05595767\n",
      "Iteration 50, loss = 0.05470361\n",
      "Iteration 51, loss = 0.05350740\n",
      "Iteration 52, loss = 0.05233472\n",
      "Iteration 53, loss = 0.05123349\n",
      "Iteration 54, loss = 0.05018112\n",
      "Iteration 55, loss = 0.04911694\n",
      "Iteration 56, loss = 0.04811694\n",
      "Iteration 57, loss = 0.04722250\n",
      "Iteration 58, loss = 0.04627073\n",
      "Iteration 59, loss = 0.04534722\n",
      "Iteration 60, loss = 0.04451413\n",
      "Iteration 61, loss = 0.04370060\n",
      "Iteration 62, loss = 0.04287418\n",
      "Iteration 63, loss = 0.04220483\n",
      "Iteration 64, loss = 0.04150769\n",
      "Iteration 65, loss = 0.04082508\n",
      "Iteration 66, loss = 0.04015970\n",
      "Iteration 67, loss = 0.03955245\n",
      "Iteration 68, loss = 0.03895398\n",
      "Iteration 69, loss = 0.03843190\n",
      "Iteration 70, loss = 0.03783575\n",
      "Iteration 71, loss = 0.03729821\n",
      "Iteration 72, loss = 0.03671500\n",
      "Iteration 73, loss = 0.03615451\n",
      "Iteration 74, loss = 0.03562807\n",
      "Iteration 75, loss = 0.03504474\n",
      "Iteration 76, loss = 0.03455766\n",
      "Iteration 77, loss = 0.03393671\n",
      "Iteration 78, loss = 0.03342240\n",
      "Iteration 79, loss = 0.03291162\n",
      "Iteration 80, loss = 0.03231782\n",
      "Iteration 81, loss = 0.03185875\n",
      "Iteration 82, loss = 0.03127746\n",
      "Iteration 83, loss = 0.03069095\n",
      "Iteration 84, loss = 0.03018704\n",
      "Iteration 85, loss = 0.02973502\n",
      "Iteration 86, loss = 0.02931513\n",
      "Iteration 87, loss = 0.02892870\n",
      "Iteration 88, loss = 0.02858035\n",
      "Iteration 89, loss = 0.02818749\n",
      "Iteration 90, loss = 0.02779752\n",
      "Iteration 91, loss = 0.02716732\n",
      "Iteration 92, loss = 0.02600603\n",
      "Iteration 93, loss = 0.02518330\n",
      "Iteration 94, loss = 0.02478514\n",
      "Iteration 95, loss = 0.02441535\n",
      "Iteration 96, loss = 0.02394933\n",
      "Iteration 97, loss = 0.02365365\n",
      "Iteration 98, loss = 0.02336398\n",
      "Iteration 99, loss = 0.02320912\n",
      "Iteration 100, loss = 0.02287150\n",
      "Iteration 101, loss = 0.02250056\n",
      "Iteration 102, loss = 0.02214983\n",
      "Iteration 103, loss = 0.02179373\n",
      "Iteration 104, loss = 0.02154613\n",
      "Iteration 105, loss = 0.02118910\n",
      "Iteration 106, loss = 0.02087805\n",
      "Iteration 107, loss = 0.02050222\n",
      "Iteration 108, loss = 0.02021366\n",
      "Iteration 109, loss = 0.02001275\n",
      "Iteration 110, loss = 0.01978591\n",
      "Iteration 111, loss = 0.01954935\n",
      "Iteration 112, loss = 0.01935947\n",
      "Iteration 113, loss = 0.01921277\n",
      "Iteration 114, loss = 0.01901557\n",
      "Iteration 115, loss = 0.01884364\n",
      "Iteration 116, loss = 0.01868934\n",
      "Iteration 117, loss = 0.01861118\n",
      "Iteration 118, loss = 0.01845650\n",
      "Iteration 119, loss = 0.01834924\n",
      "Iteration 120, loss = 0.01824125\n",
      "Iteration 121, loss = 0.01814152\n",
      "Iteration 122, loss = 0.01805821\n",
      "Iteration 123, loss = 0.01791737\n",
      "Iteration 124, loss = 0.01774311\n",
      "Iteration 125, loss = 0.01767228\n",
      "Iteration 126, loss = 0.01744678\n",
      "Iteration 127, loss = 0.01724714\n",
      "Iteration 128, loss = 0.01715482\n",
      "Iteration 129, loss = 0.01690463\n",
      "Iteration 130, loss = 0.01676255\n",
      "Iteration 131, loss = 0.01662538\n",
      "Iteration 132, loss = 0.01651638\n",
      "Iteration 133, loss = 0.01647200\n",
      "Iteration 134, loss = 0.01639552\n",
      "Iteration 135, loss = 0.01631155\n",
      "Iteration 136, loss = 0.01629304\n",
      "Iteration 137, loss = 0.01619095\n",
      "Iteration 138, loss = 0.01616175\n",
      "Iteration 139, loss = 0.01611566\n",
      "Iteration 140, loss = 0.01606485\n",
      "Iteration 141, loss = 0.01603385\n",
      "Iteration 142, loss = 0.01599404\n",
      "Iteration 143, loss = 0.01596378\n",
      "Iteration 144, loss = 0.01593226\n",
      "Iteration 145, loss = 0.01590206\n",
      "Iteration 146, loss = 0.01585944\n",
      "Iteration 147, loss = 0.01584593\n",
      "Iteration 148, loss = 0.01581184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77831358\n",
      "Iteration 2, loss = 0.72519795\n",
      "Iteration 3, loss = 0.68447848\n",
      "Iteration 4, loss = 0.65899958\n",
      "Iteration 5, loss = 0.64046571\n",
      "Iteration 6, loss = 0.62659813\n",
      "Iteration 7, loss = 0.61356180\n",
      "Iteration 8, loss = 0.59816340\n",
      "Iteration 9, loss = 0.57686722\n",
      "Iteration 10, loss = 0.54810633\n",
      "Iteration 11, loss = 0.51259532\n",
      "Iteration 12, loss = 0.47253408\n",
      "Iteration 13, loss = 0.43171645\n",
      "Iteration 14, loss = 0.39289623\n",
      "Iteration 15, loss = 0.35702365\n",
      "Iteration 16, loss = 0.32534915\n",
      "Iteration 17, loss = 0.29750232\n",
      "Iteration 18, loss = 0.27324963\n",
      "Iteration 19, loss = 0.25346116\n",
      "Iteration 20, loss = 0.23539976\n",
      "Iteration 21, loss = 0.21948059\n",
      "Iteration 22, loss = 0.20545840\n",
      "Iteration 23, loss = 0.19285150\n",
      "Iteration 24, loss = 0.18138579\n",
      "Iteration 25, loss = 0.17103792\n",
      "Iteration 26, loss = 0.16172843\n",
      "Iteration 27, loss = 0.15340109\n",
      "Iteration 28, loss = 0.14568769\n",
      "Iteration 29, loss = 0.13870547\n",
      "Iteration 30, loss = 0.13219582\n",
      "Iteration 31, loss = 0.12624694\n",
      "Iteration 32, loss = 0.12069355\n",
      "Iteration 33, loss = 0.11551620\n",
      "Iteration 34, loss = 0.11072664\n",
      "Iteration 35, loss = 0.10628548\n",
      "Iteration 36, loss = 0.10196623\n",
      "Iteration 37, loss = 0.09802173\n",
      "Iteration 38, loss = 0.09424671\n",
      "Iteration 39, loss = 0.09064658\n",
      "Iteration 40, loss = 0.08716587\n",
      "Iteration 41, loss = 0.08389813\n",
      "Iteration 42, loss = 0.08092365\n",
      "Iteration 43, loss = 0.07811540\n",
      "Iteration 44, loss = 0.07556669\n",
      "Iteration 45, loss = 0.07303228\n",
      "Iteration 46, loss = 0.07071939\n",
      "Iteration 47, loss = 0.06856009\n",
      "Iteration 48, loss = 0.06654798\n",
      "Iteration 49, loss = 0.06462286\n",
      "Iteration 50, loss = 0.06276506\n",
      "Iteration 51, loss = 0.06099890\n",
      "Iteration 52, loss = 0.05923298\n",
      "Iteration 53, loss = 0.05769113\n",
      "Iteration 54, loss = 0.05613323\n",
      "Iteration 55, loss = 0.05459630\n",
      "Iteration 56, loss = 0.05307463\n",
      "Iteration 57, loss = 0.05162710\n",
      "Iteration 58, loss = 0.05109041\n",
      "Iteration 59, loss = 0.04915116\n",
      "Iteration 60, loss = 0.04785610\n",
      "Iteration 61, loss = 0.04674189\n",
      "Iteration 62, loss = 0.04573102\n",
      "Iteration 63, loss = 0.04464024\n",
      "Iteration 64, loss = 0.04367025\n",
      "Iteration 65, loss = 0.04271725\n",
      "Iteration 66, loss = 0.04185665\n",
      "Iteration 67, loss = 0.04097724\n",
      "Iteration 68, loss = 0.04014741\n",
      "Iteration 69, loss = 0.03924540\n",
      "Iteration 70, loss = 0.03846556\n",
      "Iteration 71, loss = 0.03774840\n",
      "Iteration 72, loss = 0.03692687\n",
      "Iteration 73, loss = 0.03620776\n",
      "Iteration 74, loss = 0.03548760\n",
      "Iteration 75, loss = 0.03491979\n",
      "Iteration 76, loss = 0.03418292\n",
      "Iteration 77, loss = 0.03360420\n",
      "Iteration 78, loss = 0.03299564\n",
      "Iteration 79, loss = 0.03243880\n",
      "Iteration 80, loss = 0.03186501\n",
      "Iteration 81, loss = 0.03134742\n",
      "Iteration 82, loss = 0.03082130\n",
      "Iteration 83, loss = 0.03033351\n",
      "Iteration 84, loss = 0.02989718\n",
      "Iteration 85, loss = 0.02939499\n",
      "Iteration 86, loss = 0.02891213\n",
      "Iteration 87, loss = 0.02850616\n",
      "Iteration 88, loss = 0.02806657\n",
      "Iteration 89, loss = 0.02893449\n",
      "Iteration 90, loss = 0.02742959\n",
      "Iteration 91, loss = 0.02701537\n",
      "Iteration 92, loss = 0.02667453\n",
      "Iteration 93, loss = 0.02636107\n",
      "Iteration 94, loss = 0.02617520\n",
      "Iteration 95, loss = 0.02595907\n",
      "Iteration 96, loss = 0.02577455\n",
      "Iteration 97, loss = 0.02552686\n",
      "Iteration 98, loss = 0.02532699\n",
      "Iteration 99, loss = 0.02511785\n",
      "Iteration 100, loss = 0.02494004\n",
      "Iteration 101, loss = 0.02471749\n",
      "Iteration 102, loss = 0.02451880\n",
      "Iteration 103, loss = 0.02434189\n",
      "Iteration 104, loss = 0.02410279\n",
      "Iteration 105, loss = 0.02384147\n",
      "Iteration 106, loss = 0.02365626\n",
      "Iteration 107, loss = 0.02340823\n",
      "Iteration 108, loss = 0.02320707\n",
      "Iteration 109, loss = 0.02304016\n",
      "Iteration 110, loss = 0.02283284\n",
      "Iteration 111, loss = 0.02266675\n",
      "Iteration 112, loss = 0.02254604\n",
      "Iteration 113, loss = 0.02231100\n",
      "Iteration 114, loss = 0.02211921\n",
      "Iteration 115, loss = 0.02194154\n",
      "Iteration 116, loss = 0.02168434\n",
      "Iteration 117, loss = 0.02149783\n",
      "Iteration 118, loss = 0.02126109\n",
      "Iteration 119, loss = 0.02102141\n",
      "Iteration 120, loss = 0.02084349\n",
      "Iteration 121, loss = 0.02065296\n",
      "Iteration 122, loss = 0.02041025\n",
      "Iteration 123, loss = 0.02025460\n",
      "Iteration 124, loss = 0.01999074\n",
      "Iteration 125, loss = 0.01975375\n",
      "Iteration 126, loss = 0.01957802\n",
      "Iteration 127, loss = 0.01944387\n",
      "Iteration 128, loss = 0.01928995\n",
      "Iteration 129, loss = 0.01915331\n",
      "Iteration 130, loss = 0.01906061\n",
      "Iteration 131, loss = 0.01897376\n",
      "Iteration 132, loss = 0.01885683\n",
      "Iteration 133, loss = 0.01874182\n",
      "Iteration 134, loss = 0.01864851\n",
      "Iteration 135, loss = 0.01858555\n",
      "Iteration 136, loss = 0.01851863\n",
      "Iteration 137, loss = 0.01839624\n",
      "Iteration 138, loss = 0.01830986\n",
      "Iteration 139, loss = 0.01825310\n",
      "Iteration 140, loss = 0.01816819\n",
      "Iteration 141, loss = 0.01810825\n",
      "Iteration 142, loss = 0.01804267\n",
      "Iteration 143, loss = 0.01798082\n",
      "Iteration 144, loss = 0.01792157\n",
      "Iteration 145, loss = 0.01786911\n",
      "Iteration 146, loss = 0.01780591\n",
      "Iteration 147, loss = 0.01775030\n",
      "Iteration 148, loss = 0.01770265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75763342\n",
      "Iteration 2, loss = 0.70906702\n",
      "Iteration 3, loss = 0.67697992\n",
      "Iteration 4, loss = 0.65618571\n",
      "Iteration 5, loss = 0.64100657\n",
      "Iteration 6, loss = 0.62570329\n",
      "Iteration 7, loss = 0.60613700\n",
      "Iteration 8, loss = 0.57992108\n",
      "Iteration 9, loss = 0.54604536\n",
      "Iteration 10, loss = 0.50629295\n",
      "Iteration 11, loss = 0.46388362\n",
      "Iteration 12, loss = 0.42150129\n",
      "Iteration 13, loss = 0.38165131\n",
      "Iteration 14, loss = 0.34533610\n",
      "Iteration 15, loss = 0.31398142\n",
      "Iteration 16, loss = 0.28677191\n",
      "Iteration 17, loss = 0.26312351\n",
      "Iteration 18, loss = 0.24242619\n",
      "Iteration 19, loss = 0.22443387\n",
      "Iteration 20, loss = 0.20833243\n",
      "Iteration 21, loss = 0.19427722\n",
      "Iteration 22, loss = 0.18175686\n",
      "Iteration 23, loss = 0.17049819\n",
      "Iteration 24, loss = 0.16034941\n",
      "Iteration 25, loss = 0.15114054\n",
      "Iteration 26, loss = 0.14282126\n",
      "Iteration 27, loss = 0.13510684\n",
      "Iteration 28, loss = 0.12812386\n",
      "Iteration 29, loss = 0.12179795\n",
      "Iteration 30, loss = 0.11585339\n",
      "Iteration 31, loss = 0.11054181\n",
      "Iteration 32, loss = 0.10540136\n",
      "Iteration 33, loss = 0.10086593\n",
      "Iteration 34, loss = 0.09646995\n",
      "Iteration 35, loss = 0.09241573\n",
      "Iteration 36, loss = 0.08863953\n",
      "Iteration 37, loss = 0.08506130\n",
      "Iteration 38, loss = 0.08176191\n",
      "Iteration 39, loss = 0.07870187\n",
      "Iteration 40, loss = 0.07570379\n",
      "Iteration 41, loss = 0.07306497\n",
      "Iteration 42, loss = 0.07048644\n",
      "Iteration 43, loss = 0.06794286\n",
      "Iteration 44, loss = 0.06568546\n",
      "Iteration 45, loss = 0.06350100\n",
      "Iteration 46, loss = 0.06143061\n",
      "Iteration 47, loss = 0.05950564\n",
      "Iteration 48, loss = 0.05765205\n",
      "Iteration 49, loss = 0.05580765\n",
      "Iteration 50, loss = 0.05430476\n",
      "Iteration 51, loss = 0.05258757\n",
      "Iteration 52, loss = 0.05104887\n",
      "Iteration 53, loss = 0.04968801\n",
      "Iteration 54, loss = 0.04823543\n",
      "Iteration 55, loss = 0.04694876\n",
      "Iteration 56, loss = 0.04573664\n",
      "Iteration 57, loss = 0.04453991\n",
      "Iteration 58, loss = 0.04332913\n",
      "Iteration 59, loss = 0.04215340\n",
      "Iteration 60, loss = 0.04104443\n",
      "Iteration 61, loss = 0.03989742\n",
      "Iteration 62, loss = 0.03888885\n",
      "Iteration 63, loss = 0.03799052\n",
      "Iteration 64, loss = 0.03698214\n",
      "Iteration 65, loss = 0.03612147\n",
      "Iteration 66, loss = 0.03531213\n",
      "Iteration 67, loss = 0.03451770\n",
      "Iteration 68, loss = 0.03360519\n",
      "Iteration 69, loss = 0.03268351\n",
      "Iteration 70, loss = 0.03200193\n",
      "Iteration 71, loss = 0.03132581\n",
      "Iteration 72, loss = 0.03072620\n",
      "Iteration 73, loss = 0.03015662\n",
      "Iteration 74, loss = 0.02964272\n",
      "Iteration 75, loss = 0.02919503\n",
      "Iteration 76, loss = 0.02863421\n",
      "Iteration 77, loss = 0.02810284\n",
      "Iteration 78, loss = 0.02765692\n",
      "Iteration 79, loss = 0.02718349\n",
      "Iteration 80, loss = 0.02670931\n",
      "Iteration 81, loss = 0.02655733\n",
      "Iteration 82, loss = 0.02622893\n",
      "Iteration 83, loss = 0.02589809\n",
      "Iteration 84, loss = 0.02557764\n",
      "Iteration 85, loss = 0.02524662\n",
      "Iteration 86, loss = 0.02498592\n",
      "Iteration 87, loss = 0.02456465\n",
      "Iteration 88, loss = 0.02416174\n",
      "Iteration 89, loss = 0.02384437\n",
      "Iteration 90, loss = 0.02356137\n",
      "Iteration 91, loss = 0.02324823\n",
      "Iteration 92, loss = 0.02284685\n",
      "Iteration 93, loss = 0.02271058\n",
      "Iteration 94, loss = 0.02222582\n",
      "Iteration 95, loss = 0.02197546\n",
      "Iteration 96, loss = 0.02168306\n",
      "Iteration 97, loss = 0.02145742\n",
      "Iteration 98, loss = 0.02124766\n",
      "Iteration 99, loss = 0.02104372\n",
      "Iteration 100, loss = 0.02085732\n",
      "Iteration 101, loss = 0.02066493\n",
      "Iteration 102, loss = 0.02048654\n",
      "Iteration 103, loss = 0.02032177\n",
      "Iteration 104, loss = 0.02014103\n",
      "Iteration 105, loss = 0.01996783\n",
      "Iteration 106, loss = 0.01981043\n",
      "Iteration 107, loss = 0.01964435\n",
      "Iteration 108, loss = 0.02003824\n",
      "Iteration 109, loss = 0.01933941\n",
      "Iteration 110, loss = 0.01892060\n",
      "Iteration 111, loss = 0.01876216\n",
      "Iteration 112, loss = 0.01862481\n",
      "Iteration 113, loss = 0.01850284\n",
      "Iteration 114, loss = 0.01837520\n",
      "Iteration 115, loss = 0.01825863\n",
      "Iteration 116, loss = 0.01913501\n",
      "Iteration 117, loss = 0.01812348\n",
      "Iteration 118, loss = 0.01799070\n",
      "Iteration 119, loss = 0.01787544\n",
      "Iteration 120, loss = 0.01776943\n",
      "Iteration 121, loss = 0.01767189\n",
      "Iteration 122, loss = 0.01757879\n",
      "Iteration 123, loss = 0.01748925\n",
      "Iteration 124, loss = 0.01740608\n",
      "Iteration 125, loss = 0.01732559\n",
      "Iteration 126, loss = 0.01724690\n",
      "Iteration 127, loss = 0.01717412\n",
      "Iteration 128, loss = 0.01710205\n",
      "Iteration 129, loss = 0.01703572\n",
      "Iteration 130, loss = 0.01697075\n",
      "Iteration 131, loss = 0.01690539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66842638\n",
      "Iteration 2, loss = 0.64389730\n",
      "Iteration 3, loss = 0.63078939\n",
      "Iteration 4, loss = 0.61667484\n",
      "Iteration 5, loss = 0.59329856\n",
      "Iteration 6, loss = 0.55803272\n",
      "Iteration 7, loss = 0.50930950\n",
      "Iteration 8, loss = 0.45008163\n",
      "Iteration 9, loss = 0.38839401\n",
      "Iteration 10, loss = 0.33233654\n",
      "Iteration 11, loss = 0.28572804\n",
      "Iteration 12, loss = 0.24863977\n",
      "Iteration 13, loss = 0.21921227\n",
      "Iteration 14, loss = 0.19560482\n",
      "Iteration 15, loss = 0.17640540\n",
      "Iteration 16, loss = 0.16062790\n",
      "Iteration 17, loss = 0.14731669\n",
      "Iteration 18, loss = 0.13607313\n",
      "Iteration 19, loss = 0.12632186\n",
      "Iteration 20, loss = 0.11795073\n",
      "Iteration 21, loss = 0.11046559\n",
      "Iteration 22, loss = 0.10394506\n",
      "Iteration 23, loss = 0.09809022\n",
      "Iteration 24, loss = 0.09300136\n",
      "Iteration 25, loss = 0.08816651\n",
      "Iteration 26, loss = 0.08387622\n",
      "Iteration 27, loss = 0.07995613\n",
      "Iteration 28, loss = 0.07648248\n",
      "Iteration 29, loss = 0.07314558\n",
      "Iteration 30, loss = 0.07007430\n",
      "Iteration 31, loss = 0.06722570\n",
      "Iteration 32, loss = 0.06452661\n",
      "Iteration 33, loss = 0.06196027\n",
      "Iteration 34, loss = 0.05966025\n",
      "Iteration 35, loss = 0.05755671\n",
      "Iteration 36, loss = 0.05551872\n",
      "Iteration 37, loss = 0.05362052\n",
      "Iteration 38, loss = 0.05191147\n",
      "Iteration 39, loss = 0.05013655\n",
      "Iteration 40, loss = 0.04854993\n",
      "Iteration 41, loss = 0.04704579\n",
      "Iteration 42, loss = 0.04568595\n",
      "Iteration 43, loss = 0.04440067\n",
      "Iteration 44, loss = 0.04318633\n",
      "Iteration 45, loss = 0.04208180\n",
      "Iteration 46, loss = 0.04100968\n",
      "Iteration 47, loss = 0.04004355\n",
      "Iteration 48, loss = 0.03893264\n",
      "Iteration 49, loss = 0.03812853\n",
      "Iteration 50, loss = 0.03709905\n",
      "Iteration 51, loss = 0.03615982\n",
      "Iteration 52, loss = 0.03535960\n",
      "Iteration 53, loss = 0.03464038\n",
      "Iteration 54, loss = 0.03382917\n",
      "Iteration 55, loss = 0.03321839\n",
      "Iteration 56, loss = 0.03254372\n",
      "Iteration 57, loss = 0.03193105\n",
      "Iteration 58, loss = 0.03134236\n",
      "Iteration 59, loss = 0.03091491\n",
      "Iteration 60, loss = 0.03029847\n",
      "Iteration 61, loss = 0.02984099\n",
      "Iteration 62, loss = 0.02935509\n",
      "Iteration 63, loss = 0.02888569\n",
      "Iteration 64, loss = 0.02843175\n",
      "Iteration 65, loss = 0.02802946\n",
      "Iteration 66, loss = 0.02757690\n",
      "Iteration 67, loss = 0.02717870\n",
      "Iteration 68, loss = 0.02679750\n",
      "Iteration 69, loss = 0.02639171\n",
      "Iteration 70, loss = 0.02602724\n",
      "Iteration 71, loss = 0.02569153\n",
      "Iteration 72, loss = 0.02543064\n",
      "Iteration 73, loss = 0.02505531\n",
      "Iteration 74, loss = 0.02475672\n",
      "Iteration 75, loss = 0.02438797\n",
      "Iteration 76, loss = 0.02404418\n",
      "Iteration 77, loss = 0.02380650\n",
      "Iteration 78, loss = 0.02344410\n",
      "Iteration 79, loss = 0.02321026\n",
      "Iteration 80, loss = 0.02292254\n",
      "Iteration 81, loss = 0.02270073\n",
      "Iteration 82, loss = 0.02248071\n",
      "Iteration 83, loss = 0.02229328\n",
      "Iteration 84, loss = 0.02205053\n",
      "Iteration 85, loss = 0.02180218\n",
      "Iteration 86, loss = 0.02160334\n",
      "Iteration 87, loss = 0.02145298\n",
      "Iteration 88, loss = 0.02119909\n",
      "Iteration 89, loss = 0.02100754\n",
      "Iteration 90, loss = 0.02080870\n",
      "Iteration 91, loss = 0.02062584\n",
      "Iteration 92, loss = 0.02039242\n",
      "Iteration 93, loss = 0.02014155\n",
      "Iteration 94, loss = 0.02012655\n",
      "Iteration 95, loss = 0.01980961\n",
      "Iteration 96, loss = 0.01959401\n",
      "Iteration 97, loss = 0.01941093\n",
      "Iteration 98, loss = 0.01925092\n",
      "Iteration 99, loss = 0.01915171\n",
      "Iteration 100, loss = 0.01901701\n",
      "Iteration 101, loss = 0.01886645\n",
      "Iteration 102, loss = 0.01872493\n",
      "Iteration 103, loss = 0.01857402\n",
      "Iteration 104, loss = 0.01828068\n",
      "Iteration 105, loss = 0.01816108\n",
      "Iteration 106, loss = 0.01796442\n",
      "Iteration 107, loss = 0.01776764\n",
      "Iteration 108, loss = 0.01760213\n",
      "Iteration 109, loss = 0.01749454\n",
      "Iteration 110, loss = 0.01731241\n",
      "Iteration 111, loss = 0.01738188\n",
      "Iteration 112, loss = 0.01655706\n",
      "Iteration 113, loss = 0.01610043\n",
      "Iteration 114, loss = 0.01589488\n",
      "Iteration 115, loss = 0.01578109\n",
      "Iteration 116, loss = 0.01568208\n",
      "Iteration 117, loss = 0.01563161\n",
      "Iteration 118, loss = 0.01537417\n",
      "Iteration 119, loss = 0.01526939\n",
      "Iteration 120, loss = 0.01495598\n",
      "Iteration 121, loss = 0.01461096\n",
      "Iteration 122, loss = 0.01428051\n",
      "Iteration 123, loss = 0.01408825\n",
      "Iteration 124, loss = 0.01391429\n",
      "Iteration 125, loss = 0.01380648\n",
      "Iteration 126, loss = 0.01372345\n",
      "Iteration 127, loss = 0.01363744\n",
      "Iteration 128, loss = 0.01355765\n",
      "Iteration 129, loss = 0.01346515\n",
      "Iteration 130, loss = 0.01334520\n",
      "Iteration 131, loss = 0.01325632\n",
      "Iteration 132, loss = 0.01315605\n",
      "Iteration 133, loss = 0.01311696\n",
      "Iteration 134, loss = 0.01306165\n",
      "Iteration 135, loss = 0.01301407\n",
      "Iteration 136, loss = 0.01295445\n",
      "Iteration 137, loss = 0.01293382\n",
      "Iteration 138, loss = 0.01284511\n",
      "Iteration 139, loss = 0.01270844\n",
      "Iteration 140, loss = 0.01244828\n",
      "Iteration 141, loss = 0.01252174\n",
      "Iteration 142, loss = 0.01221616\n",
      "Iteration 143, loss = 0.01210369\n",
      "Iteration 144, loss = 0.01199402\n",
      "Iteration 145, loss = 0.01192511\n",
      "Iteration 146, loss = 0.01188176\n",
      "Iteration 147, loss = 0.01183229\n",
      "Iteration 148, loss = 0.01178932\n",
      "Iteration 149, loss = 0.01167960\n",
      "Iteration 150, loss = 0.01137064\n",
      "Iteration 151, loss = 0.01125295\n",
      "Iteration 152, loss = 0.01120245\n",
      "Iteration 153, loss = 0.01117443\n",
      "Iteration 154, loss = 0.01115033\n",
      "Iteration 155, loss = 0.01112679\n",
      "Iteration 156, loss = 0.01109959\n",
      "Iteration 157, loss = 0.01111175\n",
      "Iteration 158, loss = 0.01101426\n",
      "Iteration 159, loss = 0.01090579\n",
      "Iteration 160, loss = 0.01078739\n",
      "Iteration 161, loss = 0.01065907\n",
      "Iteration 162, loss = 0.01064628\n",
      "Iteration 163, loss = 0.01060814\n",
      "Iteration 164, loss = 0.01060817\n",
      "Iteration 165, loss = 0.01057727\n",
      "Iteration 166, loss = 0.01055410\n",
      "Iteration 167, loss = 0.01053690\n",
      "Iteration 168, loss = 0.01052529\n",
      "Iteration 169, loss = 0.01051312\n",
      "Iteration 170, loss = 0.01048726\n",
      "Iteration 171, loss = 0.01046043\n",
      "Iteration 172, loss = 0.01042638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.90556531\n",
      "Iteration 2, loss = 0.81896234\n",
      "Iteration 3, loss = 0.75162282\n",
      "Iteration 4, loss = 0.70329162\n",
      "Iteration 5, loss = 0.67010494\n",
      "Iteration 6, loss = 0.64686032\n",
      "Iteration 7, loss = 0.62997368\n",
      "Iteration 8, loss = 0.61629063\n",
      "Iteration 9, loss = 0.60317446\n",
      "Iteration 10, loss = 0.58819442\n",
      "Iteration 11, loss = 0.56960570\n",
      "Iteration 12, loss = 0.54639682\n",
      "Iteration 13, loss = 0.51834924\n",
      "Iteration 14, loss = 0.48773179\n",
      "Iteration 15, loss = 0.45389820\n",
      "Iteration 16, loss = 0.41830204\n",
      "Iteration 17, loss = 0.38226390\n",
      "Iteration 18, loss = 0.34781493\n",
      "Iteration 19, loss = 0.31608729\n",
      "Iteration 20, loss = 0.28723979\n",
      "Iteration 21, loss = 0.26126008\n",
      "Iteration 22, loss = 0.23865331\n",
      "Iteration 23, loss = 0.21841396\n",
      "Iteration 24, loss = 0.20067352\n",
      "Iteration 25, loss = 0.18522411\n",
      "Iteration 26, loss = 0.17148678\n",
      "Iteration 27, loss = 0.15935721\n",
      "Iteration 28, loss = 0.14871757\n",
      "Iteration 29, loss = 0.13920522\n",
      "Iteration 30, loss = 0.13070202\n",
      "Iteration 31, loss = 0.12318525\n",
      "Iteration 32, loss = 0.11631308\n",
      "Iteration 33, loss = 0.11015989\n",
      "Iteration 34, loss = 0.10459158\n",
      "Iteration 35, loss = 0.09953594\n",
      "Iteration 36, loss = 0.09494861\n",
      "Iteration 37, loss = 0.09076135\n",
      "Iteration 38, loss = 0.08685116\n",
      "Iteration 39, loss = 0.08332860\n",
      "Iteration 40, loss = 0.07991909\n",
      "Iteration 41, loss = 0.07681539\n",
      "Iteration 42, loss = 0.07402222\n",
      "Iteration 43, loss = 0.07127643\n",
      "Iteration 44, loss = 0.06872197\n",
      "Iteration 45, loss = 0.06631194\n",
      "Iteration 46, loss = 0.06378981\n",
      "Iteration 47, loss = 0.06161196\n",
      "Iteration 48, loss = 0.05954205\n",
      "Iteration 49, loss = 0.05766065\n",
      "Iteration 50, loss = 0.05596343\n",
      "Iteration 51, loss = 0.05424549\n",
      "Iteration 52, loss = 0.05268008\n",
      "Iteration 53, loss = 0.05126132\n",
      "Iteration 54, loss = 0.04990887\n",
      "Iteration 55, loss = 0.04856955\n",
      "Iteration 56, loss = 0.04726614\n",
      "Iteration 57, loss = 0.04605779\n",
      "Iteration 58, loss = 0.04486640\n",
      "Iteration 59, loss = 0.04374915\n",
      "Iteration 60, loss = 0.04258143\n",
      "Iteration 61, loss = 0.04153191\n",
      "Iteration 62, loss = 0.04057213\n",
      "Iteration 63, loss = 0.03966372\n",
      "Iteration 64, loss = 0.03872421\n",
      "Iteration 65, loss = 0.03791535\n",
      "Iteration 66, loss = 0.03717330\n",
      "Iteration 67, loss = 0.03641045\n",
      "Iteration 68, loss = 0.03569662\n",
      "Iteration 69, loss = 0.03498365\n",
      "Iteration 70, loss = 0.03442588\n",
      "Iteration 71, loss = 0.03372173\n",
      "Iteration 72, loss = 0.03306248\n",
      "Iteration 73, loss = 0.03256694\n",
      "Iteration 74, loss = 0.03181406\n",
      "Iteration 75, loss = 0.03117980\n",
      "Iteration 76, loss = 0.03064836\n",
      "Iteration 77, loss = 0.03009416\n",
      "Iteration 78, loss = 0.02964667\n",
      "Iteration 79, loss = 0.02911275\n",
      "Iteration 80, loss = 0.02863407\n",
      "Iteration 81, loss = 0.02823666\n",
      "Iteration 82, loss = 0.02780446\n",
      "Iteration 83, loss = 0.02742590\n",
      "Iteration 84, loss = 0.02701634\n",
      "Iteration 85, loss = 0.02670221\n",
      "Iteration 86, loss = 0.02635651\n",
      "Iteration 87, loss = 0.02618541\n",
      "Iteration 88, loss = 0.02573754\n",
      "Iteration 89, loss = 0.02553198\n",
      "Iteration 90, loss = 0.02526312\n",
      "Iteration 91, loss = 0.02495178\n",
      "Iteration 92, loss = 0.02469000\n",
      "Iteration 93, loss = 0.02443245\n",
      "Iteration 94, loss = 0.02414546\n",
      "Iteration 95, loss = 0.02388898\n",
      "Iteration 96, loss = 0.02359947\n",
      "Iteration 97, loss = 0.02342309\n",
      "Iteration 98, loss = 0.02304224\n",
      "Iteration 99, loss = 0.02645356\n",
      "Iteration 100, loss = 0.02351317\n",
      "Iteration 101, loss = 0.02313296\n",
      "Iteration 102, loss = 0.02464379\n",
      "Iteration 103, loss = 0.02336791\n",
      "Iteration 104, loss = 0.02299537\n",
      "Iteration 105, loss = 0.02277823\n",
      "Iteration 106, loss = 0.02261040\n",
      "Iteration 107, loss = 0.02244940\n",
      "Iteration 108, loss = 0.02278824\n",
      "Iteration 109, loss = 0.02221793\n",
      "Iteration 110, loss = 0.02202132\n",
      "Iteration 111, loss = 0.02191484\n",
      "Iteration 112, loss = 0.02182597\n",
      "Iteration 113, loss = 0.02172690\n",
      "Iteration 114, loss = 0.02164192\n",
      "Iteration 115, loss = 0.02155859\n",
      "Iteration 116, loss = 0.02147668\n",
      "Iteration 117, loss = 0.02139812\n",
      "Iteration 118, loss = 0.02132804\n",
      "Iteration 119, loss = 0.02124518\n",
      "Iteration 120, loss = 0.02118567\n",
      "Iteration 121, loss = 0.02110031\n",
      "Iteration 122, loss = 0.02102249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66626501\n",
      "Iteration 2, loss = 0.64411785\n",
      "Iteration 3, loss = 0.63274909\n",
      "Iteration 4, loss = 0.62419033\n",
      "Iteration 5, loss = 0.61199917\n",
      "Iteration 6, loss = 0.59375733\n",
      "Iteration 7, loss = 0.56769299\n",
      "Iteration 8, loss = 0.53301747\n",
      "Iteration 9, loss = 0.49008228\n",
      "Iteration 10, loss = 0.44070884\n",
      "Iteration 11, loss = 0.39029500\n",
      "Iteration 12, loss = 0.34416598\n",
      "Iteration 13, loss = 0.30433907\n",
      "Iteration 14, loss = 0.26996398\n",
      "Iteration 15, loss = 0.24097435\n",
      "Iteration 16, loss = 0.21659567\n",
      "Iteration 17, loss = 0.19585875\n",
      "Iteration 18, loss = 0.17857905\n",
      "Iteration 19, loss = 0.16392530\n",
      "Iteration 20, loss = 0.15169672\n",
      "Iteration 21, loss = 0.14105002\n",
      "Iteration 22, loss = 0.13179700\n",
      "Iteration 23, loss = 0.12346491\n",
      "Iteration 24, loss = 0.11692891\n",
      "Iteration 25, loss = 0.11038908\n",
      "Iteration 26, loss = 0.10495993\n",
      "Iteration 27, loss = 0.09997244\n",
      "Iteration 28, loss = 0.09548856\n",
      "Iteration 29, loss = 0.09140734\n",
      "Iteration 30, loss = 0.08769392\n",
      "Iteration 31, loss = 0.08423734\n",
      "Iteration 32, loss = 0.08098747\n",
      "Iteration 33, loss = 0.07796418\n",
      "Iteration 34, loss = 0.07510332\n",
      "Iteration 35, loss = 0.07252242\n",
      "Iteration 36, loss = 0.07016935\n",
      "Iteration 37, loss = 0.06769324\n",
      "Iteration 38, loss = 0.06551253\n",
      "Iteration 39, loss = 0.06330865\n",
      "Iteration 40, loss = 0.06122727\n",
      "Iteration 41, loss = 0.05922942\n",
      "Iteration 42, loss = 0.05736307\n",
      "Iteration 43, loss = 0.05552566\n",
      "Iteration 44, loss = 0.05382748\n",
      "Iteration 45, loss = 0.05224093\n",
      "Iteration 46, loss = 0.05073383\n",
      "Iteration 47, loss = 0.04932278\n",
      "Iteration 48, loss = 0.04792410\n",
      "Iteration 49, loss = 0.04668733\n",
      "Iteration 50, loss = 0.04554505\n",
      "Iteration 51, loss = 0.04423460\n",
      "Iteration 52, loss = 0.04326508\n",
      "Iteration 53, loss = 0.04211823\n",
      "Iteration 54, loss = 0.04118373\n",
      "Iteration 55, loss = 0.04013423\n",
      "Iteration 56, loss = 0.03923188\n",
      "Iteration 57, loss = 0.03828446\n",
      "Iteration 58, loss = 0.03751811\n",
      "Iteration 59, loss = 0.03661207\n",
      "Iteration 60, loss = 0.03573543\n",
      "Iteration 61, loss = 0.03479733\n",
      "Iteration 62, loss = 0.03404032\n",
      "Iteration 63, loss = 0.03327989\n",
      "Iteration 64, loss = 0.03251448\n",
      "Iteration 65, loss = 0.03180207\n",
      "Iteration 66, loss = 0.03115576\n",
      "Iteration 67, loss = 0.03056736\n",
      "Iteration 68, loss = 0.02999030\n",
      "Iteration 69, loss = 0.02931046\n",
      "Iteration 70, loss = 0.02879713\n",
      "Iteration 71, loss = 0.02826014\n",
      "Iteration 72, loss = 0.02775937\n",
      "Iteration 73, loss = 0.02725701\n",
      "Iteration 74, loss = 0.02687549\n",
      "Iteration 75, loss = 0.02642400\n",
      "Iteration 76, loss = 0.02591687\n",
      "Iteration 77, loss = 0.02564563\n",
      "Iteration 78, loss = 0.02512298\n",
      "Iteration 79, loss = 0.02473780\n",
      "Iteration 80, loss = 0.02425272\n",
      "Iteration 81, loss = 0.02409832\n",
      "Iteration 82, loss = 0.02375380\n",
      "Iteration 83, loss = 0.02329981\n",
      "Iteration 84, loss = 0.02305098\n",
      "Iteration 85, loss = 0.02274294\n",
      "Iteration 86, loss = 0.02245210\n",
      "Iteration 87, loss = 0.02211390\n",
      "Iteration 88, loss = 0.02200519\n",
      "Iteration 89, loss = 0.02155723\n",
      "Iteration 90, loss = 0.02119698\n",
      "Iteration 91, loss = 0.02089352\n",
      "Iteration 92, loss = 0.02057989\n",
      "Iteration 93, loss = 0.02026787\n",
      "Iteration 94, loss = 0.01994890\n",
      "Iteration 95, loss = 0.01976682\n",
      "Iteration 96, loss = 0.01952507\n",
      "Iteration 97, loss = 0.01933834\n",
      "Iteration 98, loss = 0.01914621\n",
      "Iteration 99, loss = 0.01895119\n",
      "Iteration 100, loss = 0.01880616\n",
      "Iteration 101, loss = 0.01866668\n",
      "Iteration 102, loss = 0.01850650\n",
      "Iteration 103, loss = 0.01840581\n",
      "Iteration 104, loss = 0.01821342\n",
      "Iteration 105, loss = 0.01801129\n",
      "Iteration 106, loss = 0.01791510\n",
      "Iteration 107, loss = 0.01775201\n",
      "Iteration 108, loss = 0.01763705\n",
      "Iteration 109, loss = 0.01741746\n",
      "Iteration 110, loss = 0.01726683\n",
      "Iteration 111, loss = 0.01703476\n",
      "Iteration 112, loss = 0.01698542\n",
      "Iteration 113, loss = 0.01661853\n",
      "Iteration 114, loss = 0.01640094\n",
      "Iteration 115, loss = 0.01622456\n",
      "Iteration 116, loss = 0.01608951\n",
      "Iteration 117, loss = 0.01582353\n",
      "Iteration 118, loss = 0.01566547\n",
      "Iteration 119, loss = 0.01552173\n",
      "Iteration 120, loss = 0.01533234\n",
      "Iteration 121, loss = 0.01524751\n",
      "Iteration 122, loss = 0.01515624\n",
      "Iteration 123, loss = 0.01504116\n",
      "Iteration 124, loss = 0.01491384\n",
      "Iteration 125, loss = 0.01483498\n",
      "Iteration 126, loss = 0.01478011\n",
      "Iteration 127, loss = 0.01471375\n",
      "Iteration 128, loss = 0.01464828\n",
      "Iteration 129, loss = 0.01462351\n",
      "Iteration 130, loss = 0.01453955\n",
      "Iteration 131, loss = 0.01449059\n",
      "Iteration 132, loss = 0.01448047\n",
      "Iteration 133, loss = 0.01439565\n",
      "Iteration 134, loss = 0.01434909\n",
      "Iteration 135, loss = 0.01455608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69560865\n",
      "Iteration 2, loss = 0.65979704\n",
      "Iteration 3, loss = 0.64158032\n",
      "Iteration 4, loss = 0.63068951\n",
      "Iteration 5, loss = 0.62243752\n",
      "Iteration 6, loss = 0.61233299\n",
      "Iteration 7, loss = 0.59866870\n",
      "Iteration 8, loss = 0.57901801\n",
      "Iteration 9, loss = 0.54992622\n",
      "Iteration 10, loss = 0.51015860\n",
      "Iteration 11, loss = 0.46053597\n",
      "Iteration 12, loss = 0.40709063\n",
      "Iteration 13, loss = 0.35625789\n",
      "Iteration 14, loss = 0.31204749\n",
      "Iteration 15, loss = 0.27451000\n",
      "Iteration 16, loss = 0.24386124\n",
      "Iteration 17, loss = 0.21826110\n",
      "Iteration 18, loss = 0.19707867\n",
      "Iteration 19, loss = 0.17947817\n",
      "Iteration 20, loss = 0.16459266\n",
      "Iteration 21, loss = 0.15156807\n",
      "Iteration 22, loss = 0.13942434\n",
      "Iteration 23, loss = 0.12896238\n",
      "Iteration 24, loss = 0.12047411\n",
      "Iteration 25, loss = 0.11295832\n",
      "Iteration 26, loss = 0.10656484\n",
      "Iteration 27, loss = 0.10085178\n",
      "Iteration 28, loss = 0.09576069\n",
      "Iteration 29, loss = 0.09115076\n",
      "Iteration 30, loss = 0.08683327\n",
      "Iteration 31, loss = 0.08296608\n",
      "Iteration 32, loss = 0.07939132\n",
      "Iteration 33, loss = 0.07608129\n",
      "Iteration 34, loss = 0.07300164\n",
      "Iteration 35, loss = 0.07013944\n",
      "Iteration 36, loss = 0.06752181\n",
      "Iteration 37, loss = 0.06504185\n",
      "Iteration 38, loss = 0.06266827\n",
      "Iteration 39, loss = 0.06038533\n",
      "Iteration 40, loss = 0.05812302\n",
      "Iteration 41, loss = 0.05602580\n",
      "Iteration 42, loss = 0.05419739\n",
      "Iteration 43, loss = 0.05239354\n",
      "Iteration 44, loss = 0.05074650\n",
      "Iteration 45, loss = 0.04927345\n",
      "Iteration 46, loss = 0.04784991\n",
      "Iteration 47, loss = 0.04654940\n",
      "Iteration 48, loss = 0.04529949\n",
      "Iteration 49, loss = 0.04405845\n",
      "Iteration 50, loss = 0.04292505\n",
      "Iteration 51, loss = 0.04180640\n",
      "Iteration 52, loss = 0.04073081\n",
      "Iteration 53, loss = 0.03974977\n",
      "Iteration 54, loss = 0.03859669\n",
      "Iteration 55, loss = 0.03780709\n",
      "Iteration 56, loss = 0.03672804\n",
      "Iteration 57, loss = 0.03579094\n",
      "Iteration 58, loss = 0.03504376\n",
      "Iteration 59, loss = 0.03421169\n",
      "Iteration 60, loss = 0.03345848\n",
      "Iteration 61, loss = 0.03277700\n",
      "Iteration 62, loss = 0.03205352\n",
      "Iteration 63, loss = 0.03139855\n",
      "Iteration 64, loss = 0.03073530\n",
      "Iteration 65, loss = 0.03016777\n",
      "Iteration 66, loss = 0.02954728\n",
      "Iteration 67, loss = 0.02903500\n",
      "Iteration 68, loss = 0.02850246\n",
      "Iteration 69, loss = 0.02799003\n",
      "Iteration 70, loss = 0.02749125\n",
      "Iteration 71, loss = 0.02714481\n",
      "Iteration 72, loss = 0.02651784\n",
      "Iteration 73, loss = 0.02602421\n",
      "Iteration 74, loss = 0.02564376\n",
      "Iteration 75, loss = 0.02523929\n",
      "Iteration 76, loss = 0.02818872\n",
      "Iteration 77, loss = 0.02515056\n",
      "Iteration 78, loss = 0.02462777\n",
      "Iteration 79, loss = 0.02426053\n",
      "Iteration 80, loss = 0.02395124\n",
      "Iteration 81, loss = 0.02371747\n",
      "Iteration 82, loss = 0.02344175\n",
      "Iteration 83, loss = 0.02316280\n",
      "Iteration 84, loss = 0.02297769\n",
      "Iteration 85, loss = 0.02270481\n",
      "Iteration 86, loss = 0.02250327\n",
      "Iteration 87, loss = 0.02229230\n",
      "Iteration 88, loss = 0.02206544\n",
      "Iteration 89, loss = 0.02184016\n",
      "Iteration 90, loss = 0.02164704\n",
      "Iteration 91, loss = 0.02147964\n",
      "Iteration 92, loss = 0.02125135\n",
      "Iteration 93, loss = 0.02102814\n",
      "Iteration 94, loss = 0.02086404\n",
      "Iteration 95, loss = 0.02065546\n",
      "Iteration 96, loss = 0.02050357\n",
      "Iteration 97, loss = 0.02032923\n",
      "Iteration 98, loss = 0.02015050\n",
      "Iteration 99, loss = 0.02001043\n",
      "Iteration 100, loss = 0.01986501\n",
      "Iteration 101, loss = 0.01970685\n",
      "Iteration 102, loss = 0.01962450\n",
      "Iteration 103, loss = 0.01945412\n",
      "Iteration 104, loss = 0.01934499\n",
      "Iteration 105, loss = 0.01922515\n",
      "Iteration 106, loss = 0.01912385\n",
      "Iteration 107, loss = 0.01901831\n",
      "Iteration 108, loss = 0.01893412\n",
      "Iteration 109, loss = 0.01880561\n",
      "Iteration 110, loss = 0.01870793\n",
      "Iteration 111, loss = 0.01862743\n",
      "Iteration 112, loss = 0.01851861\n",
      "Iteration 113, loss = 0.01840657\n",
      "Iteration 114, loss = 0.01830582\n",
      "Iteration 115, loss = 0.01805450\n",
      "Iteration 116, loss = 0.01789788\n",
      "Iteration 117, loss = 0.01777536\n",
      "Iteration 118, loss = 0.01759297\n",
      "Iteration 119, loss = 0.01745886\n",
      "Iteration 120, loss = 0.01738855\n",
      "Iteration 121, loss = 0.01734577\n",
      "Iteration 122, loss = 0.01727280\n",
      "Iteration 123, loss = 0.01800633\n",
      "Iteration 124, loss = 0.01732433\n",
      "Iteration 125, loss = 0.01722725\n",
      "Iteration 126, loss = 0.01715772\n",
      "Iteration 127, loss = 0.01711496\n",
      "Iteration 128, loss = 0.01707474\n",
      "Iteration 129, loss = 0.01705006\n",
      "Iteration 130, loss = 0.01700729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65317155\n",
      "Iteration 2, loss = 0.63798091\n",
      "Iteration 3, loss = 0.63126454\n",
      "Iteration 4, loss = 0.62320103\n",
      "Iteration 5, loss = 0.60823037\n",
      "Iteration 6, loss = 0.58113717\n",
      "Iteration 7, loss = 0.53899107\n",
      "Iteration 8, loss = 0.48452396\n",
      "Iteration 9, loss = 0.42257824\n",
      "Iteration 10, loss = 0.36212334\n",
      "Iteration 11, loss = 0.31060013\n",
      "Iteration 12, loss = 0.26823073\n",
      "Iteration 13, loss = 0.23424920\n",
      "Iteration 14, loss = 0.20681535\n",
      "Iteration 15, loss = 0.18470566\n",
      "Iteration 16, loss = 0.16642504\n",
      "Iteration 17, loss = 0.15113366\n",
      "Iteration 18, loss = 0.13850046\n",
      "Iteration 19, loss = 0.12760771\n",
      "Iteration 20, loss = 0.11830811\n",
      "Iteration 21, loss = 0.11015760\n",
      "Iteration 22, loss = 0.10309677\n",
      "Iteration 23, loss = 0.09676700\n",
      "Iteration 24, loss = 0.09109333\n",
      "Iteration 25, loss = 0.08606962\n",
      "Iteration 26, loss = 0.08161579\n",
      "Iteration 27, loss = 0.07736844\n",
      "Iteration 28, loss = 0.07365145\n",
      "Iteration 29, loss = 0.07020247\n",
      "Iteration 30, loss = 0.06703311\n",
      "Iteration 31, loss = 0.06412553\n",
      "Iteration 32, loss = 0.06139684\n",
      "Iteration 33, loss = 0.05892176\n",
      "Iteration 34, loss = 0.05664931\n",
      "Iteration 35, loss = 0.05449032\n",
      "Iteration 36, loss = 0.05260800\n",
      "Iteration 37, loss = 0.05067400\n",
      "Iteration 38, loss = 0.04890401\n",
      "Iteration 39, loss = 0.04735372\n",
      "Iteration 40, loss = 0.04569607\n",
      "Iteration 41, loss = 0.04420888\n",
      "Iteration 42, loss = 0.04273421\n",
      "Iteration 43, loss = 0.04139094\n",
      "Iteration 44, loss = 0.04019025\n",
      "Iteration 45, loss = 0.03890091\n",
      "Iteration 46, loss = 0.03778085\n",
      "Iteration 47, loss = 0.03669785\n",
      "Iteration 48, loss = 0.03571525\n",
      "Iteration 49, loss = 0.03472986\n",
      "Iteration 50, loss = 0.03378698\n",
      "Iteration 51, loss = 0.03298506\n",
      "Iteration 52, loss = 0.03218826\n",
      "Iteration 53, loss = 0.03146045\n",
      "Iteration 54, loss = 0.03071938\n",
      "Iteration 55, loss = 0.03004206\n",
      "Iteration 56, loss = 0.02939686\n",
      "Iteration 57, loss = 0.02874470\n",
      "Iteration 58, loss = 0.02815672\n",
      "Iteration 59, loss = 0.02758396\n",
      "Iteration 60, loss = 0.02696093\n",
      "Iteration 61, loss = 0.02639053\n",
      "Iteration 62, loss = 0.02583292\n",
      "Iteration 63, loss = 0.02523002\n",
      "Iteration 64, loss = 0.02474237\n",
      "Iteration 65, loss = 0.02432833\n",
      "Iteration 66, loss = 0.02393807\n",
      "Iteration 67, loss = 0.02352277\n",
      "Iteration 68, loss = 0.02322166\n",
      "Iteration 69, loss = 0.02286228\n",
      "Iteration 70, loss = 0.02246221\n",
      "Iteration 71, loss = 0.02215342\n",
      "Iteration 72, loss = 0.02181999\n",
      "Iteration 73, loss = 0.02147695\n",
      "Iteration 74, loss = 0.02115166\n",
      "Iteration 75, loss = 0.02088815\n",
      "Iteration 76, loss = 0.02066994\n",
      "Iteration 77, loss = 0.02033961\n",
      "Iteration 78, loss = 0.02008881\n",
      "Iteration 79, loss = 0.01986868\n",
      "Iteration 80, loss = 0.01964810\n",
      "Iteration 81, loss = 0.01948688\n",
      "Iteration 82, loss = 0.01924744\n",
      "Iteration 83, loss = 0.01908291\n",
      "Iteration 84, loss = 0.01891027\n",
      "Iteration 85, loss = 0.01878191\n",
      "Iteration 86, loss = 0.01864368\n",
      "Iteration 87, loss = 0.01848181\n",
      "Iteration 88, loss = 0.01837460\n",
      "Iteration 89, loss = 0.01822458\n",
      "Iteration 90, loss = 0.01807550\n",
      "Iteration 91, loss = 0.01794211\n",
      "Iteration 92, loss = 0.01775606\n",
      "Iteration 93, loss = 0.01761420\n",
      "Iteration 94, loss = 0.01748989\n",
      "Iteration 95, loss = 0.01737171\n",
      "Iteration 96, loss = 0.01726901\n",
      "Iteration 97, loss = 0.01719424\n",
      "Iteration 98, loss = 0.01710281\n",
      "Iteration 99, loss = 0.01701607\n",
      "Iteration 100, loss = 0.01691741\n",
      "Iteration 101, loss = 0.01685946\n",
      "Iteration 102, loss = 0.01674239\n",
      "Iteration 103, loss = 0.01660000\n",
      "Iteration 104, loss = 0.01633235\n",
      "Iteration 105, loss = 0.01615936\n",
      "Iteration 106, loss = 0.01591069\n",
      "Iteration 107, loss = 0.01573252\n",
      "Iteration 108, loss = 0.01569292\n",
      "Iteration 109, loss = 0.01560983\n",
      "Iteration 110, loss = 0.01555322\n",
      "Iteration 111, loss = 0.01550353\n",
      "Iteration 112, loss = 0.01545729\n",
      "Iteration 113, loss = 0.01541590\n",
      "Iteration 114, loss = 0.01536908\n",
      "Iteration 115, loss = 0.01531907\n",
      "Iteration 116, loss = 0.01528420\n",
      "Iteration 117, loss = 0.01523742\n",
      "Iteration 118, loss = 0.01518721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63696776\n",
      "Iteration 2, loss = 0.63282654\n",
      "Iteration 3, loss = 0.61979715\n",
      "Iteration 4, loss = 0.58432919\n",
      "Iteration 5, loss = 0.51284467\n",
      "Iteration 6, loss = 0.41116867\n",
      "Iteration 7, loss = 0.30730750\n",
      "Iteration 8, loss = 0.22890933\n",
      "Iteration 9, loss = 0.17644726\n",
      "Iteration 10, loss = 0.14237327\n",
      "Iteration 11, loss = 0.11901568\n",
      "Iteration 12, loss = 0.10230366\n",
      "Iteration 13, loss = 0.09010300\n",
      "Iteration 14, loss = 0.08088586\n",
      "Iteration 15, loss = 0.07357607\n",
      "Iteration 16, loss = 0.06773244\n",
      "Iteration 17, loss = 0.06298971\n",
      "Iteration 18, loss = 0.05900810\n",
      "Iteration 19, loss = 0.05551217\n",
      "Iteration 20, loss = 0.05250987\n",
      "Iteration 21, loss = 0.05000982\n",
      "Iteration 22, loss = 0.04768969\n",
      "Iteration 23, loss = 0.04566582\n",
      "Iteration 24, loss = 0.04384786\n",
      "Iteration 25, loss = 0.04216660\n",
      "Iteration 26, loss = 0.04065004\n",
      "Iteration 27, loss = 0.03922716\n",
      "Iteration 28, loss = 0.03793656\n",
      "Iteration 29, loss = 0.03654416\n",
      "Iteration 30, loss = 0.03530183\n",
      "Iteration 31, loss = 0.03414051\n",
      "Iteration 32, loss = 0.03324197\n",
      "Iteration 33, loss = 0.03213869\n",
      "Iteration 34, loss = 0.03126175\n",
      "Iteration 35, loss = 0.03046073\n",
      "Iteration 36, loss = 0.02972216\n",
      "Iteration 37, loss = 0.02908683\n",
      "Iteration 38, loss = 0.02840904\n",
      "Iteration 39, loss = 0.02783102\n",
      "Iteration 40, loss = 0.02733395\n",
      "Iteration 41, loss = 0.02687156\n",
      "Iteration 42, loss = 0.02630759\n",
      "Iteration 43, loss = 0.02582324\n",
      "Iteration 44, loss = 0.02534254\n",
      "Iteration 45, loss = 0.02498255\n",
      "Iteration 46, loss = 0.02461212\n",
      "Iteration 47, loss = 0.02422079\n",
      "Iteration 48, loss = 0.02386910\n",
      "Iteration 49, loss = 0.02350821\n",
      "Iteration 50, loss = 0.02315350\n",
      "Iteration 51, loss = 0.02288117\n",
      "Iteration 52, loss = 0.02250514\n",
      "Iteration 53, loss = 0.02240396\n",
      "Iteration 54, loss = 0.02185516\n",
      "Iteration 55, loss = 0.02142630\n",
      "Iteration 56, loss = 0.02133425\n",
      "Iteration 57, loss = 0.02088855\n",
      "Iteration 58, loss = 0.02049908\n",
      "Iteration 59, loss = 0.02020657\n",
      "Iteration 60, loss = 0.02000421\n",
      "Iteration 61, loss = 0.01957761\n",
      "Iteration 62, loss = 0.01935810\n",
      "Iteration 63, loss = 0.01903880\n",
      "Iteration 64, loss = 0.01860600\n",
      "Iteration 65, loss = 0.01826925\n",
      "Iteration 66, loss = 0.01789192\n",
      "Iteration 67, loss = 0.01762503\n",
      "Iteration 68, loss = 0.01716231\n",
      "Iteration 69, loss = 0.01684138\n",
      "Iteration 70, loss = 0.01651393\n",
      "Iteration 71, loss = 0.01618466\n",
      "Iteration 72, loss = 0.01599587\n",
      "Iteration 73, loss = 0.01590914\n",
      "Iteration 74, loss = 0.01555265\n",
      "Iteration 75, loss = 0.01551419\n",
      "Iteration 76, loss = 0.01512517\n",
      "Iteration 77, loss = 0.01494829\n",
      "Iteration 78, loss = 0.01480798\n",
      "Iteration 79, loss = 0.01463094\n",
      "Iteration 80, loss = 0.01448392\n",
      "Iteration 81, loss = 0.01427834\n",
      "Iteration 82, loss = 0.01421339\n",
      "Iteration 83, loss = 0.01403036\n",
      "Iteration 84, loss = 0.01389796\n",
      "Iteration 85, loss = 0.01377095\n",
      "Iteration 86, loss = 0.01362441\n",
      "Iteration 87, loss = 0.01358662\n",
      "Iteration 88, loss = 0.01341500\n",
      "Iteration 89, loss = 0.01331974\n",
      "Iteration 90, loss = 0.01321514\n",
      "Iteration 91, loss = 0.01389253\n",
      "Iteration 92, loss = 0.01323548\n",
      "Iteration 93, loss = 0.01301324\n",
      "Iteration 94, loss = 0.01291117\n",
      "Iteration 95, loss = 0.01274972\n",
      "Iteration 96, loss = 0.01275753\n",
      "Iteration 97, loss = 0.01259417\n",
      "Iteration 98, loss = 0.01252104\n",
      "Iteration 99, loss = 0.01241378\n",
      "Iteration 100, loss = 0.01347396\n",
      "Iteration 101, loss = 0.01233813\n",
      "Iteration 102, loss = 0.01220188\n",
      "Iteration 103, loss = 0.01196234\n",
      "Iteration 104, loss = 0.01180929\n",
      "Iteration 105, loss = 0.01172687\n",
      "Iteration 106, loss = 0.01161639\n",
      "Iteration 107, loss = 0.01156394\n",
      "Iteration 108, loss = 0.01148462\n",
      "Iteration 109, loss = 0.01144386\n",
      "Iteration 110, loss = 0.01140504\n",
      "Iteration 111, loss = 0.01130481\n",
      "Iteration 112, loss = 0.01132439\n",
      "Iteration 113, loss = 0.01119085\n",
      "Iteration 114, loss = 0.01109905\n",
      "Iteration 115, loss = 0.01113417\n",
      "Iteration 116, loss = 0.01104610\n",
      "Iteration 117, loss = 0.01096517\n",
      "Iteration 118, loss = 0.01089459\n",
      "Iteration 119, loss = 0.01082642\n",
      "Iteration 120, loss = 0.01079874\n",
      "Iteration 121, loss = 0.01074307\n",
      "Iteration 122, loss = 0.01076073\n",
      "Iteration 123, loss = 0.01061388\n",
      "Iteration 124, loss = 0.01058623\n",
      "Iteration 125, loss = 0.01046115\n",
      "Iteration 126, loss = 0.01042667\n",
      "Iteration 127, loss = 0.01041165\n",
      "Iteration 128, loss = 0.01033353\n",
      "Iteration 129, loss = 0.01030110\n",
      "Iteration 130, loss = 0.01023369\n",
      "Iteration 131, loss = 0.01018053\n",
      "Iteration 132, loss = 0.01011404\n",
      "Iteration 133, loss = 0.01003995\n",
      "Iteration 134, loss = 0.00992716\n",
      "Iteration 135, loss = 0.00983151\n",
      "Iteration 136, loss = 0.00979711\n",
      "Iteration 137, loss = 0.00977351\n",
      "Iteration 138, loss = 0.00974896\n",
      "Iteration 139, loss = 0.00973451\n",
      "Iteration 140, loss = 0.00970927\n",
      "Iteration 141, loss = 0.00967127\n",
      "Iteration 142, loss = 0.00965674\n",
      "Iteration 143, loss = 0.00962286\n",
      "Iteration 144, loss = 0.00961168\n",
      "Iteration 145, loss = 0.00957306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=MLPClassifier(activation=&#x27;logistic&#x27;,\n",
       "                                           hidden_layer_sizes=(7, 2, 4),\n",
       "                                           verbose=True),\n",
       "                   param_distributions={&#x27;activation&#x27;: (&#x27;logistic&#x27;, &#x27;relu&#x27;),\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(2, 2, 2),\n",
       "                                                               (2, 2, 3),\n",
       "                                                               (2, 2, 4),\n",
       "                                                               (2, 2, 5),\n",
       "                                                               (2, 2, 6),\n",
       "                                                               (2, 2, 7),\n",
       "                                                               (2, 2, 8),\n",
       "                                                               (2, 2, 9),\n",
       "                                                               (2, 3, 2),\n",
       "                                                               (2, 3, 3),\n",
       "                                                               (2, 3, 4),\n",
       "                                                               (2, 3, 5),\n",
       "                                                               (2, 3, 6),\n",
       "                                                               (2, 3, 7),\n",
       "                                                               (2, 3, 8),\n",
       "                                                               (2, 3, 9),\n",
       "                                                               (2, 4, 2),\n",
       "                                                               (2, 4, 3),\n",
       "                                                               (2, 4, 4),\n",
       "                                                               (2, 4, 5),\n",
       "                                                               (2, 4, 6),\n",
       "                                                               (2, 4, 7),\n",
       "                                                               (2, 4, 8),\n",
       "                                                               (2, 4, 9),\n",
       "                                                               (2, 5, 2),\n",
       "                                                               (2, 5, 3),\n",
       "                                                               (2, 5, 4),\n",
       "                                                               (2, 5, 5),\n",
       "                                                               (2, 5, 6),\n",
       "                                                               (2, 5, 7), ...]},\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=MLPClassifier(activation=&#x27;logistic&#x27;,\n",
       "                                           hidden_layer_sizes=(7, 2, 4),\n",
       "                                           verbose=True),\n",
       "                   param_distributions={&#x27;activation&#x27;: (&#x27;logistic&#x27;, &#x27;relu&#x27;),\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(2, 2, 2),\n",
       "                                                               (2, 2, 3),\n",
       "                                                               (2, 2, 4),\n",
       "                                                               (2, 2, 5),\n",
       "                                                               (2, 2, 6),\n",
       "                                                               (2, 2, 7),\n",
       "                                                               (2, 2, 8),\n",
       "                                                               (2, 2, 9),\n",
       "                                                               (2, 3, 2),\n",
       "                                                               (2, 3, 3),\n",
       "                                                               (2, 3, 4),\n",
       "                                                               (2, 3, 5),\n",
       "                                                               (2, 3, 6),\n",
       "                                                               (2, 3, 7),\n",
       "                                                               (2, 3, 8),\n",
       "                                                               (2, 3, 9),\n",
       "                                                               (2, 4, 2),\n",
       "                                                               (2, 4, 3),\n",
       "                                                               (2, 4, 4),\n",
       "                                                               (2, 4, 5),\n",
       "                                                               (2, 4, 6),\n",
       "                                                               (2, 4, 7),\n",
       "                                                               (2, 4, 8),\n",
       "                                                               (2, 4, 9),\n",
       "                                                               (2, 5, 2),\n",
       "                                                               (2, 5, 3),\n",
       "                                                               (2, 5, 4),\n",
       "                                                               (2, 5, 5),\n",
       "                                                               (2, 5, 6),\n",
       "                                                               (2, 5, 7), ...]},\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(7, 2, 4), verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(7, 2, 4), verbose=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=MLPClassifier(activation='logistic',\n",
       "                                           hidden_layer_sizes=(7, 2, 4),\n",
       "                                           verbose=True),\n",
       "                   param_distributions={'activation': ('logistic', 'relu'),\n",
       "                                        'hidden_layer_sizes': [(2, 2, 2),\n",
       "                                                               (2, 2, 3),\n",
       "                                                               (2, 2, 4),\n",
       "                                                               (2, 2, 5),\n",
       "                                                               (2, 2, 6),\n",
       "                                                               (2, 2, 7),\n",
       "                                                               (2, 2, 8),\n",
       "                                                               (2, 2, 9),\n",
       "                                                               (2, 3, 2),\n",
       "                                                               (2, 3, 3),\n",
       "                                                               (2, 3, 4),\n",
       "                                                               (2, 3, 5),\n",
       "                                                               (2, 3, 6),\n",
       "                                                               (2, 3, 7),\n",
       "                                                               (2, 3, 8),\n",
       "                                                               (2, 3, 9),\n",
       "                                                               (2, 4, 2),\n",
       "                                                               (2, 4, 3),\n",
       "                                                               (2, 4, 4),\n",
       "                                                               (2, 4, 5),\n",
       "                                                               (2, 4, 6),\n",
       "                                                               (2, 4, 7),\n",
       "                                                               (2, 4, 8),\n",
       "                                                               (2, 4, 9),\n",
       "                                                               (2, 5, 2),\n",
       "                                                               (2, 5, 3),\n",
       "                                                               (2, 5, 4),\n",
       "                                                               (2, 5, 5),\n",
       "                                                               (2, 5, 6),\n",
       "                                                               (2, 5, 7), ...]},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (6, 5, 8), 'activation': 'logistic'}\n",
      "{'mean_fit_time': array([ 9.54378343, 18.16308675, 12.03468947, 22.31278887, 27.59460759,\n",
      "       10.43801007, 13.54548831, 16.07876048, 33.01964002, 22.547472  ]), 'std_fit_time': array([7.8607568 , 6.92381546, 2.46079537, 3.99833032, 9.44997412,\n",
      "       5.14576206, 1.76930274, 7.0065336 , 6.0051647 , 3.08362057]), 'mean_score_time': array([0.02361732, 0.02156153, 0.02493176, 0.02549381, 0.21474643,\n",
      "       0.01945095, 0.03155756, 0.02407827, 0.02791166, 0.03320956]), 'std_score_time': array([0.004845  , 0.00239372, 0.00411929, 0.00210266, 0.37217686,\n",
      "       0.001924  , 0.01157809, 0.00406484, 0.00291158, 0.00575843]), 'param_hidden_layer_sizes': masked_array(data=[(3, 4, 3), (9, 6, 3), (7, 5, 8), (6, 5, 8), (8, 7, 9),\n",
      "                   (3, 7, 2), (5, 6, 8), (6, 7, 2), (8, 8, 3), (6, 7, 4)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_activation': masked_array(data=['relu', 'relu', 'relu', 'logistic', 'logistic', 'relu',\n",
      "                   'relu', 'relu', 'logistic', 'logistic'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (3, 4, 3), 'activation': 'relu'}, {'hidden_layer_sizes': (9, 6, 3), 'activation': 'relu'}, {'hidden_layer_sizes': (7, 5, 8), 'activation': 'relu'}, {'hidden_layer_sizes': (6, 5, 8), 'activation': 'logistic'}, {'hidden_layer_sizes': (8, 7, 9), 'activation': 'logistic'}, {'hidden_layer_sizes': (3, 7, 2), 'activation': 'relu'}, {'hidden_layer_sizes': (5, 6, 8), 'activation': 'relu'}, {'hidden_layer_sizes': (6, 7, 2), 'activation': 'relu'}, {'hidden_layer_sizes': (8, 8, 3), 'activation': 'logistic'}, {'hidden_layer_sizes': (6, 7, 4), 'activation': 'logistic'}], 'split0_test_score': array([0.66644452, 0.98467178, 0.98633789, 0.988004  , 0.98767078,\n",
      "       0.98633789, 0.98567144, 0.66644452, 0.98733755, 0.988004  ]), 'split1_test_score': array([0.98666667, 0.98766667, 0.985     , 0.98833333, 0.989     ,\n",
      "       0.66666667, 0.98666667, 0.989     , 0.98766667, 0.98833333]), 'split2_test_score': array([0.98866667, 0.989     , 0.98666667, 0.99      , 0.98966667,\n",
      "       0.98566667, 0.98766667, 0.991     , 0.99      , 0.99066667]), 'split3_test_score': array([0.66666667, 0.98833333, 0.99      , 0.992     , 0.99133333,\n",
      "       0.66666667, 0.989     , 0.99      , 0.99      , 0.98966667]), 'split4_test_score': array([0.66666667, 0.66666667, 0.96366667, 0.96666667, 0.96666667,\n",
      "       0.963     , 0.962     , 0.66666667, 0.96766667, 0.96733333]), 'mean_test_score': array([0.79502224, 0.92326769, 0.98233424, 0.9850008 , 0.98486749,\n",
      "       0.85366758, 0.98220096, 0.86062224, 0.98453418, 0.9848008 ]), 'std_test_score': array([0.15729481, 0.12830904, 0.00947784, 0.00927627, 0.00917655,\n",
      "       0.1529166 , 0.01016038, 0.15845603, 0.00850805, 0.00878563]), 'rank_test_score': array([10,  7,  5,  1,  2,  9,  6,  8,  4,  3])}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivial Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trivial classifier =  [0.78373815]\n"
     ]
    }
   ],
   "source": [
    "s=sum(y_test)\n",
    "p=len(y_test)\n",
    "#predict all zeros:\n",
    "trivial_score = (p-s)/p\n",
    "print('Accuracy of trivial classifier = ',trivial_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_prob_triv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(np\u001b[39m.\u001b[39mshape(y_prob))\n\u001b[0;32m      2\u001b[0m y_prob_triv[:,\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(np\u001b[39m.\u001b[39mshape(y_prob[:,\u001b[39m0\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_prob' is not defined"
     ]
    }
   ],
   "source": [
    "y_prob_triv = np.zeros(np.shape(y_prob))\n",
    "y_prob_triv[:,0] = np.ones(np.shape(y_prob[:,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = mlp.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('false positive rate')\n",
    "# plt.ylabel('true positive rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_neg, tpr_neg, thresholds_neg = roc_curve(y_test, y_prob[:,0], pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_triv, tpr_triv, thresholds_triv = roc_curve(y_test, y_prob_triv[:,0], pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr_neg, tpr_neg)\n",
    "plt.plot(fpr_triv,tpr_triv)\n",
    "plt.xlabel('false negative rate')\n",
    "plt.ylabel('true negative rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(why is the roc curve for trivial classifier same as for a random classifier?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.9385390196295069\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(y_test, y_prob[:,1])\n",
    "print('auc = ', auc_score)\n",
    "\n",
    "# auc_score_neg = roc_auc_score((np.ones(np.shape(y_test))-y_test), y_prob[:,0])\n",
    "# print('negative auc score = ', auc_score_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score of 0.68; quite poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters for a 3-layer MLP: AUC score = 0.94; very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [(b) Use PUL file (60/40) + `train_df` for training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = np.zeros(PUL_df.shape[0])\n",
    "train_ind[:int(np.floor(PUL_df.shape[0]*0.6))] = 1 #prop. train\n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "X_PUL, y_PUL = PUL_df.to_numpy()[:,:-1], PUL_df.to_numpy()[:,-1:]\n",
    "\n",
    "X_PUL_train, y_PUL_train = X_PUL[train_ind==1,:], y_PUL[train_ind==1]\n",
    "X_PUL_test, y_PUL_test = X_PUL[train_ind==0,:], y_PUL[train_ind==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, y1_train = np.concatenate([np.array(train_df.iloc[:,:-1]), X_PUL_train]), np.concatenate([np.array(train_df.iloc[:,-1:]), y_PUL_train])\n",
    "X1_test, y1_test = X_PUL_test, y_PUL_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55924477\n",
      "Iteration 2, loss = 0.29139208\n",
      "Iteration 3, loss = 0.22117616\n",
      "Iteration 4, loss = 0.20915026\n",
      "Iteration 5, loss = 0.20070854\n",
      "Iteration 6, loss = 0.19331638\n",
      "Iteration 7, loss = 0.18711276\n",
      "Iteration 8, loss = 0.18265605\n",
      "Iteration 9, loss = 0.17920480\n",
      "Iteration 10, loss = 0.17504394\n",
      "Iteration 11, loss = 0.17313134\n",
      "Iteration 12, loss = 0.16985304\n",
      "Iteration 13, loss = 0.16765605\n",
      "Iteration 14, loss = 0.16487077\n",
      "Iteration 15, loss = 0.16344621\n",
      "Iteration 16, loss = 0.16196618\n",
      "Iteration 17, loss = 0.16021931\n",
      "Iteration 18, loss = 0.15801320\n",
      "Iteration 19, loss = 0.15618093\n",
      "Iteration 20, loss = 0.15528583\n",
      "Iteration 21, loss = 0.15392939\n",
      "Iteration 22, loss = 0.15088660\n",
      "Iteration 23, loss = 0.14992218\n",
      "Iteration 24, loss = 0.14986731\n",
      "Iteration 25, loss = 0.14754133\n",
      "Iteration 26, loss = 0.14511670\n",
      "Iteration 27, loss = 0.14479812\n",
      "Iteration 28, loss = 0.14488288\n",
      "Iteration 29, loss = 0.14122364\n",
      "Iteration 30, loss = 0.14026534\n",
      "Iteration 31, loss = 0.13930781\n",
      "Iteration 32, loss = 0.13640778\n",
      "Iteration 33, loss = 0.13719185\n",
      "Iteration 34, loss = 0.13446066\n",
      "Iteration 35, loss = 0.13298580\n",
      "Iteration 36, loss = 0.13135012\n",
      "Iteration 37, loss = 0.13188630\n",
      "Iteration 38, loss = 0.12911649\n",
      "Iteration 39, loss = 0.12813363\n",
      "Iteration 40, loss = 0.12666047\n",
      "Iteration 41, loss = 0.12516994\n",
      "Iteration 42, loss = 0.12406269\n",
      "Iteration 43, loss = 0.12308235\n",
      "Iteration 44, loss = 0.12151294\n",
      "Iteration 45, loss = 0.11922356\n",
      "Iteration 46, loss = 0.11866244\n",
      "Iteration 47, loss = 0.11808564\n",
      "Iteration 48, loss = 0.11761269\n",
      "Iteration 49, loss = 0.11580052\n",
      "Iteration 50, loss = 0.11326819\n",
      "Iteration 51, loss = 0.11407523\n",
      "Iteration 52, loss = 0.11182439\n",
      "Iteration 53, loss = 0.11029862\n",
      "Iteration 54, loss = 0.10888584\n",
      "Iteration 55, loss = 0.10844928\n",
      "Iteration 56, loss = 0.10876150\n",
      "Iteration 57, loss = 0.10750824\n",
      "Iteration 58, loss = 0.10741558\n",
      "Iteration 59, loss = 0.10495868\n",
      "Iteration 60, loss = 0.10448230\n",
      "Iteration 61, loss = 0.10420631\n",
      "Iteration 62, loss = 0.10199502\n",
      "Iteration 63, loss = 0.10085547\n",
      "Iteration 64, loss = 0.10107114\n",
      "Iteration 65, loss = 0.10242165\n",
      "Iteration 66, loss = 0.09814789\n",
      "Iteration 67, loss = 0.09652970\n",
      "Iteration 68, loss = 0.09663775\n",
      "Iteration 69, loss = 0.09559004\n",
      "Iteration 70, loss = 0.09605898\n",
      "Iteration 71, loss = 0.09414653\n",
      "Iteration 72, loss = 0.09518098\n",
      "Iteration 73, loss = 0.09614531\n",
      "Iteration 74, loss = 0.09080549\n",
      "Iteration 75, loss = 0.09107247\n",
      "Iteration 76, loss = 0.08948247\n",
      "Iteration 77, loss = 0.09144564\n",
      "Iteration 78, loss = 0.09295039\n",
      "Iteration 79, loss = 0.09076427\n",
      "Iteration 80, loss = 0.08857596\n",
      "Iteration 81, loss = 0.09042773\n",
      "Iteration 82, loss = 0.08933098\n",
      "Iteration 83, loss = 0.08707305\n",
      "Iteration 84, loss = 0.08993337\n",
      "Iteration 85, loss = 0.08601673\n",
      "Iteration 86, loss = 0.08589219\n",
      "Iteration 87, loss = 0.08653173\n",
      "Iteration 88, loss = 0.08531895\n",
      "Iteration 89, loss = 0.08442138\n",
      "Iteration 90, loss = 0.08507588\n",
      "Iteration 91, loss = 0.08422397\n",
      "Iteration 92, loss = 0.08059345\n",
      "Iteration 93, loss = 0.08069802\n",
      "Iteration 94, loss = 0.08384620\n",
      "Iteration 95, loss = 0.08058050\n",
      "Iteration 96, loss = 0.08068472\n",
      "Iteration 97, loss = 0.08064427\n",
      "Iteration 98, loss = 0.08195625\n",
      "Iteration 99, loss = 0.07937596\n",
      "Iteration 100, loss = 0.08199007\n",
      "Iteration 101, loss = 0.07828719\n",
      "Iteration 102, loss = 0.07953981\n",
      "Iteration 103, loss = 0.07562774\n",
      "Iteration 104, loss = 0.08387362\n",
      "Iteration 105, loss = 0.08284704\n",
      "Iteration 106, loss = 0.07914355\n",
      "Iteration 107, loss = 0.07519486\n",
      "Iteration 108, loss = 0.07373464\n",
      "Iteration 109, loss = 0.07512293\n",
      "Iteration 110, loss = 0.07511697\n",
      "Iteration 111, loss = 0.08343873\n",
      "Iteration 112, loss = 0.07410545\n",
      "Iteration 113, loss = 0.07289695\n",
      "Iteration 114, loss = 0.07082442\n",
      "Iteration 115, loss = 0.07052561\n",
      "Iteration 116, loss = 0.07342104\n",
      "Iteration 117, loss = 0.07294571\n",
      "Iteration 118, loss = 0.07007540\n",
      "Iteration 119, loss = 0.07338114\n",
      "Iteration 120, loss = 0.07186557\n",
      "Iteration 121, loss = 0.06959671\n",
      "Iteration 122, loss = 0.07343326\n",
      "Iteration 123, loss = 0.07020582\n",
      "Iteration 124, loss = 0.06939228\n",
      "Iteration 125, loss = 0.06829622\n",
      "Iteration 126, loss = 0.06640320\n",
      "Iteration 127, loss = 0.06787265\n",
      "Iteration 128, loss = 0.06615104\n",
      "Iteration 129, loss = 0.06796794\n",
      "Iteration 130, loss = 0.06908494\n",
      "Iteration 131, loss = 0.06725186\n",
      "Iteration 132, loss = 0.06795441\n",
      "Iteration 133, loss = 0.06832567\n",
      "Iteration 134, loss = 0.06915430\n",
      "Iteration 135, loss = 0.07113305\n",
      "Iteration 136, loss = 0.06854727\n",
      "Iteration 137, loss = 0.06254273\n",
      "Iteration 138, loss = 0.06412740\n",
      "Iteration 139, loss = 0.06333927\n",
      "Iteration 140, loss = 0.06317426\n",
      "Iteration 141, loss = 0.06334952\n",
      "Iteration 142, loss = 0.06280923\n",
      "Iteration 143, loss = 0.06084453\n",
      "Iteration 144, loss = 0.06480564\n",
      "Iteration 145, loss = 0.06071911\n",
      "Iteration 146, loss = 0.05883010\n",
      "Iteration 147, loss = 0.06432337\n",
      "Iteration 148, loss = 0.07045332\n",
      "Iteration 149, loss = 0.06437956\n",
      "Iteration 150, loss = 0.05918374\n",
      "Iteration 151, loss = 0.06119153\n",
      "Iteration 152, loss = 0.05850180\n",
      "Iteration 153, loss = 0.05844482\n",
      "Iteration 154, loss = 0.06038813\n",
      "Iteration 155, loss = 0.06270840\n",
      "Iteration 156, loss = 0.05983144\n",
      "Iteration 157, loss = 0.06015369\n",
      "Iteration 158, loss = 0.06082673\n",
      "Iteration 159, loss = 0.05887568\n",
      "Iteration 160, loss = 0.05928668\n",
      "Iteration 161, loss = 0.05462650\n",
      "Iteration 162, loss = 0.05350442\n",
      "Iteration 163, loss = 0.06321139\n",
      "Iteration 164, loss = 0.05735028\n",
      "Iteration 165, loss = 0.05702334\n",
      "Iteration 166, loss = 0.05804177\n",
      "Iteration 167, loss = 0.05738718\n",
      "Iteration 168, loss = 0.05663722\n",
      "Iteration 169, loss = 0.06075030\n",
      "Iteration 170, loss = 0.05641776\n",
      "Iteration 171, loss = 0.05954658\n",
      "Iteration 172, loss = 0.08143264\n",
      "Iteration 173, loss = 0.05777054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,10,10),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    verbose = True).fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y1_pred \u001b[39m=\u001b[39m mlp1\u001b[39m.\u001b[39mpredict(X1_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlp1' is not defined"
     ]
    }
   ],
   "source": [
    "y1_pred = mlp1.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412337662337662\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             1938          461\n",
      "Cazyme                  336          345\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y1_test,y1_pred))\n",
    "#confusion matrix\n",
    "mat = confusion_matrix(y1_test, y1_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 layer:**\n",
    "\n",
    "- 66.4% accuracy; still much worse than trivial classifier\n",
    "\n",
    "- Positive prediction really poor\n",
    "\n",
    "**2 layers:** (100,100)\n",
    "\n",
    "- Relu: 73.6%\n",
    "\n",
    "**3 layers:** (100,100,100)\n",
    "\n",
    "- Relu: 76.7% , poor positive prediction (<50%)\n",
    "\n",
    "**4:**\n",
    "- Relu: 75.2%, poor positive prediction (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Only use PUL file for training and testing: (60/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, y2_train = X_PUL_train, y_PUL_train\n",
    "X2_test, y2_test = X_PUL_test, y_PUL_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55567637\n",
      "Iteration 2, loss = 0.48351977\n",
      "Iteration 3, loss = 0.46832773\n",
      "Iteration 4, loss = 0.46029756\n",
      "Iteration 5, loss = 0.45292093\n",
      "Iteration 6, loss = 0.44632938\n",
      "Iteration 7, loss = 0.43771742\n",
      "Iteration 8, loss = 0.43126449\n",
      "Iteration 9, loss = 0.42326714\n",
      "Iteration 10, loss = 0.41405291\n",
      "Iteration 11, loss = 0.40557543\n",
      "Iteration 12, loss = 0.39433457\n",
      "Iteration 13, loss = 0.38140771\n",
      "Iteration 14, loss = 0.37806976\n",
      "Iteration 15, loss = 0.36222290\n",
      "Iteration 16, loss = 0.34947108\n",
      "Iteration 17, loss = 0.33497619\n",
      "Iteration 18, loss = 0.33345877\n",
      "Iteration 19, loss = 0.31490704\n",
      "Iteration 20, loss = 0.30467610\n",
      "Iteration 21, loss = 0.29570629\n",
      "Iteration 22, loss = 0.29469141\n",
      "Iteration 23, loss = 0.28360093\n",
      "Iteration 24, loss = 0.26760903\n",
      "Iteration 25, loss = 0.25455942\n",
      "Iteration 26, loss = 0.25855992\n",
      "Iteration 27, loss = 0.24419306\n",
      "Iteration 28, loss = 0.23548279\n",
      "Iteration 29, loss = 0.23478562\n",
      "Iteration 30, loss = 0.22276626\n",
      "Iteration 31, loss = 0.21173582\n",
      "Iteration 32, loss = 0.21055562\n",
      "Iteration 33, loss = 0.21046419\n",
      "Iteration 34, loss = 0.20013969\n",
      "Iteration 35, loss = 0.21819698\n",
      "Iteration 36, loss = 0.19353431\n",
      "Iteration 37, loss = 0.18025912\n",
      "Iteration 38, loss = 0.17164254\n",
      "Iteration 39, loss = 0.17105498\n",
      "Iteration 40, loss = 0.17500035\n",
      "Iteration 41, loss = 0.17241933\n",
      "Iteration 42, loss = 0.15471744\n",
      "Iteration 43, loss = 0.15360922\n",
      "Iteration 44, loss = 0.15065597\n",
      "Iteration 45, loss = 0.14810023\n",
      "Iteration 46, loss = 0.15736140\n",
      "Iteration 47, loss = 0.14480004\n",
      "Iteration 48, loss = 0.13034247\n",
      "Iteration 49, loss = 0.12963461\n",
      "Iteration 50, loss = 0.14007803\n",
      "Iteration 51, loss = 0.14243194\n",
      "Iteration 52, loss = 0.13294739\n",
      "Iteration 53, loss = 0.12943857\n",
      "Iteration 54, loss = 0.11551450\n",
      "Iteration 55, loss = 0.12083270\n",
      "Iteration 56, loss = 0.12136853\n",
      "Iteration 57, loss = 0.12196392\n",
      "Iteration 58, loss = 0.12024021\n",
      "Iteration 59, loss = 0.11156580\n",
      "Iteration 60, loss = 0.12101500\n",
      "Iteration 61, loss = 0.11448066\n",
      "Iteration 62, loss = 0.12431444\n",
      "Iteration 63, loss = 0.11711987\n",
      "Iteration 64, loss = 0.11493320\n",
      "Iteration 65, loss = 0.12315288\n",
      "Iteration 66, loss = 0.12576551\n",
      "Iteration 67, loss = 0.10335898\n",
      "Iteration 68, loss = 0.09599847\n",
      "Iteration 69, loss = 0.09642549\n",
      "Iteration 70, loss = 0.09486623\n",
      "Iteration 71, loss = 0.09304361\n",
      "Iteration 72, loss = 0.09445703\n",
      "Iteration 73, loss = 0.10010370\n",
      "Iteration 74, loss = 0.08909470\n",
      "Iteration 75, loss = 0.08864664\n",
      "Iteration 76, loss = 0.10703056\n",
      "Iteration 77, loss = 0.09306277\n",
      "Iteration 78, loss = 0.08679808\n",
      "Iteration 79, loss = 0.08557771\n",
      "Iteration 80, loss = 0.09101618\n",
      "Iteration 81, loss = 0.08200378\n",
      "Iteration 82, loss = 0.10554619\n",
      "Iteration 83, loss = 0.09939182\n",
      "Iteration 84, loss = 0.08138967\n",
      "Iteration 85, loss = 0.07835039\n",
      "Iteration 86, loss = 0.08435137\n",
      "Iteration 87, loss = 0.08920643\n",
      "Iteration 88, loss = 0.08570269\n",
      "Iteration 89, loss = 0.09739528\n",
      "Iteration 90, loss = 0.09820611\n",
      "Iteration 91, loss = 0.08353258\n",
      "Iteration 92, loss = 0.09490031\n",
      "Iteration 93, loss = 0.08004158\n",
      "Iteration 94, loss = 0.07103520\n",
      "Iteration 95, loss = 0.08222889\n",
      "Iteration 96, loss = 0.08732374\n",
      "Iteration 97, loss = 0.08306501\n",
      "Iteration 98, loss = 0.07541736\n",
      "Iteration 99, loss = 0.07820552\n",
      "Iteration 100, loss = 0.09644322\n",
      "Iteration 101, loss = 0.07618397\n",
      "Iteration 102, loss = 0.08756649\n",
      "Iteration 103, loss = 0.07975766\n",
      "Iteration 104, loss = 0.09161657\n",
      "Iteration 105, loss = 0.08153388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100),\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    verbose = True).fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = mlp2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707792207792208\n",
      "            Pred_not_cazyme  Pred_cazyme\n",
      "Not_cazyme             2048          351\n",
      "Cazyme                  355          326\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y2_test,y2_pred))\n",
    "mat = confusion_matrix(y2_test, y2_pred)\n",
    "cfmat_df = pd.DataFrame(np.array(mat))\n",
    "index_, columns_ = ['Not_cazyme','Cazyme'], ['Pred_not_cazyme', 'Pred_cazyme']\n",
    "cfmat_df.index, cfmat_df.columns = index_, columns_\n",
    "\n",
    "print(cfmat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 layer (10 neurons): 80.7% accuracy, \n",
    "\n",
    "- 2 layers: 78.3% accuracy (100,100); 78.7% (10,10)\n",
    "- poor positive prediction\n",
    "\n",
    "- 3 layers: 79.7% (100,100,100); 78.2% (10,10,10) \n",
    "\n",
    "- 4 layers: 77.0% (10,10,10,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fewer neurons lead to poorer positive prediction (why?)\n",
    "- (10,10): 35.4%\n",
    "- (3,3): 30.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y2_prob = mlp.predict_proba(X2_test)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y2_test, y2_prob[:,0], pos_label=0)\n",
    "plt.plot(fpr2, tpr2)\n",
    "plt.xlabel('false negative rate')\n",
    "plt.ylabel('true negative rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.694573852663769\n"
     ]
    }
   ],
   "source": [
    "auc2_score = roc_auc_score(y2_test, y2_prob[:,1])\n",
    "print('auc = ', auc2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score = 0.69; not much better than in (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
